1. Document Clarity and Potential Issues

After a thorough review of the provided operations guide, the overall structure and intent are clear and highly detailed. The document lays out a comprehensive framework for an AI-friendly repository template. However, a few points may need clarification or confirmation:
	•	“八大文档” (Eight Major Documents): The guide references “同步八大文档与注册信息” during module scaffolding ￼, implying each new module should generate eight key documents along with registry entries. The exact list of these eight documents isn’t explicitly enumerated in one place. Based on context, they likely include: the module’s ROUTING.md, AGENTS.md, CAPABILITIES.md, CONTRACT.md, and several “knowledge” docs like Guide/Quickstart, Runbook, Changelog, and Bug/Issue log (some of which may correspond to workdocs as dynamic logs). We will confirm the exact intended eight documents (e.g. whether Quickstart or Guide is counted separately, etc.) with the user to ensure no document is missing.
	•	Quickstart Document Placement: The guide describes a Quickstart document for modules as an optional “hub” that chains other docs (routing -> Quickstart -> plan/context/tasks -> contract/runbook/bugs) ￼ ￼. However, there is a slight ambiguity on where this Quickstart doc resides. In section 5.2.1 it’s listed under workdocs/active/quickstart.md (suggesting a workdoc used for progressive context loading) ￼. Later, it says the module’s knowledge docs (in modules/<instance>/doc/) include quickstart/runbook/spec/guide files ￼. This dual mention is a bit confusing. Our interpretation is: the Quickstart content is primarily for AI’s contextual chain (thus living under workdocs as a short-lived hub), while the module’s doc/ contains more permanent guides/specs/runbooks. We should confirm if the Quickstart should exist as a persistent guide in doc/ or just as a dynamic workdoc entry for context recovery.
	•	Document Roles vs. Locations: The document defines various doc roles (router, agent_policy, capability_index, guide, spec, quickstart, contract, etc.) and emphasizes separating AI-facing docs from human-facing docs ￼. It will be important to confirm any specific naming or placement conventions for certain roles. For example, type contracts for module types should likely reside in a consistent location (perhaps modules/<type>/contract.md or under a doc/ subdirectory). We’ll ensure our document list follows the naming conventions (the guide often uses uppercase names for key docs like ROUTING.md, AGENTS.md, CAPABILITIES.md, CONTRACT.md) and that no AI-oriented doc is mistakenly placed in a human docs folder.
	•	Complexity and Overlap: The system is quite elaborate with many moving parts (routing, graph, registry, triggers, guardrails, workdocs, etc.). There is potential overlap between some components – for instance, the doc_node_map vs. context_routes vs. graph_node_id mapping might seem redundant if not clearly explained. Similarly, scripts like doc_route_check.py vs. a route_lint Make target could be doing similar checks. We didn’t find explicit contradictions, but we should verify our assumptions: e.g., does make route_lint simply run the doc_route_check.py script (to validate context_routes) or something slightly different? We will clarify these during implementation to avoid conceptual confusion.
	•	Scope Hierarchy in Routing: In the functional requirements list for the routing system ￼ ￼, some topics are nested (e.g. “项目初始化” with sub-points “模块初始化”, “后端开发”, etc.). It’s a bit unclear if those sub-points are meant as separate when cases under the “项目初始化” topic, or if each should be its own topic. We will assume each sub-point represents a distinct scenario (likely a when under a broader topic) and structure the routing accordingly (see section 5 below), but we’ll double-check the intended hierarchy with the user.

Overall, the document’s logic is consistent and very detailed. The above points will be confirmed with the user before proceeding, to ensure the execution plan aligns perfectly with the intended standards. With these clarifications noted, we can now extract and organize the core requirements.

2. Preliminary Document List (doc_list_pre)

Based on the guide, the repository will require a comprehensive set of documents to implement the standardized, AI-oriented framework. Below is the preliminary list of documents to be created or maintained, along with their purpose. We include both routing documents (ROUTING.md files that direct the AI “where to find information”) and content/policy documents (which provide the actual knowledge, rules, and guides). We also ensure that every major standard or procedure mentioned in the guide has a corresponding document explaining it, so the AI agent can refer to these for guidance.
	•	Root ROUTING.md (doc_role: router, audience: AI) – Top-level documentation routing for the entire repository. This is the root of the documentation tree, defining high-level scopes and topics (e.g. “Execution”, “Governance”, “Systems”) and pointing the AI to the appropriate sub-route or leaf docs under each. It contains the context_routes structure (scope → topic → when → target_docs) for the repository’s main knowledge areas ￼ ￼. This file tells the AI “where to go” for any given type of task or information.
	•	Root README.md (audience: human) – High-level overview for human developers. It will likely include a summary of the project and a route table (auto-generated or manually maintained) listing scopes/topics and linking to key docs ￼ ￼. This helps human contributors (and can also assist the AI) to see the documentation structure at a glance. (The AI is not required to read this for routing – the ROUTING.md serves the AI – but README provides a synchronized human-readable index and should be kept consistent with the routing structure.)
	•	doc_agent/ROUTING.md (if needed) – A routing node under doc_agent/ if we further organize AI-specific docs. For example, if doc_agent/ contains multiple categories (quickstarts, policies, orchestration configs), a ROUTING.md here can direct the AI within the doc_agent section. (If the doc_agent directory is flat enough, this may be optional. We will decide based on final doc placement.)
	•	doc_agent/AGENTS.md (doc_kind: agent_policy, audience: AI) – Global Orchestrator Agent Policy. This document defines the orchestrator agent’s role, responsibilities, and safety boundaries at the top level ￼ ￼. It includes what the orchestrator can/cannot do, any global guardrails (e.g. “never commit to protected branches without approval”), required checks before execution, handoff rules, and references to relevant policies. It must use the standard front matter for agent policies (role: orchestrator, graph_node_id for orchestrator node, etc.) ￼ and be ≤200 lines ￼. Essentially, this is the “rules of engagement” for the AI at the root level.
	•	doc_agent/CAPABILITIES.md (doc_kind: capability_index, audience: AI) – Global Capability Index. This provides a catalog of all high-level capability domains and entry points exposed in the repository ￼ ￼. For example, it will list domains like DocOps, Guardrail, Workflow & Telemetry, Data & Schema, API Gateway, etc., each with their associated capabilities, scripts, or services. Its front matter will include scope (likely “global” or multiple scopes) and a purpose like “Catalog callable abilities under global scope” ￼. The body is structured by scope → capability → trigger, enumerating each capability’s ID, type (tool/service/composite), entrypoint (script or command), expected inputs/outputs, constraints, and related docs ￼. This index helps the AI in ability retrieval: once it knows what kind of action it needs, it can find the appropriate tool/agent here.
	•	Orchestration Config Files (YAML/JSON, machine-readable, for internal consistency – typically not directly read by AI, but maintained for automation):
	•	doc_agent/orchestration/registry.yaml – The orchestrator registry, listing all exposed agents (or composite services) in the system ￼. Each entry includes the agent’s id, role, owned paths, tools_allowed, etc. The orchestrator and domain-specific agents (DocOps, Guardrail, etc., as well as module agents) will be registered here. This is the Single Source of Truth (SSOT) for what agents exist and their basic metadata.
	•	doc_agent/orchestration/capabilities.yaml – The capability registry for base abilities. It enumerates low-level actions (scripts, API calls, etc.) with their capability_id, kind (tool/service/script), entrypoint, inputs/outputs, constraints, etc. This is the machine-readable counterpart to CAPABILITIES.md for automated checks.
	•	doc_agent/orchestration/agent-graph.yaml – The directed graph specification of the agent orchestration flow ￼. Defines nodes (orchestrator, domain agents, tool nodes) and edges (data flow and handoff conditions) for the AI agent system. It declares graph metadata (id, version, mode, entry/terminal nodes, scheduling strategy) ￼ and the connections between nodes (from->to with conditions, transforms, error routes) ￼ ￼.
	•	doc_agent/orchestration/doc-node-map.yaml – Mapping between documentation and graph nodes. Every topic in a ROUTING.md and every leaf doc is mapped to the corresponding graph_node_id (or capability node) if applicable ￼. This ensures the documentation hierarchy and the agent graph remain synchronized (so the AI’s context routing aligns with actual agent abilities).
	•	doc_agent/orchestration/agent-triggers.yaml – Trigger definitions for events that can occur during development (e.g. a new migration file added, or a PR opened with certain keywords) ￼ ￼. Each trigger has an ID, matching criteria (file path or prompt pattern), a target_agent to invoke, enforcement level (info/warn/block), any preload_docs to automatically load, required_commands to run first, and auto_tasks to create ￼. This file drives the automated event→agent responses.
	•	doc_agent/orchestration/trigger-map.yaml – An auto-generated mapping that expands the triggers into a matrix of trigger → agent policy → capability → command mapping ￼ ￼. It’s built by a script (trigger_map_sync.py) and helps in verifying that each trigger is linked to the right agent and tool. (The README or docs may reference this for transparency, but it’s not manually edited.)
	•	Policy and Guide Documents (AI-facing, lightweight docs on specific rules or procedures):
	•	Security/Compliance Policies (doc_agent/policies/*.md): A set of documents detailing safe-operating procedures and rules that the AI must follow for security, quality, and governance. For example, a Guardrail Policy doc might outline forbidden actions (like not touching production database without approval), approval workflows, and incident response. A Quality Assurance Policy might describe testing minimums, code style enforcement, etc. These documents are to be referenced whenever the AI hits a related “when” or a guardrail trigger. They are stored separately (doc_agent/policies/) and loaded as needed (for example, a preload_docs may inject the relevant policy into context on certain triggers) ￼. Each policy doc will have clear front matter (audience: ai, role: policy, purpose, etc.) and then structured content: scope of policy, allowed/prohibited actions, audit requirements, etc ￼.
	•	Operational Guides and “Quickstarts” (doc_agent/quickstart/ and doc_agent/flows/): These are AI-oriented how-tos for common workflows. They serve as mid-tier guides that the routing system can point to when the AI is about to perform a certain operation. For instance, a “Quality-Quickstart” guide is mentioned ￼ ￼, likely containing steps to run linters and tests before committing (it’s referenced when the AI is preparing to run make dev_check). Similarly, we might have a “Project Initialization Guide” for setting up a new repository or “Workflow & Trigger Guide” for maintaining the trigger system. These guides follow the progressive disclosure principle: short, focused, and linking out to deeper specs if needed. They reside in doc_agent/quickstart/ or doc_agent/flows/ and are registered in the routing under appropriate topics.
	•	Documentation Standards Spec (doc_agent/guide/documentation-spec.md or similar): A guide that consolidates the documentation norms and front matter schema (much of which is described in Chapter 1 of the guide). It would outline how to format docs (front matter fields, line limits, use of ✅/⚠️ signals, avoiding long prose, etc.), how to maintain the routing index, and the process to update docs when things change ￼ ￼. This can be the “single reference” the AI reads to ensure it adheres to documentation style and knows what metadata to include. (The user’s guide itself serves this role to some extent, but we’ll likely create a distilled version for the repository.)
	•	Script Usage Guide (scripts/operations-guide.md and sub-guides): Under the scripts/ directory, we will maintain an Operations Guide that explains the naming conventions, input/output, and dry-run usage for scripts ￼. Additionally, subdirectories in scripts/ (e.g. scripts/data/, scripts/guardrail/, scripts/workflow/, scripts/ci/) will contain a README each, documenting the purpose and usage of scripts in that category ￼. These guides ensure the AI (and humans) know how to call the scripts properly and interpret their output. They are referenced in CAPABILITIES.md and by the AI when deciding which script to run for a task.
	•	Module System Documents:
	•	modules/ROUTING.md – Modules index routing. This file (in the root of the modules/ directory) provides a navigation for all modules and module types in the repository ￼ ￼. It lists each module type and the instances available, using scope/topic/when entries to help the AI quickly jump to a particular module’s documentation. For example, scope might be the business domain or stage (e.g. “Assignment Stage”), topics could be specific module types or phases (e.g. “Selector Modules”), and the when could point to a particular instance’s quickstart or contract (“when the user chooses AI-driven selection, go to M_AI_Select’s docs”). This routing ensures the AI can locate a module’s docs in two hops ￼ (from root modules routing to instance docs).
	•	Module Type Contracts (modules/<type>/TYPE_CONTRACT.md): For each module type (the guide calls these “类型契约”), we need a document that specifies the responsibilities, interfaces, and data contracts that any instance of this type must fulfill ￼. For example, if <type> is 1_Assign (an assignment stage in a workflow), the type contract doc would describe what all assignment modules should do, what inputs/outputs they expect, etc. This ensures different module implementations are interchangeable as long as they follow the contract. These could be named uniformly (perhaps CONTRACT.md in the type folder, or included in a README). They are primarily human-oriented but also can be read by AI to understand requirements of that stage.
	•	Module Instance Documentation (within each modules/<instance>/doc/ directory): Each module instance provides a full set of documentation so that an AI (or human) can understand and work on it without needing external context. These include:
	•	modules/<instance>/doc/ROUTING.md – A local routing if the module doc set is complex enough. Often, the module’s ROUTING might be very simple, pointing to its Quickstart, Agent policy, Capability index, and any major guide or spec in its doc/. If the module’s documentation is straightforward, the parent modules/ROUTING.md could directly link to the instance’s key docs, and a separate local ROUTING might be optional. However, to maintain consistency, we will likely have a minimal ROUTING.md in each instance’s doc folder to act as the leaf routing node for that module (with route_role: leaf).
	•	modules/<instance>/doc/AGENTS.md – Module Agent Policy. This defines the rules and guardrails specific to that module’s agent (the module’s node in the larger agent graph) ￼ ￼. For example, a Payments module agent might forbid altering financial calculations without certain tests, etc. It will contain front matter with the module’s graph_node_id, role (module-agent), and use the same schema as other AGENTS.md ￼. Content includes Access Control rules, required tests or approvals before module-specific deployments, allowed tools (likely referencing module-specific scripts listed in its CAPABILITIES.md), escalation procedures, etc ￼ ￼. If a module doesn’t have any executable logic (unlikely, but if it were just a documentation module), AGENTS.md could be omitted ￼.
	•	modules/<instance>/doc/CAPABILITIES.md – Module Capability Index. Similar to the global one but scoped to this module’s features ￼ ￼. It lists the module’s exposed abilities (e.g. “process payment”, “recalculate risk score”) along with entrypoints (APIs, internal scripts) and how they map to base capabilities. If the module has internal scripts or tools, they are listed here. It also includes any triggers the module might produce or handle, mapped via target_nodes to the global graph ￼. Front matter includes scope (inherited or extended from the type’s scope) ￼.
	•	modules/<instance>/doc/CONTRACT.md – Module Interface Contract. This is the spec detailing the module’s API or interface (for back-end modules, possibly an OpenAPI spec or gRPC interface; for front-end modules, maybe a data contract or component interface). It declares inputs/outputs, data models, and compatibility requirements (what baseline or versioning is maintained) ￼ ￼. Keeping this updated is critical (failure to update contracts is cited as an error example ￼). The CONTRACT.md helps the AI verify it’s not breaking interfaces and should be cross-checked in tests (like make contract_compat_check).
	•	Guides/Runbooks/Specs (module-specific): Additional explanatory documents under modules/<instance>/doc/ as needed:
	•	Module Quickstart Guide (if not using workdocs quickstart): A guide on how to quickly get started with this module’s development or testing, possibly overlapping with runbook.
	•	Runbook/Operations Guide: Step-by-step procedures to operate or troubleshoot the module in runtime (e.g. how to deploy just this module’s service, common errors and resolutions, how to trigger certain workflows) ￼ ￼.
	•	Module Design Spec / Guide: If the module has complex logic, a design document or technical spec can be included to explain algorithms or decisions (audience might be more human but can be structured for AI too).
	•	Changelog (module-specific): A history of changes for this module (major updates, breaking changes, etc.). This could be a CHANGELOG.md in the module doc. However, since ongoing changes are tracked in workdocs and maintenance reports, a static changelog may be more for human consumption and traceability.
	•	Known Issues / Lessons Learned: A summary of key bugs or lessons that were discovered in this module, if any persistent knowledge should be preserved. (Critical issues are also logged in workdocs lessons log, but a distilled “gotchas” guide could be beneficial for future AI context to avoid repeating mistakes in this module.)
	•	All module doc files will have full front matter specifying audience, purpose, doc_role (e.g. guide, spec, runbook, etc.), owner, updated_at, etc., to ensure the AI can identify and load them appropriately ￼. They will also be registered in the module’s and/or global routing so the orchestrator can find them when needed.
	•	modules/registry.yaml – The Module Registry (SSOT for modules) ￼ ￼. It lists every module instance with fields like:
	•	id: a unique identifier (e.g. <type>.<instance> name),
	•	type_path and instance_path: directory paths,
	•	route_refs: pointers to the module’s key docs (quickstart or README links, etc.),
	•	graph_bindings: how this module’s agent is connected in the global graph (e.g. which parent node it attaches to),
	•	capability_refs: links to any capabilities provided,
	•	requires / provides: dependency metadata (which other modules or global services it relies on or offers to),
	•	related_modules: siblings or alternatives,
	•	ownership, status (active/deprecated), doc_set completeness, last_verified_at, etc.
This registry is updated whenever modules are added/removed or changed, and serves as the basis for syncing the orchestrator’s view of modules ￼.
	•	doc_agent/orchestration/module-registry.yaml – An auto-generated view of the module registry for the orchestrator’s consumption ￼. This is created by a script (module_registry_sync.py) from modules/registry.yaml and used to update the global agent graph and routing references for modules. (Not directly read by AI, but ensures the AI’s indices remain accurate.)
	•	AI Workdocs and Logs: (These are dynamic documents updated during development rather than static reference docs, but they form a crucial part of the AI’s context mechanism.)
	•	Global Workdocs (ai/workdocs/active/…) – The interactive context for active tasks at the project level. For each active task or feature, e.g. “template-bootstrap” or a user story, a folder is created with:
	•	plan.md – the strategy and plan for that task (objectives, acceptance criteria, guardrails to consider) ￼ ￼.
	•	context.md – a concise hub (≤120 lines) summarizing progress and linking to deeper context logs ￼ ￼.
	•	tasks.md – task breakdown, to-dos, and status for the task ￼ ￼.
	•	context/ subdocs: session-progress.md, decisions.md, risks.md, active-files.md, lessons.md – detailed logs for each area (running history of steps, decisions/approvals, risk tracking, file changes, and errors/lessons respectively) ￼ ￼.
	•	These workdocs are maintained by the AI (with human oversight when needed) to recover context and ensure continuity between sessions ￼ ￼. They are not part of the permanent codebase docs but are archived under ai/workdocs/archive/ once a task completes ￼ ￼. They are crucial for the AI’s memory (answering “what did I do last time?” without rereading the entire repo).
	•	Module Workdocs (modules/<instance>/workdocs/active/…) – Each module instance has its own workdoc folder for module-specific development context ￼ ￼. It contains a similar set of files (plan.md, context.md, tasks.md, etc., focusing only on that module) to keep local context. This separation means when the AI is working within a module, it uses the module’s workdocs for details, reducing clutter in the global context. These are updated and archived in parallel with the global ones, and cross-references can be added if needed (but the system tries to minimize cross-coupling to avoid confusion) ￼ ￼.
	•	handoff-history.md – A global handoff log that records important transfer points, such as approvals, phase completions, or when development is handed over between AI and humans ￼ ￼. Every significant event (e.g., guardrail approval outcome, finalizing a feature and archiving its workdocs) is appended here with a timestamp, participants, and a summary of next steps ￼ ￼. This provides an audit trail across the project.
	•	Maintenance Reports (ai/maintenance_reports/*.md): Various global reports updated by automation for monitoring and retrospective:
	•	route-health.md – records stats about the routing/graph usage, trigger hit rates, any orphan triggers/nodes, performance metrics (like how often each path is taken, average latency) ￼ ￼. It’s updated by telemetry (context tracker) and periodic checks (trigger and guardrail dry-runs) ￼ ￼.
	•	retrospective.md – a running log of major lessons learned and improvements, aggregated from the context/lessons.md of various tasks ￼ ￼. This allows the AI to recall global lessons from past errors.
	•	Evolution or progress logs (if any) – e.g., an agent graph evolution report if we maintain one after each major change (the guide suggests publishing graph evolution logs in ai/maintenance_reports/ after each iteration) ￼.
	•	These reports are primarily for maintainers to gauge system health and for the AI to plan optimizations.
	•	Ops/Evaluation Documents (if required): The guide mentions placing evaluation and baseline documents in ops/evals/ to avoid accidental AI loading ￼. This could include test scenarios for the AI or baseline metrics. We will include such docs if needed (e.g., an initial test plan or evaluation metrics definition), but since these are not core to the template’s operation, we consider them optional for now.

This document list covers all major types of documentation and assets described in the guide. Each standard or mechanism introduced (documentation format, routing, agent policies, capabilities, triggers, guardrails, module contracts, context logs, etc.) has a corresponding document or file to support it. The naming conventions are taken from the guide to ensure consistency (e.g. using ROUTING.md, AGENTS.md, CAPABILITIES.md exactly as specified, uppercase for key index files). In the next steps, we will verify that these names and locations conform to the naming scheme rules and adjust if necessary.

3. Preliminary Functionality List (func_list_pre)

The repository template also requires implementing a variety of foundation functions and tools to automate processes and empower the AI. These functions can be categorized into scripts, automation commands, and agent capabilities. Below is a list of the needed functionalities (with tentative names as given or implied by the guide) and a brief description of their purpose and typical usage scenario. We will later examine if some of these can be merged or simplified, but initially we list them individually for clarity:
	•	Scaffolding Command (make ai_begin MODULE=<name>): Automates the creation of a new module instance directory. Usage: When starting a new module, the developer/AI runs this Make target with the module name. Function: It generates the module’s folder structure and boilerplate files according to the template (creating modules/<name>/ with all subfolders, initial code files, and the eight key docs in doc/ and workdocs/active/ populated with templates) ￼ ￼. It may prompt for or consume prepared metadata (module responsibilities, interfaces, etc. from the requirements doc) to fill in front matter and placeholders. This ensures consistency in module setup and prevents manual omissions (manual copying is explicitly forbidden) ￼.
	•	Documentation/Registry Sync Script (python scripts/module_registry_sync.py): Usage: Run after adding or modifying modules in modules/registry.yaml. Function: Reads modules/registry.yaml (the SSOT for modules) and propagates updates to the global orchestrator view. It writes changes into doc_agent/orchestration/module-registry.yaml and also updates related entries in doc_agent/orchestration/registry.yaml, agent-graph.yaml, and doc-node-map.yaml for any new module nodes or removed ones ￼ ￼. This script essentially synchronizes module information across the system, so the orchestrator is aware of new module agents and their docs. It outputs logs which the AI will record in the workdocs (e.g., context.md#QA) for verification ￼.
	•	Documentation-Graph Map Sync (python scripts/doc_node_map_sync.py): Usage: Can be run in two modes – --write to update mappings or --report to check consistency. Function: Ensures that every doc, route, and agent node mapping is consistent. It likely generates or updates doc_node_map.yaml by scanning all ROUTING.md files for context_routes and matching them with agent-graph nodes and capability entries. In report mode, it flags any inconsistencies (like a ROUTING topic with no corresponding agent node, or a graph node with no doc) ￼ ￼. The AI should run this (or make route_lint, which may call it) before executing tasks to verify documentation routing integrity ￼. The output (listing any broken links or mismatches) is pasted into the workdoc QA section for review.
	•	Route Linting (make route_lint target / doc_route_check.py script): Usage: Frequently (before commits or before running an agent graph). Function: Checks that all context_routes in ROUTING.md files are syntactically and logically correct, and that target_docs paths exist and correspond to the intended docs. According to the guide, a doc_route_check.py script is provided ￼ which likely performs these verifications (ensuring no orphan context_routes and that front matter keywords align with topics) ￼. make route_lint probably wraps this script. The AI is expected to run route lint routinely to catch any routing documentation errors early ￼.
	•	Capability Index Consistency Check (make capability_index_check / script): Usage: After updating any capabilities (adding a new script or tool) or periodically in CI. Function: Compares the capabilities.yaml registry and each CAPABILITIES.md with the agent graph and orchestrator registry to ensure consistency ￼ ￼. It verifies that for each base capability in YAML, there’s a corresponding entry in the readable index, and that any new capability appears in the graph and is provided by an agent if needed. Essentially, it guards the base capability layer consistency. The guide explicitly mentions running this after adding new capabilities and as a CI check ￼.
	•	Orchestrator Registry Check (python scripts/registry_check.py): Usage: After adding or modifying an agent (composite capability) or as part of CI. Function: Checks that any new agent (or tool) is properly registered in doc_agent/orchestration/registry.yaml with all required fields, and that registry entries align with the agent-graph nodes and capability registry. For example, if a new agent “data_schema” was added to handle DB tasks, this script ensures an entry exists in the registry with correct id, scope, tools_allowed, etc., and that its provides match some base capabilities ￼ ￼. It likely also checks for orphan entries or missing links.
	•	Agent Lint (python scripts/agent_lint.py): Usage: During CI or pre-commit to validate agent policy docs. Function: Scans all AGENTS.md files to ensure they contain the required front matter fields (audience: ai, doc_kind: agent_policy, role, graph_node_id, etc.) and that each graph_node_id corresponds to an entry in the agent graph and registry ￼ ￼. It also likely checks that no forbidden fields (like target_agent or hard-coded tools_allowed) appear in AGENTS.md files (since those belong in central registry) ￼ ￼. Additionally, it validates mappings: for instance, if an AGENTS.md declares a certain graph_node_id, it cross-verifies that the doc-node-map and routing have that node associated with the doc’s topic. Running this ensures all agent policies are well-formed and linked.
	•	Agent Graph Structure Check (python scripts/agent_graph_check.py): Usage: Whenever the agent-graph.yaml is edited (or periodically). Function: Validates the graph YAML structure against a schema ￼. It checks that every node has necessary fields (id, kind, entrypoint, etc.), edges connect valid node ports, no cycles unless allowed (DAG preferred) ￼, and that things like entry_nodes and terminal_nodes are defined. It also might enforce that each node has an owner and constraints per policy. Essentially, it’s a static analysis of the agent orchestration graph to ensure it’s well-formed and adheres to the defined JSON schema ￼. In CI, this prevents deploying an inconsistent agent graph.
	•	Trigger Mapping Sync (python scripts/trigger_map_sync.py): Usage: After editing agent-triggers.yaml. Function: Reads the triggers YAML and generates/updates trigger-map.yaml which maps triggers to specific agent nodes and capability entries ￼ ￼. It may also create a human-readable matrix or graph of triggers. The goal is to allow easy verification that for each trigger ID, the system knows which policy to load, which scripts to run. The script likely outputs any issues (e.g. a trigger referencing a non-existent agent or tool).
	•	Trigger Consistency Check (make trigger_check target): Usage: As part of CI or a weekly cron, and after adding new triggers. Function: Likely a composite command that:
	•	Runs a dry-run simulation of all triggers using trigger_runner.py --dry-run all (see below) to ensure each trigger’s flow can execute without error ￼ ￼.
	•	Possibly scans tasks.md across workdocs to ensure triggers that should have created tasks did so, and that every active trigger in agent-triggers.yaml has an associated handling (the guide mentions a Chapter 8 trigger inspection script that compares tasks and triggers ￼ ￼).
	•	Checks for “孤立 trigger” (orphan triggers with no policy or no matching events) and would alert if found ￼.
	•	The results (trigger simulation outcomes, orphan check) are logged to route-health.md and any issues need to be fixed or documented.
	•	Trigger Execution Script (python scripts/trigger_runner.py): Usage: Invoked when an event occurs (or manually with --event <ID> for dry-run). Function: Implements the Trigger → Agent → Tool workflow:
	•	It listens for or is called with specific events (like a file commit or a user command).
	•	On an event, it finds the corresponding trigger in agent-triggers.yaml, loads the associated policy section from an AGENTS.md (Trigger Handling section) ￼, and preloads any specified docs (e.g. relevant SOPs).
	•	If required_commands are specified (like running tests or lint first), it executes those (possibly via PreToolUse hook).
	•	It then invokes the target agent or tool (e.g. calls the entrypoint of the capability specified).
	•	After execution, in the PostToolUse phase, it logs the output and any files changed to the context logs (context.md#Automation and context/active-files.md) ￼ ￼.
	•	In dry-run mode, it would simulate these steps without making changes, to validate that the flow is set up correctly ￼.
	•	This tool is essential for automating responses to triggers (like auto-running DB migrations check when a new migration file is added, etc.).
	•	Guardrail Execution Script (python scripts/guardrail_runner.py): Usage: Invoked when a guardrail-level trigger (enforcement: block) fires, or run in dry-run to verify guardrail flows. Function: Automates the approval workflow for high-risk actions:
	•	On a critical event (e.g. code about to connect to prod DB, or a PR hits a blocked trigger), this is called to handle the approval sequence ￼ ￼.
	•	It loads the relevant guardrail policy from AGENTS.md (or Trigger Handling for that trigger) ￼, ensures required_commands (like extra lint or safety checks) are executed and passed ￼.
	•	It then helps prepare the approval bundle: gathering diff, test results, risk assessment into the context.md#Approvals section ￼ ￼ for the human approver to review.
	•	It waits or signals for approval input (this may be out-of-band from AI, but the process is documented).
	•	If approved, it logs the decision (with rationale, rollback plan) in context and plan docs ￼ ￼, updates tasks (marks Guardrail: task as completed) ￼, and allows the originally blocked action to proceed.
	•	If rejected or requiring rollback, it ensures the rollback script (defined either in policy or as per approver notes) is executed and logs the outcome ￼.
	•	Dry-run (--dry-run all) mode will simulate the above for all defined guardrail triggers to ensure the process can complete automatically (i.e., all required commands exist and pass, all references are correct) ￼.
	•	This function is crucial to maintain a closed-loop approval mechanism and is tightly integrated with the AI’s workflow (the AI must pause and wait for approval where required, then read the approver’s decision from the context to continue).
	•	Context Usage Tracker (python scripts/context_usage_tracker.py): Usage: Run periodically (maybe via CI or a daemon) to collect telemetry. Function: It scans logs of agent runs and context files to compute metrics such as:
	•	Trigger hit rates (how often each trigger fired over time, how often guardrails were invoked, etc.) ￼.
	•	Average execution times per agent or tool, failure rates, etc.
	•	These stats are then written to ai/maintenance_reports/route-health.md for analysis ￼.
	•	This helps identify hotspots or inefficiencies in the orchestration, e.g. if a particular path is very slow or a trigger is firing too frequently.
	•	AI Chain Optimizer (python scripts/ai_chain_optimizer.py): Usage: Possibly triggered after enough telemetry is gathered, or as part of retrospectives. Function: Based on telemetry, this agent/tool might suggest optimizations to the agent graph or routing:
	•	E.g., if it finds an 80% hit rate on one path, it might suggest making that path more direct (less hops).
	•	Or if certain nodes often fail and fallback, propose improvements or splitting/merging nodes ￼.
	•	It would write recommendations to route-health.md or a similar report. (This is more speculative – the guide mentions the output of telemetry is used as basis for next graph optimization ￼, implying a step where the AI or a maintainer reviews and adjusts the system.)
	•	Schema/Contract Validators:
	•	Database Schema Linter (python scripts/db_lint.py and make db_lint): Usage: Before applying DB migrations or whenever schema files are changed. Function: Validates the YAML schema files for the database (checking field definitions, naming conventions, required fields) ￼ and ensures no illegal changes (e.g. direct DROP TABLE is disallowed) ￼. Also likely checks if every _up.sql migration has a corresponding _down.sql ￼. This is run as part of guardrail required commands for DB triggers and in CI.
	•	Migration Check (make migrate_check / part of db agent tools): Simulates or verifies that pending migrations can be applied and rolled back cleanly. It might use a test database to apply the _up.sql then _down.sql, making sure no errors.
	•	Rollback Check (make rollback_check): Ensures that the down migrations truly reverse the ups and that no data loss beyond intended occurs. The Data & Schema agent would call these via tools_allowed on triggers like “new migration file detected” ￼ ￼.
	•	These DB-related commands are likely integrated with the Data & Schema agent and listed in its tools_allowed in registry (e.g., make db_lint, make migrate_check, etc.) ￼ ￼.
	•	API Contract Checker (python scripts/type_contract_check.py and make contract_compat_check): Usage: When API interface files (like OpenAPI spec or gRPC proto) or module CONTRACT.md are changed. Function: Compares the current API contract with a saved baseline (e.g., .contracts_baseline/ or previous version) to detect breaking changes ￼ ￼. It ensures implementations (code or tests) are updated accordingly. It may also regenerate client code or types if needed. This is run in CI to enforce that any interface change is intentional and recorded (the guide mentions logging approval for contract changes in plan.md and updating doc-node-map if baseline changes) ￼.
	•	Continuous Integration (CI) Composite Commands:
	•	make dev_check: A master command that runs all standard checks and tests for the repository. As described, this would include formatters (code lint), unit tests, the documentation and graph consistency checks (route_lint, registry_gen –check, capability_index_check, registry_check, agent_lint, etc.), and maybe security scans ￼. The AI is expected to run make dev_check before any commit and certainly before opening a PR, and paste the results in the workdoc QA section ￼. Only if all sub-checks pass can it proceed to create a PR.
	•	make registry_gen: A utility to regenerate the README route table (and possibly other index files) from the source of truth (registries). With --check, it just verifies the current README table matches the expected output ￼. This helps catch if someone updated a routing file but forgot to reflect it in the README.
	•	make guardrail_check: Similar to trigger_check, runs guardrail_runner.py --dry-run all to simulate all guardrail enforcement scenarios and ensure the automated approval pipeline is sound ￼. It would confirm that for each enforcement: block trigger, the required commands succeed and the system is capable of handling the approval without dead-ends. Logs to route-health.
	•	make module_health_check: (Inferred) The guide suggests possibly having module-specific test runs ￼. This target could run tests or health checks for a specific module (e.g. its unit tests, or verifying its config overrides) to ensure a module’s integrity before integration.
	•	Periodic Cron/CI tasks: Weekly or periodic tasks like:
	•	Running make trigger_check and make guardrail_check to produce updated telemetry.
	•	Exporting snapshots of trigger-map.yaml to check for consistency manually ￼.
	•	Checking for any “孤立” (orphaned) docs or nodes not covered by any route (perhaps part of route lint already).
	•	Config Lookup Utility (python scripts/config_lookup.py and make config_show): Usage: When the AI needs to retrieve a configuration value or confirm a setting. Function: Given a key (like llm.default.model), this script looks it up in a centralized config index (perhaps modules/config/config-index.yaml) and returns the value or the path to the config doc ￼ ￼. The AI can use this to quickly answer “which model are we using by default” or “what’s the database host for staging” without trawling through config files. The output is meant to be pasted into a workdoc for reference ￼.
	•	Visualization and Reporting Tools:
	•	The guide mentions a make trigger_visualizer to output a diagram of the agent graph or trigger map ￼. Implementing this might involve a script that reads agent-graph.yaml and produces a Mermaid or Graphviz diagram for documentation purposes (perhaps inserted into doc_agent/orchestration/routing.md as a reference).
	•	Similarly, after all setup, generating an Agent Index or summary (maybe a Markdown table of agents and their roles) could be useful, though not explicitly mentioned, it could be derived from registry.yaml for documentation.
	•	If needed, a Telemetry Dashboard script could compile the route-health metrics into charts or more digestible reports, but that may be beyond initial scope and left for future.

Each of these functions corresponds to a need identified in the guide: ensuring consistency across docs and code (linting and syncing scripts), enabling triggered automation (trigger runner, guardrail runner), supporting context management (scaffold tasks, context trackers), and enforcing quality and safety gates (tests, approvals, contract checks). In the next step, we will consider naming conventions and see if any of these can be consolidated. For example, some scripts might be combined or some Make targets may wrap multiple Python scripts – we will rationalize that structure. At this stage, however, no functionality appears redundant; each serves a distinct purpose in the overall system.

4. Naming Conventions and Document Classification

It is crucial that the document and function names we choose adhere to the naming conventions outlined in the guide, to avoid confusion for the AI. We will also categorize the documents into routing vs. content docs and ensure the progressive disclosure principle is applied, splitting content into smaller pieces when necessary.

Compliance with Naming Conventions:
	•	Document Names: The guide specifies certain standard names:
	•	Routing files must be named ROUTING.md (all caps) in each directory ￼.
	•	Agent policy files must be AGENTS.md (all caps) ￼.
	•	Capability indexes must be CAPABILITIES.md ￼.
	•	Other special docs like README.md and CONTRACT.md are also uppercase (the guide itself uses CONTRACT.md in examples ￼).
	•	For more free-form guides or quickstarts, the names should reflect their role (and likely include the word “guide” or “quickstart”). The guide’s YAML example uses quality-quickstart.md in lowercase with hyphen ￼, which suggests that quickstart guides can be named descriptively (lowercase, hyphen-separated if multi-word).
	•	We will follow this pattern: if a doc has a specific role (Router, Agents, Capabilities, Contract, Changelog etc.), use the capitalized conventional name. If it’s a general content doc (guide, quickstart, spec), use lowercase descriptive names, possibly suffixed by role (e.g., -guide.md, -spec.md, -runbook.md) for clarity. Each such doc’s front matter will include a doc_role matching its purpose (e.g., doc_role: guide or doc_role: runbook).
	•	Module directories are named with numeric prefixes (e.g., 1_Assign/, 3_SelectMethod/) to convey hierarchy ￼. Module instance directories are prefixed with M_ or some clear label (the example uses M_AI_Select, M_Manual_Select) ￼. We’ll adopt the same naming scheme for module instances (perhaps M_<Name> to denote a module implementation).
	•	Graph Node IDs: Although not file names, it’s worth noting that any IDs for agents or capabilities should follow domain-capability[-module] format ￼. We will ensure any new ID we introduce respects this (e.g., an agent for documentation might be docops-review, a capability for DB migration could be data-migrate).
	•	Function/Script Names: The guide mostly provides explicit names for the required scripts, which we will stick to:
	•	Use snake_case for Python scripts (as in doc_route_check.py, agent_graph_check.py).
	•	Make targets are typically lowercase and hyphenated if needed (e.g. make registry_gen --check). We might use underscores in Make targets if needed, but the given examples use either plain or hyphen (e.g., dev_check, trigger_check).
	•	No function names conflict with reserved words or each other as listed; each describes its action. If any do overlap in functionality (like route_lint vs doc_route_check), we’ll clarify by possibly aliasing one to the other. For instance, make route_lint target can simply call the doc_route_check.py script – that way the naming is consistent (the AI calls a high-level “lint” command, but under the hood it’s using the check script).
	•	The naming of module scaffolding command ai_begin is fine, but we might clarify it as make module_init for readability. However, since the guide explicitly gave ai_begin, we’ll adhere to that to avoid confusion ￼.

After comparing our doc and function lists with naming standards, everything aligns well. The documents we listed follow the patterns given (we may adjust minor things like pluralization or singular as needed). One thing to ensure: the front matter of each doc declares its doc_kind and doc_role properly. For example, all routing docs have doc_kind: router and perhaps a route_role (like root vs intermediate vs leaf), agent policies have doc_kind: agent_policy, capabilities indexes have doc_kind: capability_index, and so forth ￼ ￼. We will prepare templates for these front matters so that the AI can fill them consistently.

Routing vs. Content Docs:

From the list in section 2, we can classify:
	•	Routing Docs: These include any ROUTING.md at various levels (root, doc_agent if used, modules root, module instance if needed, config directory, etc.). They exist solely to direct to other docs. They will not contain large content or detailed explanations – only minimal context and links (scope, topics, whens). According to the guide, routing docs should be very lightweight (just enough to route) ￼, and avoid duplicating content from leaf docs. We will ensure any information that belongs in a guide or spec is not mistakenly put in a routing doc.
	•	Agent Policy Docs: (AGENTS.md files). These are content docs in that they carry important rules, but they are specialized content – essentially part of the “control” layer docs for the AI. Each AGENTS.md is meant for the AI to read fully when loaded (though still ≤200 lines) ￼. They do not contain routing info themselves ￼. We ensure that anything in agent policies is truly about policy and not general guidance or design (those belong in guides/specs).
	•	Capability Index Docs: (CAPABILITIES.md files). Also content docs (catalogs), structured for AI reading. They list abilities but do not route elsewhere except referencing related docs for implementation details. We will keep these within ~250 lines as specified ￼.
	•	Explanatory Guides/Specs: These are pure content meant to help the AI (or human) perform tasks: e.g., quickstart guides, runbooks, design specs, policy documents. They should each cover a single focused area and follow the progressive disclosure principle:
	•	If a guide becomes too long or covers multiple scenarios, split it. For example, if we had a single “Testing & Quality Guide” that included both how to write tests and how to run CI, it might be better to split into “Testing Guide” and “CI Pipeline Guide” to keep each under the recommended length. The guide explicitly says to avoid long monolithic docs and prefer layered detail ￼.
	•	The “渐进式原则” means higher-level docs should summarize and point deeper rather than contain everything ￼ ￼. We have applied this in our plan: e.g., a Quickstart (hub) doc gives an overview and links to plan, tasks, runbook, etc., rather than repeating their contents ￼ ￼. Another example: the root routing will point to a “Database governance spec” under Governance scope; that spec might itself be split into a high-level policy vs a technical spec (one in doc_agent/policies/ for rules, one in doc_agent/specs/ for detailed schema format) to keep each piece focused.
	•	Leaf docs should remain leafs: If a leaf doc (like a guide) starts to have subsections that are themselves deep, we can create sub-guides. For instance, if Deployment Guide had separate steps for Docker vs serverless, we might have two sub-guides and the main Deployment Guide links to them. This way the AI only loads the part it needs (“when deploying to Docker, read docker-deploy.md”).
	•	Our current document list looks granular enough – but we will double-check if any item potentially violates the gradual principle:
	•	The Documentation Standards spec might be lengthy because it has many rules. If it exceeds ~150 lines, we could split it (e.g., one doc for “Front Matter Schema” and another for “Writing Style Guide”). However, since this is primarily for the AI’s reference, we can also keep it as one structured document with sections.
	•	The Module type contract vs module instance contract: We should ensure that module instances don’t have to load a giant type contract doc if not needed. Possibly, the module’s AGENTS.md or quickstart could summarize relevant pieces of the type contract so the AI doesn’t have to read the entire type spec every time. The routing can help: e.g., when working in a module, the routing might direct the AI to the instance’s own docs primarily; the type-level contract could be loaded only “when designing a new instance” or when explicitly checking cross-instance compatibility.
	•	The Operations guide in scripts/ might become large if it covers all script categories. But the plan is to have a main index and then category-specific READMEs. This is already a progressive breakdown (one guide per script domain), which is good.
	•	Progressive Disclosure in Routing: In designing the routing (next section), we will ensure that:
	•	The root ROUTING.md contains only one layer of detail (point to either next-level routing or key leaf docs). It should not list every single document in the repo, only group them by topic. The guide advises keeping top-level routing very concise ￼ ￼.
	•	If a directory has only leaf docs and no subdirectories, the parent routing can point directly to those leafs (with appropriate descriptions). If a directory has substructure, we include an intermediate routing file.
	•	We also incorporate references to Agent Policies and Capability Indexes in the routing where needed, labeled as such. The guide says if a directory has an AGENTS.md or CAPABILITIES.md, the routing target_docs should include them (e.g., "Agent Policy": path/to/AGENTS.md, so the AI knows to load the policy first) ￼.
	•	No single route path should become too long or deeply nested. If we find more than, say, 3 hops needed frequently, we might simplify by adding a direct link. The guide suggests if a routing chain becomes overly long or repetitive, restructure it ￼ ￼. We’ll watch for that.

At this stage, our document list already reflects a modular breakdown aligned with the progressive principle. Each doc serves a specific role. For instance, workdocs hold transient info separate from permanent guides (so that the main docs don’t get bloated with ephemeral notes) ￼. Similarly, splitting AI vs human docs avoids mixing audiences and keeps AI’s context lean.

One specific application of the progressive approach is the module Quickstart hub document. We will implement it such that:
	•	The module’s ROUTING.md or parent routing will point to the module’s Quickstart (or README) as the first stop.
	•	That Quickstart (if implemented as a workdoc or as a doc with route_role: hub) will then guide the AI to load only what it needs next (e.g., “open the plan, tasks, etc. as needed”) ￼. This prevents the AI from loading all module docs at once.
	•	We ensure, as the guide says, not to expose all leaf docs directly in the routing, but rather chain them: e.g., do not list plan.md, tasks.md, contract.md all in the modules routing; instead, have Quickstart or Agent Policy as entry, and inside those or via quickstart provide links to the others ￼. This way the AI only loads deeper details when required.

In summary, we have verified that our document naming aligns with the conventions (uppercase for key index docs, meaningful names for guides/specs, consistent prefixes for modules). We have categorized which docs are for routing (control navigation) and which are for knowledge content. We also identified opportunities to apply the 渐进式 (gradual reading) principle, which we will implement in the routing structure and by possibly creating intermediate “hub” docs (like quickstarts) to avoid listing too many leaves in one place. So far, it appears our plan meets the naming and structural guidelines well.

5. Routing Structure Design (Scope → Topic → When)

Using the information from the guide (especially the functional requirements in section 2. 文档路由体系), we can now outline the complete documentation routing tree for the template repository. The routing uses a three-level structure – Scope, Topic, When – to help the AI decide where to go for information ￼ ￼. We will present the routing in a hierarchical format, indicating for each route node: the scope (if applicable), the topic, the when (scenario description), and the target documents to load. (For brevity, we might not list every single “when” sub-bullet from the earlier list if they are numerous, but we will capture the intended structure comprehensively.)

Top-Level (Root) ROUTING.md:

At the root, we define broad Scopes that cover the major areas of concern. The guide suggests scopes akin to “Execution”, “Governance”, “Systems” ￼ ￼ (matching the functional groups Development Execution, Governance & Security, System Assets). We will use those as our first-level divisions. Each scope contains several Topics, which in turn have one or more When entries.

``` yaml
context_routes:
  - scope: Execution – Project Development  
    topics:
      - name: Project Initialization  
        when:
          - description: "Starting a new project repository from scratch, initial setup & dependencies."  
            target_docs:
              - path: doc_agent/quickstart/project-init-guide.md   # Guide for initial repo setup (install, structure)
              - path: doc_agent/AGENTS.md                          # Orchestrator policy (to ensure safe initialization)
      - name: Module Development  
        when:
          - description: "Creating a new module via scaffolding."  
            target_docs:
              - path: modules/ROUTING.md   # Jump to modules routing to select specific module type/instance
              - path: doc_agent/quickstart/module-scaffold.md   # Quickstart for module scaffolding (checks to do)
          - description: "Implementing back-end logic for a module instance."  
            target_docs:
              - path: modules/ROUTING.md   # Will route to that module's dev guide
              - path: doc_agent/quickstart/backend-guide.md     # General backend development guide (if any)
          - description: "Implementing front-end interface for a module instance."  
            target_docs:
              - path: modules/ROUTING.md
              - path: doc_agent/quickstart/frontend-guide.md    # General frontend dev guide (if any)
          - description: "Working on cross-tier core logic or algorithms in a module."  
            target_docs:
              - path: modules/ROUTING.md
              - path: doc_agent/guide/core-logic-spec.md        # Possibly a generic spec for core logic patterns
      - name: Workflow & Prompts  
        when:
          - description: "Managing automated workflows or prompt templates across the project."  
            target_docs:
              - path: doc_agent/quickstart/workflow-quickstart.md   # Guide on writing standard workflows and prompts
              - path: modules/config/prompts/README.md              # (If exists) Info on prompt repository/format
      - name: Testing & Quality  
        when:
          - description: "Running tests and quality checks before committing code (CI pipeline)."  
            target_docs:
              - path: doc_agent/quickstart/quality-quickstart.md    # Quickstart for running linters/tests [oai_citation:193‡file-1x5fteqchrxp7nsowu1pst](file://file-1x5fTEqChRxP7NSowu1pSt#:~:text=context_routes%3A%20,path%3A%20%2Fscripts%2FREADME.md)
              - path: scripts/README.md                             # Scripts overview (for commands like make dev_check) [oai_citation:194‡file-1x5fteqchrxp7nsowu1pst](file://file-1x5fTEqChRxP7NSowu1pSt#:~:text=when%3A%20,path%3A%20%2Fscripts%2FREADME.md)
          - description: "Writing new tests or ensuring coverage meets requirements."  
            target_docs:
              - path: doc_agent/guide/testing-guide.md              # Guide on writing tests, coverage standards
      - name: AI Deployment  
        when:
          - description: "Deploying or managing the AI inference/service in production."  
            target_docs:
              - path: doc_agent/quickstart/ai-service-runbook.md    # Runbook for AI service deployment & monitoring
  - scope: Governance – Policies & Maintenance  
    topics:
      - name: Agent Orchestration Rules  
        when:
          - description: "Coordinating AI agents at the root level (orchestrator strategies)."  
            target_docs:
              - path: doc_agent/AGENTS.md                    # Orchestrator agent policy (roles & rules)
              - path: doc_agent/policies/orchestrator.md     # (If separate) Orchestrator-specific guidelines
      - name: Guardrail & Triggers  
        when:
          - description: "Enforcing guardrails or reviewing trigger configurations."  
            target_docs:
              - path: doc_agent/policies/guardrail.md        # Guardrail policy doc (block/allow rules, unblocking process)
              - path: doc_agent/AGENTS.md#Trigger Handling   # Section of root agent policy about triggers (if needed)
              - path: doc_agent/orchestration/agent-triggers.yaml   # (AI can read YAML if allowed, or a generated summary)
      - name: API & Contract Management  
        when:
          - description: "Changing an interface contract or checking compatibility baselines."  
            target_docs:
              - path: modules/api/CONTRACT.md                # (If global API spec exists) or relevant module CONTRACT
              - path: doc_agent/specs/API_SPEC.md            # API governance spec (OpenAPI guidelines, baseline)
              - path: doc_agent/policies/api-compatibility.md   # Policy on maintaining backward compatibility
      - name: Database Change Control  
        when:
          - description: "Modifying database schema or performing a migration."  
            target_docs:
              - path: db/engines/postgres/docs/DB_SPEC.md    # Database specification (schema modeling guidelines) [oai_citation:195‡file-1x5fteqchrxp7nsowu1pst](file://file-1x5fTEqChRxP7NSowu1pSt#:~:text=6.10%20%E9%AA%8C%E6%94%B6%E6%8C%87%E6%A0%87%20,%E5%9C%A8%20CI)
              - path: doc_agent/policies/db-change.md        # DB change policy (approval, rollback requirements)
              - path: db/engines/postgres/ROUTING.md         # Route into DB directory for specific migration docs (if any)
      - name: Configuration Management  
        when:
          - description: "Updating configurations or switching environment settings."  
            target_docs:
              - path: modules/config/ROUTING.md              # Routing into config directory (with topics for each config area) [oai_citation:196‡file-1x5fteqchrxp7nsowu1pst](file://file-1x5fTEqChRxP7NSowu1pSt#:~:text=,DB%20%E8%BF%9E%E6%8E%A5%E3%80%81%E8%BF%81%E7%A7%BB%E7%AD%96%E7%95%A5)
              - path: doc_agent/policies/config-governance.md   # Config governance guide (naming, secrets handling) [oai_citation:197‡file-1x5fteqchrxp7nsowu1pst](file://file-1x5fTEqChRxP7NSowu1pSt#:~:text=%E8%AE%B0%E5%BD%95%E5%91%BD%E4%BB%A4%E4%B8%8E%E5%AE%A1%E6%89%B9%E7%BB%93%E6%9E%9C%E3%80%82%20)
  - scope: Systems – Assets & Automation  
    topics:
      - name: Data Flow & Performance  
        when:
          - description: "Tracing data flow or analyzing performance bottlenecks."  
            target_docs:
              - path: doc_agent/guide/performance-guide.md   # Guide on using telemetry and analyzing bottlenecks
              - path: ai/maintenance_reports/route-health.md # Latest route health report (for live metrics)
      - name: Context & Knowledge Base  
        when:
          - description: "Reviewing work logs, error records, or updating the knowledge base."  
            target_docs:
              - path: ai/workdocs/active/                    # (The AI may open the relevant active context for current tasks)
              - path: doc_agent/guide/workdocs-guide.md      # Guide on how workdocs are structured (for AI understanding)
              - path: ai/maintenance_reports/retrospective.md   # Repository-level retrospective log of lessons
      - name: Scripts & Automation Tools  
        when:
          - description: "Using or modifying automation scripts or Make commands."  
            target_docs:
              - path: scripts/operations-guide.md            # Overview of the scripts directory and conventions [oai_citation:198‡file-1x5fteqchrxp7nsowu1pst](file://file-1x5fTEqChRxP7NSowu1pSt#:~:text=registry_gen%20,triggers.yaml%60%20%E7%9A%84%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84%EF%BC%8C%E5%AE%9A%E4%B9%89)
              - path: scripts/guardrail/README.md            # (For example) Specific README for guardrail scripts
      - name: Documentation Standards  
        when:
          - description: "Adding or updating documentation and routing indexes."  
            target_docs:
              - path: doc_agent/guide/documentation-spec.md  # The documentation format & routing maintenance spec
              - path: doc_agent/orchestration/doc-node-map.yaml   # Machine-readable doc-node map (for reference in checks)

```

(Note: The above is a conceptual YAML-like outline. The actual ROUTING.md will include proper YAML front matter and perhaps additional details like route_role for leaf nodes, etc. Paths starting with doc_agent/ or modules/ are assumed to be relative to repository root.)

A few explanations of the routing choices made:
•	We have three scopes at root: Execution, Governance, Systems. This aligns with the guide’s categories ￼ ￼. We labeled them in English here (with Chinese in comments if needed in actual doc for clarity), but the actual content could be bilingual or Chinese as per project language requirements. English is likely preferred for consistency with code.
•	Under Execution, topics correspond to phases of development:
	•	Project Initialization: setting up the whole project initially (one-time per repo).
	•	Module Development: creating or editing modules. We combined several sub-cases under this topic with different “when” descriptions:
	•	New module scaffolding, backend dev, frontend dev, core logic – each was mentioned in the functional list ￼. By grouping them under Module Development, the AI first identifies that it’s working on a module, then sees specific scenarios. Each scenario points to the modules routing (to select the correct module docs) and possibly a generic guide for that kind of task. (We may refine this: alternatively, each could be its own topic to reduce branching, but then they all are modules – so grouping by “Module Dev” scope seems logical at this level.)
	•	Workflow & Prompts: covers tasks related to automated workflows or prompt engineering, from the functional point “工作流模式与提示词模板” ￼.
	•	Testing & Quality: covers the “测试与质量” item ￼. We gave two when-cases: one for executing tests/CI (pre-commit), another for writing tests (development time).
	•	AI Deployment: corresponds to “AI运行/推理服务（智能体）” ￼, i.e., deploying and maintaining the AI/agents as a service.
•	Under Governance, topics correspond to policies and rules:
	•	Agent Orchestration Rules: maps to “智能体编排规则” ￼. We direct to orchestrator policy and any additional orchestrator guidelines.
	•	Guardrail & Triggers: maps to “Guardrail机制与触发策略” ￼. We link the guardrail policy doc, and possibly the triggers config if the AI needs to check a trigger entry (the AI normally wouldn’t read raw YAML unless guided, but if we have a generated summary, that might be better).
	•	API & Contract Maintenance: maps to “接口/契约维护” ￼. We include the relevant contract docs and policies.
	•	Database Change Control: maps to “数据库操作与变更管控” ￼. We link the DB spec and policy.
	•	Configuration Management: maps to “配置管理与环境切换” ￼. We route into the config directory (which itself may have a routing listing keys or sub-configs) and a config governance guide.
•	Under Systems, topics correspond to project assets and operational tools:
	•	Data Flow & Performance: maps to “数据流向监控与性能分析” ￼. We link a performance analysis guide and the telemetry report.
	•	Context & Knowledge Base: maps to “上下文资产” (workdocs, error records, etc.) ￼. This directs the AI how to retrieve context logs or lessons learned. Instead of a single static doc, it might just instruct to open the relevant dynamic files.
	•	Scripts & Automation Tools: maps to “脚本与自动化工具” ￼. Points to the scripts documentation.
	•	Documentation Standards & Index: maps to “文档规范与路由索引” ￼. We link the spec for documentation standards and the doc-node map or any registry that helps maintain the index.

This root routing structure ensures that for any high-level task, the AI can identify the correct scope first, then the topic, then the scenario. Each when description is phrased in a trigger-like manner (“When you are doing X, you need Y”), as recommended ￼. We also included references to actual file paths (which will be relative in the Markdown). The target_docs often include two entries:
•	One typically points to a guide or policy doc for that scenario (the actual knowledge to read).
•	Another might point to a routing or index if the next step is to navigate further (e.g. going into modules/ROUTING.md or into db/ROUTING.md).
•	In some cases, we point directly to a maintenance or dynamic file (like route-health report or workdocs) when the scenario is about reviewing those. The AI’s tooling should allow reading those as needed (keeping in mind that, e.g., reading a live route-health.md might be considered an action rather than static knowledge, but since it’s just a Markdown report, it’s fine as read-only input).

Sub-Routings:
	•	Modules/ROUTING.md: In the modules directory, the routing will enumerate module types and instances. For example:

  ``` yaml 
context_routes:
  - scope: Modules – Business Workflow  
    topics:
      - name: 1_Assign (Assignment Stage)
        when:
          - description: "Working on modules in the Assignment stage of workflow."
            target_docs:
              - path: modules/1_Assign/ROUTING.md   # further routing into that type
      - name: 2_Review (Review Stage)
        when:
          - description: "Working on modules in the Review stage (approval process)."
            target_docs:
              - path: modules/2_Review/ROUTING.md
      ...
  ```

  Each module type directory (1_Assign/, 2_Review/, etc.) could have its own ROUTING if there are multiple sub-types or instances. If a type contains only instances directly, the type’s ROUTING might point to each instance’s doc. Or we might skip an intermediate if unnecessary (the guide says if a directory only has leaves, you can let parent route directly to them ￼).

For instance, if modules/1_Assign/ directly contains instances like M_AI_Select/ and M_Manual_Select/, then modules/1_Assign/ROUTING.md would list those two as topics or whens (or just target_docs since they are leaves). Alternatively, modules/ROUTING.md could jump straight to those instance quickstarts under a single topic if the type has few instances. We’ll design this with actual module structure in mind; but we ensure at least one routing layer in modules to adhere to the system (makes it easier to add more instances or types later without changing root routing).

•	Config/ROUTING.md: In modules/config/, we will also have a routing file as given in the example structure ￼. Likely it will have topics for each config category (db, llm, telemetry, parameters, prompts, feature_flags, etc.), each pointing to either a subdirectory’s README or spec. For example:

  ``` yaml
context_routes:
  - scope: Global Configurations
    topics:
      - name: Database
        when:
          - description: "Retrieving or updating database connection and migration settings."
            target_docs:
              - path: modules/config/db/README.md
      - name: LLM API
        when:
          - description: "Configuring Large Language Model API keys or parameters."
            target_docs:
              - path: modules/config/llm/README.md
      ... etc.
  ```
Additionally, since modules/config/ has an AGENTS.md (for config change policies) and CAPABILITIES.md (for config generation/validation tools) as per the guide ￼, the config ROUTING.md would include entries to those as well. E.g.:

``` yaml
          target_docs:
            - path: modules/config/AGENTS.md   # Config change policy (Agent Policy)
            - path: modules/config/CAPABILITIES.md   # Config capabilities (capability index)
            - path: modules/config/db/README.md   # etc.
```
The presence of AGENTS.md and CAPABILITIES.md in modules/config/ will be registered as “Agent Policy” and “Capability Index” in the routing above them (perhaps in root ROUTING under Governance -> Configuration, as we did by linking config-governance guide and modules/config/ROUTING).
•	Other Sub-Routes: Similarly, if we create specific subdirectories for domain-specific docs (like doc_agent/specs/ for detailed specs such as DB_SPEC.yaml, API_SPEC.yaml), they might not need their own routing file if they contain single documents. Instead, they are referenced directly by higher-level routing (as we did above for DB spec under Governance). If a subdirectory had multiple spec docs, we could add a simple routing in doc_agent/specs/ROUTING.md to list them by topic (e.g., Database Spec vs API Spec).
•	Leaf Nodes and Progressive Links: We ensure leaves do not have their own context_routes. As specified, leaf content docs rely on their front matter and structured content to guide further reading if needed ￼ ￼. For instance, a guide might say “see also X” with a link, but it won’t have a YAML context_routes. Only routing docs maintain that field.
•	Including Agent Policy and Capabilities in Routing: We will follow the guideline to include references to AGENTS.md and CAPABILITIES.md in the routing target_docs where relevant ￼ ￼. Concretely:
	•	In root ROUTING, since doc_agent/AGENTS.md exists, we might put it under a Governance topic (“Agent Orchestration Rules”) as above.
	•	In modules routing, for each instance, we might not list AGENTS.md directly at root, but the instance’s quickstart or routing might lead the AI to read its AGENTS.md as a required step. Alternatively, when the orchestrator routes to a module (via modules/ROUTING), the module’s own ROUTING.md can include:

  ``` yaml
when:
  - description: "Onboarding or troubleshooting this module."
    target_docs:
      - path: modules/<instance>/doc/Quickstart.md   # if Quickstart doc exists
      - path: modules/<instance>/doc/AGENTS.md       # Agent Policy for module (read for any tasks)
      - path: modules/<instance>/doc/CAPABILITIES.md # Capability Index for module (tools available)
  ```
  
  That way, the AI loads the module’s policy and capability index early when it delves into a module’s docs, as recommended by the guide (to enforce local guardrails and understand available actions) ￼.

	•	In the config routing example, we showed how to include AGENTS.md and CAPABILITIES.md.

•	Avoiding Overlong Chains: We will monitor if any path (Scope > Topic > When) becomes too long (they generally are 3 levels as designed). If we find ourselves adding a 4th level (like sub-when or requiring the AI to hop too many times), we might collapse one level. For example, module routing could possibly be integrated such that the root Execution scope could point directly to a module’s quickstart if we know which module (though that defeats dynamic selection). The current approach is balanced. Also, we provided multiple target_docs in single when entries in some cases – this is allowed and helps the AI load multiple needed references in one context if necessary ￼ (e.g., loading a policy doc and a guide together for context). We labeled clearly which is which (via comments or using the front matter roles in those docs).

This routing design should fulfill the requirement of helping the AI navigate to the correct documents quickly and only load what is necessary for the task at hand. It implements the scope/topic/when structure consistently, with descriptive phrasing for the “when” conditions (including triggers or commands where appropriate, like “preparing to run make dev_check” in the Testing entry, which aligns with the guide’s suggestion to mention specific triggers in descriptions ￼). We will, of course, refine the exact wording and ensure it matches the actual file paths once those docs are in place, but the above gives the full blueprint.

Finally, we note that maintaining this routing structure is an ongoing task. The guide calls for cross-checking routing whenever docs are updated (e.g., if a leaf doc’s purpose changes, update the routing accordingly) ￼. We will set up the automated checks (doc_node_map and route_lint) to enforce this consistency regularly.


6. Integrated Functional “Agent” Design

With the base functionalities listed (section 3), we now consider how to integrate related functions into higher-level “capabilities” or agents. Rather than having the AI call dozens of low-level scripts independently, the system should present cohesive functional units. The guide describes a two-tier ability system: Base Capabilities (atomic scripts/commands) and Composite Agent Abilities (which encapsulate logic and guardrails) ￼. Here we outline how we will bundle the base functions into logical agents or tool groupings:
	•	DocOps Agent (Documentation Operations): This is a domain agent responsible for documentation consistency and governance (often referred to as “DocOps”). It will incorporate several base capabilities:
	•	Running documentation lint and consistency checks: it will use doc_route_check (route lint) and doc_node_map_sync –report to verify routing docs ￼.
	•	Updating or validating the doc-node mapping: possibly an automated action of doc_node_map_sync if triggered.
	•	Ensuring all documentation front matter is correct (could call agent_lint under its domain).
	•	Possibly updating indexes (the registry_gen to refresh README route table could be under its purview).
	•	This agent acts when documentation changes are made or periodically as a “documentation auditor”. In the agent graph, it might be a planner or evaluator node (as in the example snippet where a docops planner node was shown that uses route_lint and doc_index capabilities) ￼ ￼.
	•	Integration plan: We will have doc_agent/orchestration/registry.yaml register a docops agent node (kind: planner) with capabilities tags like route_lint, doc_index mapping to the underlying scripts ￼. The DocOps agent’s strategy (in its AGENTS.md) will say it is allowed to run those checks and must do so whenever docs/routes change. This bundling means the orchestrator can simply trigger “DocOps agent” for any doc-related tasks, rather than invoking multiple separate checks.
	•	Guardrail Agent: A domain agent focused on approvals, high-risk actions, and policy enforcement.
	•	It encapsulates guardrail_runner.py operations. Instead of the AI directly calling guardrail_runner, the orchestrator will hand off to the Guardrail agent node when a block trigger is encountered.
	•	Base capabilities under it include the required safety checks commands (like db_lint, trigger_check as needed) – these would be listed in its tools_allowed and capabilities in CAPABILITIES.md (for example, a capability “run_required_commands” that sequentially runs a set of Make commands, or simply enumerating them).
	•	The Guardrail agent’s policy (AGENTS.md) defines how it handles escalation: e.g., it might have a rule “if approval needed, create a tasks entry and pause” which is logic the AI follows as described in 10.4 ￼ ￼.
	•	This agent likely has a human_in_loop nature (since it waits for approval), and so it could be modeled as such in the graph (kind: human_in_loop or similar for that node).
	•	Integration: In the graph (agent-graph.yaml), the guardrail agent node would be connected from the orchestrator for any blocked event. agent-triggers.yaml entries with enforcement: block will specify target_agent: guardrail or similar, causing orchestrator to delegate to this agent ￼ ￼. The guardrail agent then calls the base capabilities needed (via guardrail_runner or directly running required commands). We will ensure the capabilities.yaml has entries for those underlying steps, and the Guardrail agent in registry.yaml lists them in provides.
	•	Workflow & Telemetry Agent: This agent handles cross-cutting workflow maintenance and telemetry feedback.
	•	It might be called “WorkflowOrchestrator” or “TelemetryAgent”. Its role is to do things like:
	•	Task tracking and context logging: ensuring after each step, logs are updated. (Though much of that is handled by hooks in code, the agent might ensure it or fix if something missing.)
	•	Telemetry analysis: calling context_usage_tracker.py and ai_chain_optimizer.py. For example, after a development cycle or on a schedule, orchestrator might trigger this agent to analyze performance data.
	•	Possibly automating repetitive tasks: The “Workflow” part might manage recurring jobs, like ensuring triggers run weekly checks (though triggers have their own mechanism, but the agent might coordinate scheduling).
	•	Base capabilities include: context_usage_report (running the tracker and summarizing results), optimize_suggestions (running chain optimizer). Also, maybe interacting with handoff-history.md or archiving tasks (ensuring closed tasks are archived properly).
	•	Integration: Register a telemetry agent node (or call it workflow_agent) with kind: evaluator (since it evaluates system performance) or kind: service. This agent could be invoked at certain points (like end of a milestone or via a trigger like “weekly_maintenance”) to produce reports. We’ll list its provided capabilities (like metrics gathering) in CAPABILITIES.md. In the routing, if the AI needs to do telemetry tasks, it will be guided to this agent’s docs (e.g., our Systems scope had “Data Flow & Performance” topic which could route to this agent’s policy or quickstart).
	•	The policy for this agent will ensure it doesn’t flood tokens (maybe limiting how much data it reads from logs at once) and that it records its findings in the proper place (maintenance reports).
	•	Data & Schema Agent: This agent domain covers database and data lifecycle tasks.
	•	It will integrate DB schema checks, migration orchestration, and cache (Redis) lifecycle operations as described in section 6 of the guide. The guide explicitly calls for a “Data CRUD agent” and a “Redis Lifecycle agent” as part of the template ￼. We might either have two separate agents or combine them under one Data & Schema domain with two sub-nodes.
	•	Capabilities include:
	•	Running db_lint, migrate_check, rollback_check (all those Make commands) ￼.
	•	Possibly generating migration scaffolds or checking schema YAML vs actual DB (script not listed but could be extended).
	•	Managing Redis keys expiration (the guide hints at Redis TTL lifecycle, likely meaning an agent that helps ensure cache consistency; specifics not given, but we could have a capability to scan for stale cache entries or enforce TTLs).
	•	Integration: Register a data_schema agent node (or separate database_agent and cache_agent if we want to split). In registry.yaml, mark it as providing capabilities like db_schema_lint, db_migration_run (for safe migration), cache_review etc. Tools allowed can be make db_lint, etc., which presumably we list.
	•	Triggers: The guide suggests a trigger for detecting new migration files that targets this agent ￼. So agent-triggers.yaml will have something like event: new_migration_file → target_agent: data_schema with required validations. This agent’s AGENTS.md will have a Trigger Handling section explaining what to do (run lint, require approval if something is risky like a destructive migration).
	•	By bundling these in an agent, whenever the AI needs to do a DB operation, orchestrator will engage this agent rather than doing it blindly itself. This agent can then systematically run through checks and ensure docs are updated (like updating a DB spec or changelog if needed).
	•	API Gateway/Integration Agent: This covers API schemas and external service integration:
	•	Capabilities: Checking API contract compatibility (the type_contract_check.py, contract_compat_check usage) ￼, possibly updating client libraries, ensuring any external API keys or webhooks are handled (some overlap with config, but focusing on contract and schema).
	•	It might also handle communications to external services if any (though not explicitly covered, an “API agent” could theoretically manage calling an external documentation or test if needed).
	•	Integration: Register as api_gateway agent node with capabilities api_contract_test, openapi_sync, etc. Tools allowed: make contract_compat_check, maybe a script to update .contracts_baseline/.
	•	The routing (Execution – maybe under Execution or Governance) will route tasks like “ensure API changes are compatible” to this agent’s domain.
	•	The policy for this agent could enforce that any breaking change triggers a guardrail approval (which would loop in the Guardrail agent).
	•	Scripts Ops Agent: This could be a domain agent focusing on the CI/CD and scripting aspects:
	•	Capabilities: Running the CI suite (like make dev_check or portions of it), or intermediate build processes. The name “Scripts Ops” appears in the initial domain list (DocOps, Guardrail, Workflow & Telemetry, Data & Schema, API Gateway, Scripts Ops) ￼ ￼. This likely refers to tasks around managing build scripts, deployment scripts, etc.
	•	It might incorporate things like formatting code (if we had an autoformatter script, etc.), or ensuring the Makefile targets are up to date.
	•	Possibly also coordinate with the CI environment (though actual CI integration may be outside the scope of the AI template, the agent can simulate or enforce the same checks).
	•	Integration: A scripts_ops agent node in registry, with capabilities like run_dev_check (which calls make dev_check internally), validate_scripts (maybe a script lint or verifying all expected scripts exist).
	•	If there is any scaffolding or operations guide that needs execution (like generating documentation via scripts, or cleaning temp files), this agent would handle it.
	•	Orchestrator Agent (root): The orchestrator itself is a composite agent but we treat it specially. It doesn’t need to “provide” capabilities in the sense of tools; its capability is planning and delegation. But in the registry it is an agent with an id (maybe orchestrator), tools_allowed might be empty or just allow it to call sub-agents.
	•	Its AGENTS.md lists what it is allowed to do: mainly load docs, select agents, run top-level make commands if needed.
	•	The orchestrator uses the context_routes (from root ROUTING) to decide which domain agent to invoke or which docs to load ￼ ￼.
	•	It has a mapping in doc-node-map linking it to root topics (the guide says orchestrator’s node maps to scope->topic entries in doc-node-map) ￼.
	•	Module Agents: Each module instance effectively acts as a composite agent as well (kind of a service agent for that module’s functionality). Integration for modules:
	•	Each module’s AGENTS.md is its policy, and we register each module in the orchestrator graph (the modules/registry.yaml -> module-registry.yaml -> agent-graph.yaml flow ensures module agents appear as subgraph nodes) ￼ ￼.
	•	Modules might have internal tools (e.g., a module-specific script). Those should be listed in the module’s CAPABILITIES.md (base capabilities) and also bound to the module agent node in registry.yaml’s graph_bindings (composite provides).
	•	If modules share common tools, those tools are still declared in the global capability registry and the module agent would have a requires link to whichever agent or tool provides it (the guide mentions marking in orchestrator registry if module reuses a common ability).
	•	Modules can also call each other if needed, but that should be mediated by orchestrator or cross edges in agent-graph with conditions (with guardrails if high risk).
	•	We ensure the module agent orchestration is represented: orchestrator should route tasks with a certain module’s scope to that module’s subgraph. The orchestrator registry will have a mapping from a module’s scope or entry to the module agent node (perhaps via an entry that says if scope == that module’s name, go to that subgraph). In practice, this might be coded by giving each module a capability tag or target_agent entry.

Possibility of Merging Functions:

Looking at the functions from section 3, is there any obvious overlap we can simplify?
	•	Many of the scripts are for checking consistency. We could consider a single command that runs all of them (which is essentially make dev_check as the aggregator). That we already have – dev_check wraps formatting, tests, plus route_lint, registry checks, etc. So in usage, the AI might often just run make dev_check unless it specifically wants to run one check in isolation. This is already addressed by including dev_check as a capability (under ScriptsOps agent perhaps).
	•	The difference between doc_route_check.py and doc_node_map_sync.py --report is subtle (one verifies context_routes vs graph, the other verifies doc vs graph mapping). They serve a common goal (documentation-graph consistency). We might merge them conceptually by having make route_lint call both, so from AI’s perspective, one command covers both checks. We’ll implement that to simplify usage: the guide text even suggests running either route_lint or doc_node_map_sync –report in similar contexts ￼, meaning they likely do related things. We’ll make make route_lint depend on doc_node_map_sync’s checks.
	•	Similarly, capability_index_check and parts of registry_check both ensure no mismatch between capabilities.md/.yaml and registry/graph. We could potentially have one script do a combined verification of all cross-consistency (doc vs graph vs registry). But keeping them separate might be better for pinpointing issues (one focuses on capabilities, one on registry entries). We’ll keep them distinct but perhaps group their invocation (e.g. a single make consistency_check could call all of them). However, since dev_check calls them, that might suffice.
	•	trigger_map_sync.py and module_registry_sync.py and doc_node_map_sync.py are all “sync generated views” scripts. We might unify them into one registry_gen tool that regenerates all derived YAMLs (trigger-map, module-registry, doc-node-map, maybe agent-graph if needed from sources). In fact, make registry_gen in the guide likely calls module_registry_sync and trigger_map_sync etc., then compares outputs ￼. We will implement make registry_gen as a wrapper to run all sync scripts (with a --check mode to just verify no changes).
	•	The agents vs base dichotomy itself avoids needing to merge base functions unnecessarily: we keep base scripts simple and focused, and achieve integration by composing them in agent logic or in tools_allowed sequences. This is better than one monolithic script trying to handle multiple concerns.

Thus, our integration strategy is to encapsulate the base scripts into domain-specific agents, rather than merge scripts themselves. Each agent’s AGENTS.md will clearly list what base actions it can do (e.g., DocOps agent policy might list that it can run doc lints and update indexes, but cannot, say, push to repo directly) ￼. And each agent’s CAPABILITIES.md section will enumerate those base actions as capabilities (with references to the actual script entrypoints) ￼.

For example, CAPABILITIES.md (global) might have an entry:
``` yaml
scope: "Execution"  
capability: "Documentation Audit"  
trigger: "before committing documentation changes"  
target_nodes: ["docops"]   # points to DocOps agent node in graph
capabilities:
  - id: route_lint
    kind: tool
    entrypoint: local: "python scripts/doc_route_check.py"
    outputs: ...
  - id: doc_index
    kind: tool
    entrypoint: local: "python scripts/doc_node_map_sync.py --report"
```

And docops AGENTS.md would have graph_node_id: docops and mention that it will invoke route_lint, doc_index on triggers or as needed.

To summarize integration:
	•	We identified six major composite agents: Orchestrator (root), DocOps, Guardrail, Workflow/Telemetry, Data/Schema, API/Integration, ScriptsOps (plus N module agents).
	•	Each composite agent bundles related base capabilities (scripts, commands).
	•	The orchestrator will mostly handle choosing which agent to call, based on triggers and routing.
	•	By structuring this way, the AI’s task planning is simplified: it either does a high-level plan that involves invoking these agents, rather than micromanaging every script. The decision flow becomes: Orchestrator sees a need -> picks domain agent -> domain agent policy says “I should run these 3 steps in order” -> those steps correspond to actual tool calls.

This leverages the design from the guide that cross-agent handoffs are defined centrally (in agent-triggers and routing mapping) so agents can coordinate but not hard-code calls to each other in their local docs ￼. We will ensure our registry and triggers reflect these integration points (e.g., orchestrator’s triggers will cause a handoff to guardrail agent or module agent as appropriate, instead of orchestrator itself performing risky action).

7. Automation for Data Consistency and Progress Sync

One of the core promises of this template is that data updates (documentation, registries, etc.) and development progress remain synchronized, especially given the AI will be driving much of the development. We need to evaluate if the proposed automation (databases, APIs, scripts) is sufficient to maintain this synchronization, and how convenient it is for the AI developer to use.

Single Source of Truth (SSOT) and Registrations: The plan enforces SSOT for various aspects – e.g., modules/registry.yaml is the one truth for modules, capabilities.yaml for base abilities, registry.yaml for agents, and documentation is the truth for processes. Whenever something changes, the workflow dictates updating the SSOT first, then running scripts to propagate to derived files ￼ ￼. This approach is sound for consistency: for example, if a new module is added, the AI will:
	•	Update modules/registry.yaml with the new module’s details ￼.
	•	Run module_registry_sync.py to push that info into the orchestrator’s view (graph and registry) ￼.
	•	Run checks (doc_node_map_sync, route_lint) to ensure all references are now valid ￼.
	•	As a result, any route or agent that needs to know about the new module is updated, and the AI’s next steps (like orchestrator using that module) will not break due to missing links.

The presence of automation scripts for each sync task ensures that the AI doesn’t have to manually edit multiple places – reducing human error (or AI error). The guardrail requiring these scripts to be run (with CI checks enforcing it) means if the AI forgets to update something, a failed check will remind it (sustaining consistency).

Real-time Progress Logging: The use of workdocs (plan, context, tasks) for tracking progress means that as the AI executes automation, it also updates these logs. The guide instructs the AI to record every command run (with purpose and outcome) in context.md#Automation and mark tasks as done in tasks.md ￼ ￼. This practice ensures the development progress is documented step-by-step. If something is out of sync (e.g., a command was run but tasks.md not updated), the structure makes it evident (tasks will show as pending). Also triggers like the weekly trigger check script will compare agent-triggers with tasks to see if any scheduled items were not done ￼ ￼, thereby catching any missed progress updates (like an auto task not being logged).

Convenience for AI Developer:
	•	The AI has a clear place to write what it did (workdocs) and a clear procedure to update meta-data (via the provided scripts). This spares the AI from editing multiple files manually, which could be error-prone.
	•	For instance, adding a new capability: The instructions are to update capabilities.yaml & CAPABILITIES.md, run capability_index_check, then update orchestrator registry and run sync scripts, etc. ￼ ￼. It’s a multi-step process, but each step is well-defined and tools are provided. The chain is somewhat complex, but since it’s codified, the AI can learn the routine (maybe even have a checklist in the Agent Policy as suggested ￼).
	•	The trigger mechanism also adds convenience: If the AI forgets to do something, triggers might catch it. For example, if the AI adds a migration SQL file and forgets to update the schema YAML or run the checks, a trigger “on new migration file” can prompt it to do so, or even automatically create a task “Review migration and update schema” ￼. This safety net helps maintain consistency without relying solely on AI memory.

Data Updates vs. Dev Progress: The interplay is:
	•	Whenever code is changed, relevant docs should change. The guardrails enforce this: e.g., if API code changes, and CONTRACT.md not updated, that was given as an error example (ERROR-001) ￼. The solution was to create a lesson and a trigger (contract-change guardrail) to ensure next time such a change happens, the AI is reminded to update the contract doc ￼. This is a great example of sustaining sync: a mistake led to a guardrail that will automatically prompt the doc update in the future.
	•	The use of front matter linking (graph_node_id, etc.) also ties docs to code: if the code (graph) changes, those IDs might need to change. The check scripts will catch mismatches and force updates to docs or code until they align ￼.
	•	The AI has quick access to these meta-data through simple commands (like make config_show for config values, rather than opening config files manually). This saves time and ensures it’s reading the authoritative source (if config-index.yaml is SSOT for config, using a lookup script ensures it doesn’t miss an override).

In summary, the architecture is designed for tight synchronization:
	•	Proactive sync: The scaffolding and registration processes ensure new additions are registered everywhere upfront (no deferred “I’ll document later” – it’s part of creation).
	•	Reactive sync: Triggers and guardrails catch any drift (like doc not updated, contract mismatch).
	•	Continuous integration: The CI checks (route_lint, registry_check, etc.) are an automated safety net that halts progress until consistency is restored ￼ ￼.
	•	Ease of use: The AI developer mostly interacts with high-level commands (make targets) or single-source YAML files rather than editing many files. This lowers cognitive load and potential for mistakes, thereby sustaining velocity.

One potential challenge: The initial setup overhead is high – the AI (and human team) must learn to use all these tools properly. However, once the pattern is established, it becomes routine (almost like a well-defined dance for any change). This should indeed improve the AI’s task orchestration efficiency because the AI can focus on what to change, and rely on the framework to automatically propagate those changes correctly and catch any misses.

Thus, from a reliability standpoint, the automation approach can significantly reduce divergence between code, docs, and state. It effectively binds documentation updates as part of the development workflow. Provided the AI follows the procedures (which Agent Policies and triggers strongly encourage or enforce), the project’s bus factor is increased – meaning knowledge is captured in the system, not lost if context is lost.

If we identify any gap where something might not sync:
	•	Possibly knowledge in AI’s “mind” vs saved docs: if the AI deduces something but doesn’t write it down, that’s a risk. The workdocs ledger mitigates this – the AI is instructed to write down even intermediate conclusions in context.md or decisions.md. So we capture ephemeral reasoning too, making it easier to recover later.
	•	Another subtle area: Database or external state. The template covers DB migrations to ensure schema docs updated, etc., but if the AI were to run a destructive operation outside the process, guardrails forbid it (like disallowing direct DROP TABLE without following deprecation steps) ￼. So even DB state remains consistent with migration records.

In conclusion, the combination of SSOT, automated registry sync scripts, contextual triggers, and diligent work logging provides a robust mechanism for keeping data (documentation, config, schema) and development progress tightly in sync. The approach is indeed well-suited for an AI-driven project, as it compensates for the AI’s lack of long-term memory by embedding memory in the repository itself.

8. Module Instance Development & Orchestration Evaluation

The template introduces a modular structure where each module instance is somewhat self-contained (with its own docs and work context) but still integrated into the global project. Let’s evaluate a few aspects:
	•	Archiving and Context Separation Logic,
	•	Independent Agent & Task Orchestration per module,
	•	Adherence to management norms in module development,
	•	Data flow and relationship mapping for modules.

Workdocs Archiving & Context Decoupling: The plan clearly separates project-level context from module-level context by having dedicated workdocs in each module’s directory ￼ ￼. This isolation is beneficial: it reduces noise (the orchestrator’s context doesn’t overflow with every module’s minutiae) and ensures the AI can focus on one module at a time. The archiving strategy (moving completed module workdocs to archive with a timestamp and logging in handoff-history) is well-defined ￼ ￼. This solves potential clutter issues and provides traceability for module development history. No glaring logic issues are present there; it’s methodical. One thing to watch out: if a task spans multiple modules (like a refactor affecting two modules), by default the contexts are decoupled. The guide suggests you can manually link references if needed but not force merge them ￼. This is prudent because merging contexts could confuse the AI, but it does require the AI/maintainer to know to cross-reference. We might want to highlight in the orchestrator or plan that cross-module tasks should explicitly note the involved modules in plan.md#Collaboration (the guide does say to list handoff points and needed docs for multi-team or multi-module collaboration) ￼ ￼. As long as that practice is followed, the decoupling is fine.

Module Agents & Orchestration: Each module instance gets a node in the agent graph (with its own policy and capabilities). This implies an independent agent orchestration for the module to some extent:
	•	The module’s AGENTS.md acts as a local orchestrator for tasks within the module (enforcing its guardrails, selecting module-specific tools).
	•	The module’s tasks are managed in its workdocs, and triggers can be local to module context if needed.
	•	The global orchestrator treats a module agent as a subgraph that can be invoked when a task falls under that module’s scope.

This design creates a hierarchy: global Orchestrator → Module Agent → Tools. It seems consistent with the main architecture (domain agents and module agents all hang under orchestrator). It means the orchestrator can handle high-level routing (like “this is a Payment issue, go to Payments module agent”), and then the module agent will handle within-module decisions. This layered orchestration is good for scalability – adding modules doesn’t exponentially increase complexity, it just adds nodes.

We should ensure that module agents also follow the same norms:
	•	Their AGENTS.md must have similar structure (we have that).
	•	They maintain their own CAPABILITIES and possibly triggers. For example, a module might have a trigger for “when bug count exceeds X, escalate” – though such triggers might also be global. Usually triggers about code changes (like file paths) can be module-specific (like a trigger that monitors modules/<name>/bugs.md for new issues might assign a remediation agent).
	•	The guide indicates module triggers should be integrated via orchestrator registry in requires or orchestrator mapping for that subgraph. If a module needs cross-module interactions (DocOps checking module docs), they allow cross edges in the graph with safety constraints. This is advanced but indicates forethought about module orchestration interplay.

No obvious archiving logic issues: the procedure for module initialization and lifecycle is extremely detailed (5.2.1 and 5.2.2) – it covers collecting requirements, scaffolding, registering, then dev stage with clarify → retrieve → code → test → context logging. The module development process is aligned with the larger task chain (需求 → 检索 → 计划 → 执行 → 测试 → 记录) loop that the interactive development aims for. Specifically:
	•	Requirement to Plan: Module’s plan.md and tasks.md capture requirements and break them down ￼.
	•	Retrieval: It explicitly instructs to search module’s ROUTING then global ROUTING then relevant docs ￼ ￼ – meaning it uses the documentation system properly even at module scope. That ensures the module agent does not operate in a vacuum but still leverages global knowledge.
	•	Execution: Only uses registered commands (ensuring it doesn’t call anything untracked) ￼ ￼ and logs them.
	•	Testing: Ensures module-specific tests run and logs outcomes ￼.
	•	Context updates: All along, updates context.md, tasks.md, etc.

This is very thorough and mirrors the larger project norms at the module level – so consistency is maintained. The module agent development is basically a microcosm of the overall development process, which is good. It means the same patterns (triggers, guardrails, workdocs, SSOT updates) apply within a module.

Data Flow and Type Relationship Diagrams: The template expects maintaining:
	•	Module relationship graph: which modules depend on which (the registry fields requires, provides, related_modules) ￼ ￼. This is important for orchestrator to route tasks correctly. For example, if Module A requires Module B, maybe tasks that involve both escalate to a higher-level agent. The plan to store these relationships is clear (module registry YAML).
	•	Data flow diagrams: The actual runtime data flow (like how data passes between modules or between agent nodes). In agent-graph.yaml we can represent modules as subgraphs and cross links. The system even accounts for cross-subgraph edges with guardrails if needed.
	•	They also mention visualizing module relations possibly (the “模块关系图” as in 5.1 responsibilities, though not explicitly a separate doc, could be depicted or documented for maintainers).

From a project management perspective, this meets needs:
	•	We have documentation for each module’s internal workings and external interfaces (so responsibility is clear, fulfilling the encapsulation).
	•	We have a registry to quickly see all modules and their status (so management can see what’s implemented, what’s stub, what’s deprecated).
	•	We have triggers and logs to monitor each module’s health (ex: possibly, make module_health_check runs tests per module).
	•	The independence of module workdocs ensures parallel development by AI in different modules wouldn’t conflict – if at some point the AI (or multiple AIs) tackled different modules, their contexts wouldn’t scramble together.

One potential issue: Duplication vs. Reuse – by making modules independent, there might be some duplicate effort (like if two modules have similar documentation pieces). The template addresses reuse via the common assets (modules/common, modules/api) and encourages linking requires/provides. So, if something is shared, it should be factored out as a common module or noted in relationships. The orchestrator can then avoid duplicating logic. This is good for compatibility – it’s easier to swap out one module for another if they adhere to the contract.

Guardrails within Modules: If a module has a particularly sensitive part (say a Payment module dealing with money), we can impose extra guardrails in its AGENTS.md (like require additional approval for certain actions). The system supports that via either the module’s agent policy or triggers targeted at that module’s files (e.g., a trigger if a critical file in payments module changes, route to guardrail). That ensures even at module scope, nothing critical is done without oversight. This compatibility with the global guardrail system is important and seems feasible with how triggers can be filtered by path.

Conclusion: The module instance development approach is well-integrated and methodical. Each instance is like a mini project that still plugs into the big picture. There is no evident logic flaw – in fact, the procedure is possibly even more rigorous than many real-world set-ups, which is good for an AI-run project (the strictness provides safety). The independent context and agent orchestration at module level align with the main normative frameworks (documentation routing, workdocs, guardrails, etc.), so the AI won’t have to switch paradigms when switching from global tasks to module-specific tasks.

If anything, the complexity is high – managing this for many modules might be heavy. But presumably the template is aimed at projects where AI involvement is heavy and thus such structure is justified. Also, the question of multi-language or multi-environment modules: the plan doesn’t explicitly cover modules in other languages (like if one module was in Node.js while another in Python). The template is language-agnostic to an extent (mostly about docs and processes), but specific scripts like type_contract_check might assume certain stack (OpenAPI etc.). The modular approach could adapt to multi-language by having each module’s contract specify language-specific testing (like a module might have its own test runner if it’s in a different language, but orchestrator could still manage it). The structure is flexible enough to allow that, as long as we register the different tools needed for that module (which capability index per module would handle). This addresses compatibility partially (we discuss more in section 13).

9. Context Recovery, Error Logging, Triggers, Guardrails Integration

The template dedicates significant attention to context management (workdocs), error/lesson logging, trigger mechanisms, and guardrail policies. Let’s evaluate how these integrate into the project and improve the AI’s capabilities and developer experience:
	•	Context Recovery (Workdocs): This system is a boon for an AI developer. By continuously writing to structured logs (session progress, decisions, risks, etc.), the AI can leave breadcrumbs of its thought process and what it has done ￼ ￼. If the AI session ends or is interrupted, it can reload the last context from these logs instead of re-parsing the entire codebase or forgetting things. This saves time and tokens, and reduces mistakes like reimplementing something that was done already. The five-section structure in context.md (Progress, Decisions, Risks, Active Files, Learning) ensures that critical information is surfaced at a glance ￼ ￼. The enforcement that each section stays brief (≤40 lines) with details in subdocs keeps the context focused, which is AI-friendly (less irrelevant text loaded) ￼. Integration wise, the orchestrator and agents are explicitly instructed to update these after each action (through hooks like PostToolUse writing to active-files, etc.) ￼ ￼, meaning this isn’t optional or left to chance. That tight integration means the context logs effectively become part of the code (one could say “infrastructure as documentation”). For the developer (AI or human), it’s a great UX: you can quickly see what’s going on and resume work or review the AI’s decisions.
	•	Error Logging & Learning (Lessons Log): The template’s approach to errors is exemplary. Instead of burying error details in commit messages or forgetting them, each error is given a structured entry in context/lessons.md with cause, consequence, lesson, and even an “AI reminder” field that ties it to guardrail triggers ￼ ￼. This means the AI effectively builds a knowledge base of mistakes. The next time a similar situation arises, the guardrail or trigger system will prompt the AI to recall that lesson (via preload_docs injecting the relevant ERROR entry) ￼. This closed-loop learning is likely to reduce repeat errors, thus improving reliability over time. It’s integrated with triggers: e.g., after ERROR-001 (missing contract update) happened, they suggest creating a guardrail trigger for contract changes that loads that lesson ￼. So the next time AI tries to change an API without updating docs, the trigger fires, the lesson is loaded, and the AI is reminded proactively. That’s a powerful integration of logging with execution. For developer experience, it’s like having a built-in retrospective memory – the AI doesn’t get bored or offended reading its past mistakes, it will just dutifully use them to avoid pitfalls.
	•	Triggers Mechanism: The triggers system acts as an event-driven extension of the agent orchestration. It monitors both user prompts and repository events (like file changes) and routes them to appropriate handling ￼ ￼. This greatly enhances the responsiveness of the AI developer:
	•	Without triggers, the AI might only act sequentially when prompted. With triggers, the AI can react to asynchronous events (like “tests failed in CI” or “PR opened requiring review”) automatically by executing pre-defined steps. Essentially, triggers allow a more continuous integration/continuous development (CI/CD) style loop.
	•	The integration is well-thought: hooks (UserPromptSubmit, PreToolUse, PostToolUse) tie triggers into the AI’s action cycle ￼ ￼. The AI doesn’t have to check for events manually; the system will catch them and either directly execute or at least notify via adding tasks.
	•	For the AI developer, triggers improve UX by automating routine tasks. Example: “on push to Git, run tests” – instead of the AI remembering to run tests, a trigger ensures it happens (via PreToolUse or an event). Another: “on detection of security keyword in code, pause and require approval” – triggers can save the AI from committing a secret or dangerous code by intercepting.
	•	We have to ensure triggers are well-defined so as not to overwhelm or conflict. The template suggests weekly dry-run checks to keep triggers in check (so they don’t remain orphaned or misconfigured) ￼ ￼, which is good maintenance practice.
	•	Guardrails Integration: The guardrail system is essentially a specialized trigger for critical events (with enforcement: block or requiring approval) ￼ ￼. It is deeply integrated at multiple levels:
	•	Policy definition in AGENTS.md (some have a Guardrail section).
	•	agent-triggers.yaml marking certain triggers as block/critical which then funnel into the guardrail approval workflow ￼ ￼.
	•	The guardrail_runner orchestrating the approval steps, which involves both AI and a human approver via context docs ￼ ￼.
	•	This ensures that for any action beyond AI’s safety clearance (like deploying to production, modifying prod DB, merging to main branch), there’s a checkpoint.
	•	For AI’s capability, this increases trustworthiness. The AI can operate freely in safe areas, but if it tries something risky, the guardrail stops it and engages a human or at least logs an alert. This prevents catastrophic mistakes, making the AI’s continuous development approach viable in a real scenario.
	•	Developer experience: The human collaborators have clear points to intervene (the approvals recorded in context.md#Approvals). They don’t have to micromanage – they only step in when guardrails summon them. This is efficient. And the AI developer doesn’t waste time either, because it knows exactly when it must wait (the guardrail process spells it out via tasks and plan statuses).
	•	Enhancing AI Capabilities: By having these systems, the AI is effectively augmented with:
	•	Memory (via workdocs),
	•	Foresight (via triggers that preemptively run checks or load lessons),
	•	Discipline (via guardrails enforcing rules).
	•	Without these, an AI coding agent might repeat mistakes, forget to run tests, or occasionally do something unsafe. With them, the AI is guided to behave more like a diligent developer who always writes everything down, learns from errors, and asks for code review when doing something risky. This drastically improves the sustainability of AI involvement in a long-running project (the AI can “recover” state after a weekend, for example, by reading context logs).

Integration into Repo: The nice part is that these are not external additions; they are part of the repository:
	•	Workdocs are in ai/workdocs folder – versioned, searchable.
	•	Triggers configurations live in doc_agent/orchestration/*.yaml – also versioned and documented.
	•	Guardrail decisions and history are logged in handoff-history.md – part of repo history.

This means any new AI agent coming to this repo (or a human) can see the whole picture of why certain triggers exist (thanks to retrospective logs linking error to guardrail addition), what approvals happened, etc. It’s a self-documenting system.

Potential pitfalls:
	•	Overhead: The AI might spend significant tokens reading/writing workdocs and processing triggers. But since those are kept lean (120 lines hub) and loaded on demand, it’s manageable.
	•	Complex trigger logic might occasionally misfire (false positives) or not cover a scenario. But the weekly checks would highlight triggers that never fire (or always fire) so we can tune them.
	•	Ensuring the AI follows through with writing logs consistently. The policies state it must, and presumably any failure to update context would result in confusion later which the AI would realize (like if it loses track, it might see an incomplete log and notice that as a risk). Also, agent_lint could enforce something like ensuring tasks have statuses, etc., though that’s more procedural. We might consider adding a check script to enforce that all tasks have statuses and all completed tasks have archived contexts, etc., but that might be beyond initial scope. The guide did mention an Chapter 8 triggers inspection script that compares tasks statuses to triggers ￼. So some level of enforcement exists.

Overall, these systems (context, triggers, guardrails) are highly complementary to the routing and capability system. They ensure that the AI not only can find and execute things, but does so in a controlled, recoverable manner. They absolutely enhance the development experience by reducing cognitive load (the AI doesn’t have to recall everything or vigilantly watch for every little condition – the triggers and logs assist it). They also provide transparency and safety, which are crucial for convincing humans to let an AI deeply into the development loop.

In conclusion, the integration of context recovery, error/learning logs, triggers, and guardrails is one of the strongest aspects of this template, making the AI a more effective and trustworthy developer. We will make sure these systems are indeed integrated into the repository’s actual files (they largely are through the YAML configs and workdocs structure we outlined).

10. Registration Systems and Change Management Evaluation

There are several registration systems in place: documentation routing registration, capability registry, agent registry, module registry, trigger map, doc-node map, etc. Let’s enumerate the key ones and assess whether a change in any component triggers the necessary updates globally:
	•	Documentation Routing Registration: This is handled by the network of ROUTING.md files and the aggregated doc-node-map:
	•	Key asset: doc_node_map.yaml, which ties doc topics to agent/capabilities nodes ￼.
	•	When a doc is added/removed or its purpose changes, the expectation is to update the relevant ROUTING.md. The procedures we have (like make route_lint and doc_node_map_sync --report) will catch any doc that is not in a routing or any routing pointing to missing doc ￼. So if someone adds a new guide, they must add it in some routing or a check will complain.
	•	The front matter of each doc also has a purpose that should match keywords of its routing topic ￼. Changing a doc’s purpose should prompt adjusting the routing (the AI should do this when it updates the doc, per guidelines).
	•	Because the AI is in charge, we can rely on triggers: maybe a trigger could detect if a new .md file is added under doc_agent that is not referenced in any ROUTING (or doc-node-map_sync can produce a warning). This ensures no orphan docs lingering.
	•	Summation: The doc routing registration is strong. By design, adding any new knowledge requires adding to a routing. And the guardrails (like the doc consistency checks in CI) treat failing to do so as incomplete (thus block commit). So global coherence of knowledge docs is maintained.
	•	Agent & Tool Registration:
	•	Orchestrator Agent Registry (registry.yaml) holds all composite agent definitions (orchestrator itself, domain agents, and likely references to module agents which might be merged from module-registry) ￼.
	•	Capability Registry (capabilities.yaml) holds all base tools (scripts, external APIs).
	•	Module Registry (modules/registry.yaml) for modules as discussed.
	•	These have a defined update order: e.g., add base capability → update capabilities.yaml (and CAPABILITIES.md) → then if an agent needs it, update agent’s provides in registry.yaml → then update routing/triggers → etc. ￼ ￼. The plan enumerates steps to not forget any mapping.
	•	The automation with scripts covers propagating one to another:
	•	E.g. module_registry_sync reads modules/registry and updates orchestrator registry and agent-graph for module nodes ￼.
	•	trigger_map_sync ensures triggers tie to actual capabilities (so linking agent to capability to command is consistent) ￼.
	•	doc_node_map_sync ties routing to graph nodes.
	•	The make checks like registry_check, capability_index_check, etc., double-verify that each mapping is one-to-one across these systems ￼ ￼.
	•	Example: If an agent is removed, the developer (AI) should:
	1.	Remove it from registry.yaml,
	2.	Remove its node from agent-graph.yaml (or mark deprecated),
	3.	Remove related context_routes if it was an entire topic,
	4.	Update doc-node-map (script likely covers that if agent gone).
	5.	Update any CAPABILITIES that were only for that agent if necessary.
	•	If it forgot one, a check would catch. For instance, if agent-graph still has a node but registry.yaml doesn’t, registry_check fails.
	•	We might even have triggers for deletion, but deletion usually manual. At least tasks would list to ensure “cleanup steps done”.
	•	Another example: If a module is renamed or removed, the module registry update plus sync script will remove it from doc_agent orchestrator registry and graph, and likely route_lint would catch that a context_route still pointing to old name exists and needs update. So global references to that module would be cleaned.
	•	Guardrails and Policies Registration:
	•	Not exactly a registry file, but one could consider the list of guardrail triggers (in agent-triggers.yaml) and the actual content in AGENTS.md (Guardrail sections) as needing consistency. The guide covers that with the guardrail update process: if we add a new guardrail, update triggers, update policy, run all the checks (trigger_map_sync, etc.) ￼ ￼.
	•	The weekly guardrail_check will ensure every block trigger has a corresponding policy handling in AGENTS.md (if something missing, the dry-run would fail to load policy). That’s a sort of consistency test.
	•	Changing a policy in AGENTS.md (like adjusting a rule) doesn’t require a registry update, but the AI should consider if triggers need adjusting too. There isn’t an automated linking except the doc_node_map linking AGENTS to topics and graph. However, by convention the policy change might accompany adjusting an enforcement field or similar in triggers.
	•	Perhaps the agent_lint.py covers verifying that each AGENTS.md’s graph_node_id is present and that if triggers reference that node, etc., but it may not link specific content.
	•	It’s more content-level, but since guardrails are mostly about triggers and policies, which are configured meticulously, we likely won’t see mismatch often (because adding a guardrail = adding both trigger and policy segment by design).
	•	Ensuring Global Coherence on Changes:
	•	The combination of scripts (for mechanical consistency) and guardrails/triggers (for process consistency) form an automation chain that maintains global integrity. The user story of any change (add, remove, modify something) always involves either:
	•	A script that propagates the change to all needed places, or
	•	A checklist in policy (“if you do X, you must also do Y and Z”) often enforced by triggers or at least by CI failing if you don’t.
	•	Example: Adding a new trigger – the policy says update agent-triggers.yaml, then run trigger_map_sync, route_lint, etc. ￼ ￼. If a step is missed, maybe route_lint might not catch triggers but guardrail_check would catch if no policy or something. And the workflow says to paste outputs in workdocs, which a reviewer (or the AI next iteration) would see if something looks off.
	•	We should evaluate a scenario: Doc or agent removal – often hardest because references might linger:
	•	If remove doc: the doc_node_map sync will likely warn if an ID was mapped and now doc gone. Also route_lint would find a context_route pointing nowhere, failing CI. So you must remove that route entry too. Good.
	•	If remove agent: agent_graph_check might fail if node removed but edge references remain. Also doc_node_map if that agent had an AGENTS.md that is still in routing maybe. However, if removing an agent, one would remove its AGENTS.md and CAPABILITIES.md, and update parent routing to drop references (the policy says if no agent there, remove from routing or mark read-only in parent route) ￼. The check scripts would enforce no broken mapping.
	•	If remove module: similar to agent, plus module registry. The script would handle some, but you’d also remove its docs directory. A route entry might still point to it if forgot. The route_lint would catch that.
	•	Registration of Tools: The guide centralizes tools_allowed in registry.yaml and says do not list them in local policies except summarizing in text ￼ ￼. This means if we add a new script that an agent should use, we must add it to the tools_allowed list in the registry for that agent. If we forget, the agent might attempt to call a tool not in its allowed list – would something catch that? Possibly agent_lint or registry_check might catch a capability with no matching target_agent or vice versa. And at runtime, maybe the orchestrator sees tool not allowed and refuses. So better to catch statically: likely registry_check would say “capability X is not in any agent’s allowed list” or similar.
	•	The design of having a central registry means there’s one place to update for tool permissions, which is easier to maintain than editing multiple AGENTS files. So global coherence of what tools are available to what agent is maintained by that one file plus the checks to ensure consistency with CAPABILITIES and triggers.

Global Automation Chain:
The question specifically asks if when docs, functions, agents, module instances change, the automation chain ensures global coherence. Based on above:
	•	Yes, because any such change is guided by a SOP with scripts and checked by automated tests:
	•	Add doc → must update routing (or fail check).
	•	Add capability → must update capabilities.yaml + CAPABILITIES.md, then registry if agent uses it, then sync graph, etc. (tools enforce this sequence, failing CI if incomplete).
	•	Add agent → update registry (or module registry), update agent-graph, triggers if needed, update routing to include policy and any new doc it might have, etc. Many steps but each has either a script or a lint to verify.
	•	Remove things → see above, checks ensure no leftovers.
	•	Modify (like rename an ID or change a contract) → contract check scripts ensure baseline and doc updated, doc-route check ensure topic still aligns with purpose if changed (the user will have to adjust accordingly, but at least we log that if contract changed, must go through approval and update baseline) ￼ ￼.

One area to consider: changes like refactoring code architecture (affecting multiple modules or agent graph structure significantly). The guide addresses it with version lifecycle and iterative plan in workdocs for graph evolution ￼ ￼. If we massively rewire the agent graph, we need to update many docs (policies might move to other agents, etc.). This is complex but they suggest doing it in planned phases and using all the check scripts to catch mismatches. So they did foresee large changes too.

Thus, I am confident the automation chain can handle global changes. It’s likely robust because:
	•	It’s multi-layered (manual procedures + scripts + CI).
	•	Every key entity has an index or registry to refer to; nothing is floating implicitly (even docs have doc_node_map to tie them in).
	•	The AI being “main developer” might actually reduce random deviations – it will likely follow the template’s rules strictly (since it’s part of its instructions) rather than a human who might accidentally bypass a step.

No critical gap appears: The only slight gap is ensuring the AI always runs the needed scripts. But with triggers (like on commit or pre-commit hooking to run dev_check) and policy, it likely will. Perhaps we should ensure something like a git pre-push hook or CI gating – though this is outside AI, but presumably those make targets in CI do it.

Evaluation: These registration systems strongly contribute to global consistency:
	•	They reduce duplication (single sources).
	•	They ensure any local changes propagate or at least fail a check if not propagated.
	•	They provide clear mapping from conceptual nodes (like “Payment module agent”) to everything (docs, code, triggers).
	•	They allow the AI to query “what agents exist?” by reading registry.yaml, or “what does capability X do?” by reading CAPABILITIES.md – so even reasoning is supported.

In conclusion, the registration and update mechanisms are quite reliable. I would suggest maybe to explicitly have a checklist (maybe in AGENTS.md or as a PR template) that the AI can follow, but the guide already mentions adding an “Agent Policy Checklist” at end of AGENTS.md as a suggestion ￼ ￼. That would list “did you update registry? did you run lint?” etc. Implementing that will further ensure no step is missed. This is a great guard against any oversight.

11. Specification Documents and AGENTS.md Integration

The repository will contain numerous specification and guide documents (as we listed: policies, guides, specs like DB_SPEC, etc.). We need to ensure the AI can effectively use them, and see how they work together with the AGENTS.md (agent policies) which are the AI’s primary “rule book” in each scope.

List of Required Specification/Guide Docs (for AI guidance): Summarizing from our doc list, the key spec/guidance documents (aside from routing and index files) include:
	•	Coding/Documentation Standards Guide: (doc_agent/guide/documentation-spec.md, etc.) – how to format docs, how to write commit messages or PR descriptions (they gave PR guidelines in 11.5 that might be turned into a policy doc) ￼ ￼.
	•	Security Policy Documents: e.g., Guardrail policy (what requires approval, how to handle secrets), possibly separate documents for different security aspects (like one focusing on database access policy, one on code review policy).
	•	Quality/Testing Policy: specifying test coverage requirements, required checks (some could be in orchestrator AGENTS.md, but maybe also a detailed guide for writing tests).
	•	Operational Runbooks/Guides: quickstart guides for tasks (deployment, environment setup, etc.).
	•	Schema/Contract Specs: DB_SPEC.yaml and maybe a human-readable explanation of DB design rules (like naming conventions, etc.), API Spec (OpenAPI file and guidelines on versioning).
	•	Config Governance Guide: (doc_agent/guides/config-governance.md as mentioned) – how to manage config, where to put secrets, etc.
	•	Workflow Guide: maybe something about how to write workflows or use triggers properly (though much of that is in this main doc, we might put a subset in a quickstart for writing new triggers).
	•	PR/Review Guide: Possibly, since section 11.5 gave PR requirements, we might codify them in a doc_human/CONTRIBUTING.md for humans and a doc_agent/guide/pr-guidelines.md for AI reminding what to put in PR description. The AGENTS.md of orchestrator might also list some of that, but a separate guide could hold more detail.

How the AI uses these alongside AGENTS.md:
	•	AGENTS.md (especially the root one) is like the constitution for the AI – stating what it is allowed or not allowed to do, and listing major obligations (like “you must run tests before commit”). However, it intentionally does not duplicate full procedures or lists of tools ￼ ￼. It might say “Ensure to follow the Documentation Standard guide when writing docs” or “Always obtain approval for production DB changes”. The specifics of the documentation format or the procedure to get approval can be in the referenced guides/policies.
	•	The guide’s routing ensures that AGENTS.md and the relevant guides are read together: e.g., root routing under Governance: Agent Orchestration Rules includes doc_agent/AGENTS.md and maybe an orchestrator policy doc. Or Execution: Testing & Quality includes quality-quickstart (guide) and scripts README; the orchestrator AGENTS.md might say “must run dev_check”, but the quickstart details how to run it.
	•	Trigger Preloads: When a trigger fires, it can preload policy or guide docs. The example for guardrail triggers: Hook auto injects preload_docs like safety policy or recent incidents ￼ ￼. That means when the AI hits a guardrail, it automatically has the relevant policy doc in context. E.g., if enforcement: block triggers, Guardrail agent loads AGENTS.md#Guardrail section plus any referenced “security policy” doc sections in preload. So the AI will not only know “I’m blocked because of rule X in AGENTS.md” but also see “according to Security Policy doc, you need manager approval for this”.
	•	Agent Policy linking to guides: AGENTS.md front matter can have related_routes or references. They mentioned an optional related_routes field to point to relevant docs or parent policies ￼ ￼. Also in the content, the agent policy has sections like Related Routes & Capabilities linking to the local ROUTING and CAPABILITIES entries ￼ ￼. That helps the AI map “this policy corresponds to these docs and abilities.” For instance, a module agent policy might link to the module’s CONTRACT.md and runbook, so the AI is aware of them.
	•	AGENTS.md as Gateway: Typically, the reading order is ROUTING.md -> AGENTS.md (policy) -> CAPABILITIES.md -> actual guides/scripts ￼ ￼. This means the agent policy doc is read before any heavy guide. That is by design: AGENTS.md sets the stage (permissions, required steps). Then it might instruct the AI (implicitly via triggers or explicit links) to consult a guide for details. E.g., AGENTS.md might state “All commits must follow PR guidelines (see PR_GUIDELINES.md) and pass CI (see quality-quickstart.md for how to run checks).” The AI then knows to load those docs (maybe via routing or because it’s told).
	•	Mandatory vs Optional reading: Agent policies often mark some docs as must-read: e.g., in target_docs of a route, we sometimes put an “Agent Policy” and a “Guide”. The AI should treat the policy as non-negotiable (it contains rules), whereas guides are informational (how to do it). But since the routing lists both, the AI will load both and combine the knowledge.
	•	Agents.md synergy with spec docs: The orchestrator’s AGENTS.md might not detail every last rule (to keep under 200 lines) – instead it might summarize and reference spec docs. For instance, orchestrator policy might not list the entire table of sensitive commands to avoid but can reference the Security Policy doc for the full list. Then either triggers enforce those or the AI if needed can open that doc via routing.
	•	Another example: Module AGENTS.md might say “Follow the module’s contract in all changes (doc/CONTRACT.md) and ensure compatibility tests pass.” The details of the contract are in CONTRACT.md (which is in the module’s knowledge docs). The AI will likely load CONTRACT.md anyway when working on the module because the routing for module tasks would include it (we set module Quickstart to chain to contract and runbook docs).

In practice:
	•	At the start of a task, the orchestrator loads relevant policies (AGENTS.md) and maybe some quickstart.
	•	As the AI plans steps, it sees in the policy if it needs to load some other doc. If not automatically done via routing or triggers, the AI can decide to load a guide by doing a search in the doc routing (since all spec docs are part of the routing tree).
	•	Since the template encourages using the routing index for any info, the AI might do: “Need to format code. Search ‘format code’ in docs.” If we have a spec or quickstart containing that, the routing would lead it to e.g. a “Code Style Guide” doc.
	•	Also, some spec docs might be labeled with tags in doc_node_map, enabling the AI to find by topic. For example, doc_node_map.yaml maps “Documentation Standard” topic to documentation-spec doc. So if the AI queries the orchestrator agent for documentation standards, it could fetch that doc.

In summary:
	•	The spec docs and guides fill in all the “how-to” and detailed rules that are impractical to put directly in the agent policies.
	•	The agent policies (AGENTS.md) provide the “must follow” outline and often point to those docs.
	•	The routing ensures that when appropriate, the AI reads both the policy and the spec (because either target_docs includes both or triggers preload the spec when needed).
	•	This is a sensible division: AGENTS.md = “what to do / not do at a high level”, Guides/Specs = “how to do it exactly / reference info”.

We should also consider AGENTS.md combination with AGENTS.md at different levels: The root AGENTS sets global boundaries, but module AGENTS might refine or add to those (like local rules). The orchestrator when delegating to module agent will have the module’s AGENTS.md loaded at that time, which may reference back to higher-level guides if needed. There’s even a concept that if multiple sub-agents share a rule, it can be defined at a higher level AGENTS and referenced via related_routes from children ￼. That prevents duplicating the same policy text in each module. For instance, if every module has the same coding style rule, you put it in a global policy doc and just link it. This is another integration: the doc_node_map or related_routes linking means the AI can be guided to the common policy doc rather than reading similar text 10 times.

Conclusion: The spec and policy documents appear to be well integrated via the routing and referencing scheme. For any given action the AI takes, the design ensures there is a documented source of truth the AI can consult (instead of relying on guesswork or stale training). For example, if it’s about to commit code, it knows to read PR guidelines doc. If working on DB, read DB spec/policy first. AGENTS.md often explicitly instruct or at least triggers will enforce reading those.

To maximize this, we might maintain an AGENTS.md index of important docs (the doc_node_map already does some mapping). Possibly the root AGENTS.md could list key references at bottom (“Refer to X for coding standards, Y for security guidelines”). Even if not, the context_routes cover it.

All in all, the interplay is comprehensive:
	•	AGENTS.md sets the guardrails and tasks,
	•	The guides/specs provide the knowledge,
	•	The AI is pointed to both when needed, keeping it informed and within bounds.

12. Repository Base Systems Summary

Bringing it all together, the repository template has three primary interlocking systems:
	1.	Documentation Routing System – a structured knowledge base navigation (ROUTING.md tree and associated docs).
	2.	Function/Capability Routing System – a mapped network of actions and tools (capability indices, registry, agent graph).
	3.	AGENTS.md Policy System – a set of governance documents for AI agents at various scopes (root and per module), defining safe operation and procedures.

Let’s summarize each and their role in the project:
	•	Documentation Routing System:
	•	Purpose: Guide the AI (and to some extent humans) to the right documentation at the right time, minimizing cognitive load and context size. This system ensures the AI can answer “Where do I find information on X?” systematically.
	•	Structure: A hierarchical set of ROUTING.md files (root and nested) forming a tree that mirrors project domains and stages ￼. Each routing contains context_routes with scope -> topic -> when -> target_docs entries ￼. The leaves are actual documentation content files (guides, specs, policies, etc.), which themselves do not have routing but are linked via front matter references.
	•	Interactions: The Orchestrator agent uses the routing to decide which docs to load for a given task description ￼ ￼. It matches the task to a scope/topic/when and then loads the listed target_docs. The routing also sometimes directly links to an agent’s policy or a capability index if that is needed first ￼.
	•	Maintenance: The doc routing is maintained by developers (AI or human) whenever new knowledge is added or scopes change. Tools like route lint and doc-node-map ensure it stays consistent and updated ￼ ￼. This routing forms the skeleton of the knowledge base, allowing progressive loading (the AI reads summary nodes first, then drills down as needed).
	•	Function/Capability Routing (Agent Orchestration Graph & Registry):
	•	Purpose: Enable the AI to plan and execute actions by knowing what tools and agents are available and how they connect. It’s effectively the “execution roadmap” or the equivalent of code API for the AI – a map of capabilities it can invoke.
	•	Structure: At its core is the Agent Graph (agent-graph.yaml) which is a directed graph of all AI agents and tool nodes, defining how they can hand off tasks and how data flows between them ￼. Surrounding this, we have:
	•	The Capability Registry (capabilities.yaml + CAPABILITIES.md) listing all base capabilities (scripts, commands, external API calls) with details ￼ ￼.
	•	The Agent Registry (registry.yaml) listing all composite agents (orchestrator, domain agents, module agents) with their properties, including which base capabilities they provide or are allowed to call ￼ ￼.
	•	The Triggers mapping (agent-triggers.yaml & trigger-map.yaml) linking events to agent graph connections (so certain conditions directly activate certain edges or agents) ￼ ￼.
	•	The Module registry connecting module instances into the graph by mapping module entries to subgraph nodes ￼ ￼.
	•	Interactions: When the Orchestrator has determined the needed docs and understanding of the task (from the doc routing), it then uses the agent graph to decide which agent node should execute. E.g., if after reading docs it knows “This is a database schema change”, the orchestrator via triggers or logic will activate the Data & Schema agent node ￼ ￼. The orchestrator consults the mapping (target_agent in triggers or it could match on capability tags) to route the task. Once an agent is invoked, that agent in turn picks the correct tool from the capability index and executes it (the logic being: agent sees allowed tools, maybe based on conditions picks one).
	•	Example: Orchestrator sees trigger “new migration pending” → triggers data_schema agent → data_schema agent (via Guardrail if critical) runs make migrate_check (one of its capabilities).
	•	The capability index is also used by the AI to search for how to perform a specific action: e.g., if the AI decides it needs to send an email (just hypothetical), it can look at CAPABILITIES.md and see if there’s a tool for that. The structured listing (scope -> capability -> trigger) in CAPABILITIES.md helps the AI to find the relevant command by semantic tag.
	•	Maintenance: The function routing is maintained through the registries. Adding a new tool means updating capabilities.yaml (and CAPABILITIES.md) ￼. Adding a new composite agent means updating registry.yaml and agent-graph. The doc-node-map ties this to documentation by mapping agent nodes to documentation topics ￼ (so if a new agent corresponds to a new topic in routing, we reflect that in doc-node-map). Consistency scripts (capability_index_check, registry_check, agent_graph_check, trigger_map_sync) enforce everything lines up ￼ ￼.
	•	In short, the function routing is the actionable counterpart to doc routing: doc routing tells the AI what to read, function routing tells it what to do.
	•	AGENTS.md Policy System:
	•	Purpose: Define the rules, boundaries, and procedures for each AI agent or agent group. This is crucial for safety and alignment – it instructs the AI in each context what is permitted, what checks to do, when to seek approval, etc., making sure the AI’s actions remain within acceptable parameters.
	•	Structure: There is a root doc_agent/AGENTS.md governing the orchestrator (and overall policy), as well as additional AGENTS.md files in subdirectories wherever there is an autonomous agent (e.g., each module has one for its agent, the config directory has one for config changes, etc.) ￼ ￼. Each AGENTS.md follows a standard format with front matter (role, graph_node_id, etc.) and sections like Access Control, Execution Guardrails, Tools & Dependencies, Escalation & Logging, Related Routes/Capabilities ￼ ￼. They are kept concise (≤200 lines) but are densely packed with rules and checklists.
	•	Interactions: Whenever the AI (or one of its sub-agents) engages in a directory or task governed by an AGENTS.md, it is supposed to read that file before proceeding with actions ￼ ￼. For instance, when orchestrator routes to the “Data & Schema” agent, it should load doc_agent/policies/db-change.md (if that’s how the policy is split) or if embedded, the relevant section in AGENTS.md of that domain. The AGENTS.md tells the AI what it must do (e.g., “run db_lint before applying migrations”), what it must not do (“do not drop tables without approval”), and who to involve for certain steps (e.g., “requires DBA approval for prod changes”).
	•	Another example: before a module agent begins implementing a feature, it reads its module’s AGENTS.md which might say “Ensure any new API endpoints are added to CONTRACT.md and run contract tests”.
	•	These policies often reference the other two systems: they won’t list all tools but might say “only use tools listed in CAPABILITIES.md” or “refer to the guide X for detailed steps”. They also refer to documentation by stating their related_routes (so the AI can find extended info in the doc routing if needed) ￼.
	•	Handoff and Sequencing: AGENTS.md defines that sequence: Routing -> Agents (policy) -> Capabilities (tools) -> Execution ￼. So it sits in between reading knowledge and executing, translating “I have a goal” into “I have constraints and steps” then into action.
	•	Maintenance: Agent policies need updating when new rules are needed or when capabilities change (e.g., if a new dangerous tool is added, the policy might forbid it or constrain it). The doc mentions if strategy changes, update doc_node_map for new graph_node_id mapping ￼ (e.g., if we change an agent’s identity or merge agents, adjust mapping). We also have to maintain consistency that what AGENTS.md says is reflected in triggers and tools: the agent_lint.py helps ensure front matter and references are consistent ￼ ￼. Also, any major change like adding a new agent would come with an AGENTS.md creation which is part of the scaffolding in modules or similar processes.

These three systems work together as follows:
	•	The Documentation Routing system provides the AI with situational knowledge (what it needs to know).
	•	The Capability/Function system provides the AI with means to act (what it can do and how those actions flow).
	•	The Policy system (AGENTS.md) provides the governance (when and how it should act, with what precautions).

When a new task comes:
	1.	The AI uses the doc routing to load context and instructions relevant to the task.
	2.	En route, it will load any policy docs (AGENTS.md) at the appropriate points, which impose conditions on action.
	3.	It then decides on an execution plan and consults the capability graph to carry it out, choosing appropriate tools/agents.
	4.	The policies (AGENTS.md) and triggers ensure before executing any tool, the necessary checks (like approvals or preconditions) are satisfied.
	5.	Workdocs (context) log each step and feed back into doc routing (for example, lessons learned become part of documentation for next time).

AGENTS.md as the linchpin: It ties the doc reading phase to the action phase: after reading the guides, the AI checks AGENTS rules then uses the capabilities. Without AGENTS.md, the AI might know what to do and how, but not whether it should or in what order or with whose permission. So it’s critical for reliability and aligning the AI with project rules.

AGENTS.md and Routing integration: We ensure every AGENTS.md is accessible via the routing (it’s either listed in a parent route or at least known by doc_node_map so a search can find it). Indeed, the template says to always include references to existing policies in routing (even if directory has no sub-route, include a mention of the policy in the parent’s routing target_docs) ￼. That ensures the AI “never misses” reading a relevant policy. For example, root routing explicitly lists the orchestrator AGENTS.md under Governance scope, so any governance-related task would include that doc.

Summary: The repository base is essentially composed of:
	•	A knowledge layer (docs & routing),
	•	A capability layer (tools & graph),
	•	A control layer (policies & triggers via AGENTS.md and YAML configs).

This design creates a robust scaffold for AI involvement:
	•	It’s reliable because knowledge and actions are mapped and checked,
	•	It’s efficient because the AI doesn’t wander aimlessly; it knows exactly where to look and what it can do,
	•	It’s safe because the policies and guardrails step in at critical junctures.

This triple-structure also mirrors human development processes: we have documentation, we have an understanding of our tools (APIs, etc.), and we have team/project policies. Here they are formalized so an AI can navigate them autonomously.

13. Evaluation of the Four Key Ideas

We evaluate each of the four main pillars of the template – Agent Orchestration, AI-Friendly Design, Modular Development, Automation Maintenance – across three dimensions: Reliability (coverage of scenarios and error handling), Practical Viability (can it be implemented and used in real projects), and Compatibility (applicability to different languages or environments).

1. Intelligent Agent Orchestration (Knowledge + Ability Routing):
	•	Reliability: The dual retrieval system (knowledge routing and capability graph) is very comprehensive. It ensures that for virtually any task, the AI can find relevant docs (“去哪读文档”) and the right tools (“调用哪个能力”) ￼. By separating knowledge and action, it minimizes confusion (the AI first gathers context, then executes). Edge cases like ambiguous tasks are handled by the structured scopes and topics – even if a query is slightly off, the orchestrator can fall back to a broad scope then refine. The design encourages adding context (like triggers with example prompts) to the when conditions ￼, improving matching accuracy. Error handling is addressed: if a node doesn’t exist or a doc missing, the check scripts catch it (preventing silent failures). The orchestrator DAG with fallback edges means even if something fails, there’s a defined path (error_route or human takeover) ￼ ￼. This reduces the chance of the system getting stuck. Overall, the agent orchestration is reliable in covering known scenarios and has mechanisms (telemetry, retrospectives) to improve for new scenarios over time.
	•	Viability: Implementing this requires upfront effort – writing all those ROUTING.md, building the graph schema, etc. However, it’s largely a one-time setup (the “template bootstrap” covers it in Chapter 12) ￼ ￼. Once in place, adding content or abilities is formulaic. Many modern projects don’t have such explicit agent orchestration, but given the rise of AI pair programmers, this is pioneering. It can be integrated into real workflows gradually (starting with doc routing and a simple orchestrator that maybe just prints where to find answers, then adding automation). Tools like LangChain or similar could even be leveraged to implement the graph execution. One concern: the team must commit to updating the routings and registries with each change – this is a new discipline. But since the AI does a lot of it automatically with scripts, it’s viable. Early on, the system might feel heavy, but the benefits (especially for long-term or large projects) outweigh that.
	•	Compatibility: The approach is language-agnostic in structure – documentation and planning can be done regardless of code language. The ability layer (scripts) will differ: we focused on Python/Vue, but if the project was in Java, the CAPABILITIES might list Maven commands or Java-specific tools, which is fine. The agent-graph concept can orchestrate any subprocess or API call. Multi-language projects: The routing can include topics per tech stack, and the orchestrator can have separate agent nodes for, say, “Python Codegen Agent”, “Front-end (JS) Agent”, each with their tools. The main gap might be if certain languages lack good CLI tools for some tasks; then the AI might have to drive an IDE – but even that could be framed as a “capability” (e.g., trigger an editor plugin). So yes, it’s compatible with adjustment. The methodology can also be applied to multi-AI setups (if different agents specialized by language or domain, the orchestrator can coordinate them – basically what this design already does conceptually).

2. AI-Friendly Design (Context Recovery & Trigger Mechanisms):
	•	Reliability: The AI-friendly features directly target reliability of the AI’s performance over time. Context recovery ensures the AI doesn’t lose track of partial work due to token limits or session breaks – this eliminates a whole class of errors (like reintroducing a bug that was fixed an hour ago because the AI forgot). The trigger mechanism catches things the AI might overlook in real-time – making it less likely to skip important steps. For instance, a trigger can remind the AI to run tests before commit (so it reliably does so every time, not just when it “remembers”). Guardrails (part of being AI-friendly by not letting it do harmful actions) improve reliability by design – they stop disasters and ensure approvals are part of the loop. These features also help cover sustained operations: if the AI disconnects and another AI or a restarted session picks up, the workdoc context allows near-seamless transition – this is crucial for reliability in long projects (AI instances can be stateless because state is in workdocs). The potential downside is if triggers or guardrails are mis-specified (false alarms or overly restrictive), but given careful configuration and that they can be updated as lessons learned, the risk is manageable. Telemetry tracking also helps reliability by highlighting failing routes or often-triggered guardrails, prompting improvements.
	•	Viability: These AI-friendly features might be the most novel part, but they are conceptually viable. Implementing workdocs is straightforward: it’s just writing Markdown files. The AI needs prompting to do it – that can be built into the agent’s prompting system (“Remember to update context”). Tools like GPT-4 can follow that if instructed initially, and our agent policies enforce it. In practice, prototypes have shown AI can maintain a text-based scratchpad or todo list, so this is similar but structured. Triggers and hooks require integration at the environment level (like a wrapper around the AI or hooking into the VCS events). For example, to catch file events, we might run a daemon or integrate with git hooks. This is doable with scripting. In a VSCode setting, an extension could handle it; in an automated environment, a watchdog process can call trigger_runner.py. Since we are controlling the dev environment in this scenario, we can set that up. Guardrail approvals need a human in the loop interface – perhaps an email or a dashboard where a human clicks approve and the AI reads the decision from a file (context/decisions.md). That workflow can be done simply by having the AI wait (polling a file or awaiting a signal). For initial stages, the “human” could even be the user of this system who monitors the workdocs. So yes, it’s viable, though it introduces some overhead in setting up these processes around the AI. It’s essentially implementing some project management around an AI, which may require cultural adoption (developers must be okay interacting through these logs, etc.). But for an AI primary developer scenario, it’s almost necessary.
	•	Compatibility: These features are largely independent of programming language. Workdocs can track design decisions for a Rust project or a Python project alike. Triggers might need adjustment if build/test processes differ by language (but you just configure different events or required commands). Guardrails concept extends beyond code: for instance, if this template were used in a multi-language microservices environment, guardrails might cover things like “if modifying service interface, then update central API registry” – which the system can handle with appropriate triggers and docs. The only area to consider is tool support: for example, context usage tracking might rely on logging formats, but that’s minor. Since triggers can match generic conditions (file path patterns, prompt regex), we can adapt them to any stack (like catch if pom.xml changes in a Java project, etc.). Another point: if using multiple natural languages (for documentation or code), the system is mostly in English now. But one could internationalize the doc guides or have the AI speak Chinese for user-facing docs and English for code comments – the template didn’t specify but theoretically could adapt if needed (just more complexity in writing all guides bilingually). The core ideas remain intact.

3. Modular Development (Monorepo with Modules & Instances):
	•	Reliability: The modular approach enhances reliability by clear separation of concerns. Each module has a contract – so fewer unexpected side effects between modules (since interfaces are governed and tested). The module registry tracking dependencies means the orchestrator or developer knows what might be impacted by changes (so less chance to break something unknowingly – e.g., if Module A requires B, the registry reflects that, and the AI can decide to also update B if A changes something relevant). The hierarchical numbering of modules (1_Assign -> 2_Select -> etc.) provides a logical sequence in complex workflows, reducing confusion. The template’s focus on consistent structure in each module (the eight docs, etc.) means the AI doesn’t have to relearn how to handle each module – this consistency is reliable for AI understanding. Also, isolating workdocs per module ensures local issues are handled locally and don’t confuse other contexts, which is reliable in preventing cross-talk errors. One potential reliability challenge: modules often do interact (that’s why requires/provides exist). The orchestrator must handle cross-module tasks correctly. They allow cross edges with guardrails for interactions, which is good. It means if Module A needs something from Module B, the agent graph explicitly handles it, rather than the AI manually going off track. That formalization via requires metadata ensures such dependencies are not forgotten.
	•	Viability: Many large projects adopt modular or microservice structures, but few document them as rigorously as here. This template’s viability might depend on project size – for a small project, this might be overkill. But for a large project where AI is to be deeply involved, having the modules well-specified is a huge help for scaling the AI’s knowledge. The scaffolding (make ai_begin etc.) reduces the friction of adding modules with the required docs, which is great for viability because manually writing those for each module would be tedious; but the AI can automate it. Humans might find it heavy to maintain documentation for every module instance, but since the AI does it, viability is improved (the AI can handle boilerplate). The concept of Module Types with contracts is common in product-line engineering but not always in normal dev; however, it’s an excellent practice for interchangeability, and the AI can enforce it without bias (it will treat any module instance per its contract). There may be an overhead to updating the module registry and graphs whenever modules change, but again, scripts handle it. The question is: can the orchestrator effectively manage tasks in multiple modules concurrently or complex flows through modules? Possibly yes, because it’s like orchestrating microservices – doable since we can break tasks into sub-tasks per module (the agent triggers and tasks logs allow that). This is viable but complex – however, complexity is mitigated by the systematic approach provided.
	•	Compatibility: The module approach is conceptual and can apply to any language or even multi-language monorepos. For example, you could have modules in different languages as long as you define their type contracts and have a way to integrate (like perhaps via an API or messaging). The template likely assumed one repo with possibly both backend and frontend modules (hence Python + Vue in tech stack). In a polyglot scenario, we would define module types either segregated by layer (like frontend/ modules vs backend/ modules, each type with its contract style). The registry can include a field for language or build system if needed (like to know which test command to run). The main compatibility challenge is if using a decentralized versioning (like multiple repos) – this template is monorepo-centric, which is fine for many but not all projects. If modules were in separate repositories, some parts (like global routing and registry) would need adaptation (perhaps a central orchestrator repository or treating each repo as a module and orchestrating at a higher level). But if within one repo, it’s straightforward. Also, number prefixes in module names (1_Assign, etc.) are language-neutral and just ordering semantics – fine anywhere. The scaffold and scripts would need adaptation if, say, using a Windows environment (Make commands might need alternatives), but that’s more environment than language. In short, modular idea is broadly compatible with different tech, as long as the concept of modules with defined interfaces is applicable (which it usually is).

4. Automated Maintenance (SSOT, Registration, Guardrails, CI scripts):
	•	Reliability: Automated maintenance is all about preventing things from falling through the cracks, which directly improves reliability of the project infrastructure. The combination of Single Source of Truth + scripts yields consistency (no divergence in definitions between places). Guardrails and periodic checks catch issues early – e.g., a registry mismatch is flagged in CI, so it doesn’t go to production broken. The maintenance scripts integrated into dev_check and triggers means the system is self-validating to a large extent. This certainly handles most cases – any time someone (AI or human) forgets to update something, a script is likely to error out. Using generation (registry_gen) reduces manual copy-paste errors. Because of this, the project’s state remains healthier over time, unlike typical projects where documentation or config drift occurs. Telemetry and weekly dry-runs add another layer, checking for any “bit rot” in automation (like a trigger that no longer has effect because someone changed file structure – route-health would show an orphan). This proactive maintenance loop is reliable in catching issues that human maintainers might not notice for a long time.
	•	Viability: It’s one thing to propose all these checks, another to implement them and not drown in false positives. Viability depends on fine-tuning scripts to the project. Initially, it might take time to set up these scripts properly (ensuring they know where to look and what to validate). However, since it’s part of the template, presumably a base set is provided. Running them in CI is straightforward (just include in pipelines). The AI can run them frequently (like make dev_check before commit every time), which is different from a human who might skip due to time – the AI doesn’t mind. The small overhead of running these scripts is worth the prevention of bigger issues. Some script complexities: e.g., agent_graph_check needing a JSON schema for agent-graph – we have to write that schema and keep it updated if graph structure changes. That’s a bit of maintenance on maintenance tools. But not too bad if done initially. Another viable aspect: if something does fail, the AI reads the error and can usually fix it (since the errors are about consistency, the AI can likely deduce “oh I forgot to update X, let me do that”). So viability with an AI in the loop is high because tedious consistency fixes, which humans hate, can be automated by the AI itself. If a human team were using this without an AI coder, they might find it bureaucratic to update multiple YAMLs and run scripts – but with AI, that burden is gone. So ironically, the heavy process is fine because an AI will handle it swiftly.
	•	Compatibility: The maintenance strategies are general best practices (lint, checks, SSOT) that apply to any stack. We might need to adapt certain checks: e.g., contract_compat_check might be implemented differently for different API technologies (OpenAPI vs GraphQL vs gRPC). But the concept stands – you’d have some script doing it. Cost budgets or latency in constraints might need integration with specific monitoring tools. If using a cloud environment, guardrail triggers could integrate with pipeline approvals, etc. But since we’re largely local here, simpler. If the project uses a different CI system (like Jenkins instead of pure make), we can still call these scripts from Jenkins. None of these maintenance tasks are tied to a specific OS or language. They revolve around YAML, Markdown, and Python scripts – all cross-platform. For multi-language, some checks might multiply (like contract checks for each service language), but one can incorporate that (the template hints at database & API checks, you could add analogous for other).
	•	Another angle: If the code is not in Git or not a typical code project (imagine an AI writing content or doing something else), some specifics may change but housekeeping idea remains (like verifying outputs consistency).
	•	For a simpler or smaller project, one might not deploy all these checks (maybe skip telemetry if not needed). But they’re modular; you can enable as fits. That’s compatible with scaling – e.g., a small project might use doc routing and basic checks but not implement full telemetry pipeline at start.

Overall, evaluating:
	•	The design is quite comprehensive and forward-looking. It indeed handles most reliability concerns systematically.
	•	It’s feasible with some initial overhead that is justified if AI is doing majority of work (since AI doesn’t complain about extra YAML).
	•	It’s quite portable across domains; nothing inherently Python-specific except some example scripts names.

The main caution is ensuring the complexity is managed. There’s a risk that maintaining the maintenance (meta-work) could become cumbersome if the project is small or if humans unfamiliar with it join. But given the target seems to be heavy AI involvement, it’s acceptable complexity for that context.

14. Improvement Suggestions

While the current plan is highly detailed and robust, there are always areas to refine for clarity, simplicity, or broader applicability. Here are some suggestions for improvement:
	•	Simplify Onboarding for Humans: The framework is designed with AI developers in mind, but if a new human developer joins, the learning curve is steep. It would be beneficial to create a “Quickstart for Humans” guide (maybe in doc_human/) explaining the repository structure, the purpose of each key file (ROUTING.md, AGENTS.md, etc.), and how to work with the AI-driven process. This can prevent confusion and ensure human collaborators don’t accidentally violate the processes (e.g., commit without updating a registry). Essentially, treat human developers as another kind of agent that needs a policy/guide. This improves viability in mixed teams.
	•	Automate Lesson Integration: Currently, when an error occurs, a human/AI must add an entry to lessons.md and possibly create a new trigger or guardrail. We could improve this by having a script or agent to automate parts of the post-mortem process. For example, after a failure, the AI could prompt itself (or be triggered) to fill a lessons log template and suggest a new guardrail trigger. This might involve some NLP to summarize an error, but even a partial automation would ensure no error goes undocumented. This would further close the learning loop without relying on someone remembering to document lessons.
	•	Dynamic Context Management: The context logs are pruned to avoid token overload (120-line hub). We could enhance this by implementing a sliding window or summarization mechanism for context logs. For instance, if a context section grows beyond 40 lines, the system could automatically summarize older entries and replace them with a concise summary (beyond just archiving to subdocs). This would keep even the sub-logs short for the AI. The guide partially does this by splitting into subdocs, but automatic summarization (perhaps using an AI agent itself) could be employed for the lessons log or decision log after a while, to keep them tight.
	•	Unified Configuration Schema: The config management section is thorough, but perhaps too segregated (global config vs module config, plus config-index.yaml). We might simplify by using a single source (like a YAML tree) that is split logically by the script, rather than multiple YAMLs. For example, maintain one config_index.yaml with sections for global and per-module overrides, then have scripts to show either. This reduces the number of files. However, this is a trade-off as separation has its benefits (like security for secrets). If possible, providing tooling to automatically reconcile global and module config definitions (ensuring modules don’t define keys not known globally, etc.) could improve reliability further.
	•	Enhance Multi-language Support: If we foresee projects involving multiple programming languages (e.g., a Java service and a Python service in one repo), the template could include language-specific subdirectories (like a routing for each language’s conventions and tools). Currently the template is geared to one stack. To improve compatibility, we can generalize certain guides (or have variations) – for example, have a doc_agent/guide/testing-guide.md that branches into “for Python use pytest, for Java use JUnit” etc. Or maintain separate guides: python-testing.md, java-testing.md and ensure the routing picks the right one based on module or context (maybe via tags in module registry indicating language). This addition would make the template more widely applicable out-of-the-box.
	•	Monitor AI Behavior and Costs: The telemetry covers performance of routes, but we could add a cost monitoring and optimization agent in future (the text mentioned cost budgets in constraints field, but no concrete use). Perhaps include a placeholder in constraints like max_tokens and have the telemetry track if tasks exceed it. Or an agent that analyzes the length of conversations or files and suggests refactoring to reduce token usage (like splitting docs further or archiving old content). This would ensure the project remains cost-effective and the AI usage is optimized. It’s an improvement as the system scales with lots of documentation.
	•	Improve Trigger Granularity and False-Positive Handling: Currently triggers fire on pattern matches (file path or prompt). Over time, more triggers will be added as project grows. We should plan for trigger conflict resolution or prioritization. Perhaps include in agent-triggers.yaml a way to set an order or exclusivity (the schema might allow priority, it’s mentioned priority in triggers ￼ ￼). Ensuring the orchestrator knows how to handle multiple triggers (like should it run all applicable triggers or just the highest priority) will be important. Documenting that strategy or implementing a rule (like critical block triggers override others) could be clarified. This is more of a fine-tuning suggestion.
	•	User Feedback Loop: Introduce a mechanism for human feedback on AI outputs beyond guardrails. For instance, if a human reviewer sees a suboptimal AI decision that isn’t guardrail-violating, how does that feed back? We might add an “🛈 Improvement Suggestions” section in retrospective or a label in tasks that a human can fill to guide the AI next time. Or encourage the AI to periodically ask for human feedback on completed tasks (this could be structured in ai/maintenance_reports/retrospective.md). Having a spot for continuous improvement suggestions (from humans or the AI analyzing itself) and then tracking resolution of those suggestions can further improve the system’s reliability and performance over time. Essentially, treat improvements like tasks so they don’t get lost.
	•	Template Simplification for Small Projects: Perhaps provide a scaled-down mode of the template where not all components are active. For example, if a project is simple, one might disable module registry (if there’s only one module), or skip telemetry if performance is not a concern. We could include instructions in the documentation on which parts can be toggled or are optional. This would improve viability for a wider range of project sizes. The template is comprehensive, but making it somewhat modular itself (in terms of which systems are activated) could attract more usage. This isn’t a change to the system per se but to its adaptability.
	•	Update Frequency and Checks: One improvement to maintenance is scheduling some of those checks (like route lint, trigger dry-run) to run regularly (which they suggest weekly). We could implement a CI cron job that runs maintenance scripts and perhaps even auto-opens a maintenance report issue if something needs attention. That way, if the AI is not actively working for a while, any drift is caught when it next wakes up. It’s a small process suggestion to ensure continuous maintenance.

Each of these suggestions aims to either reduce complexity where possible, broaden applicability, or add further assurances. The current design is strong; these are refinements to make it even more user-friendly and adaptable.