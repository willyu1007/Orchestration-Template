# AI-Integrated Repository Template – Development Plan

This document outlines a comprehensive development plan for building an AI-integrated repository template, broken into sequential phases. Each phase describes objectives, scope, rationale, and expected outcomes. The plan assimilates initial requirements and subsequent design modifications, ensuring that all critical mechanisms, conventions, and principles are covered (with priority given to the modified specifications where differences occur).

## Phase 1: Repository Documentation Standards and Setup

Objective: Establish a robust documentation framework as the foundation for an AI-friendly repository. This includes standardizing document formats, naming conventions, directory structures, and ensuring documentation is optimized for AI consumption.

Scope: In this phase, we will set up the core documentation conventions across the repository. Key tasks include:

- **Defining document roles and audiences**: All docs must clearly indicate if they are for AI or human readers (in front matter) and adhere to role-specific guidelines. AI-oriented docs (like routing guides, agent policies, capability indexes, etc.) should be kept concise (typically <150 lines) and highly structured, whereas human-oriented docs (e.g. README) have no strict length limit but should indicate that AI can ignore them.
- **Enforcing format and style rules**: Use UTF-8 encoding and Markdown for all documentation. All content generation and edits (unless user specifically requests otherwise) should be in English. Avoid unstructured prose or excessive emoji/symbol usage beyond simple status icons.
- **Establishing naming conventions for files, directories, and code**: for example, documentation files use kebab-case (e.g. data-spec.md), environment constants use UPPER_SNAKE_CASE, Python variables/functions use camelCase, classes use PascalCase, and database entities use snake_case. Standard naming patterns prevent confusion and collisions (e.g. using a <topic>-<type> or <topic>.<type> suffix for clarity).
- **Organizing documentation directories**: All AI-facing docs will reside under dedicated paths like doc_agent/ (for AI-specific knowledge) or within module-specific doc/ subfolders, while human-facing docs (aside from a high-level README) will be under doc_human/ or clearly separated to avoid mixing contexts. Work-in-progress context notes will be in ai/workdocs/ (with active and archive subfolders) and internal reports in ai/reports/, ensuring the AI does not accidentally load irrelevant human-oriented or archival content.
- **Preparing a front matter schema for all AI-readable docs**: Each such file must begin with a standardized YAML front matter declaring fields like audience (must be "ai" or "human", not both), purpose, doc_role (e.g. quickstart, guide, spec, contract, etc.), doc_kind (e.g. router, agent_policy, capability_index, etc.), plus metadata like updated_at and ownership tags. Leaf-node docs (final content docs that routing points to) should include route_role: leaf in front matter to mark them as endpoints in the doc routing tree. We will also introduce schema files (e.g. spec/front-matter.schema.yaml, etc.) to define these front matter structures and enable validation of documentation format.
- **Implementing the "progressive disclosure" principle in documentation design**: Top-level AI entry docs (like ROUTING.md) will remain extremely lightweight – only listing essential roles, navigation links, or common commands, without lengthy background text. Deeper details are deferred to lower-level guides or specs, referenced via links, so the AI reads only what is needed for the task at hand. High-level docs answer "what to do, and where to find it"; mid-level docs serve as task-specific manuals or policies; low-level docs provide full technical details or examples. This structure helps the AI load minimal content initially, then fetch more detailed documentation stepwise as needed (avoiding token overflows). We will codify this progressive loading approach as a best practice for document writing (e.g. by adding notes in docs about which next document or section to read for deeper info).
- **Finalizing core documentation files to be present in the repository root and modules**: By convention, every directory that serves as a navigation hub will contain a ROUTING.md (for navigation) and, if that directory contains AI-executable components, possibly an ABILITY.md (formerly CAPABILITIES.md, for capability indexing) and an AGENTS.md (for local agent policies). However, based on revised consensus, the root of the repository will hold the primary AGENTS.md that acts as the single global policy and guardrail entry for the whole project (consolidating global rules). Module directories may have their own AGENTS.md if they encapsulate executable logic, but the requirement to have all three files in every subdirectory has been relaxed to reduce redundancy. If a directory is purely for documentation or does not host active agents, it can omit local AGENTS.md or ABILITY.md, and higher-level routing will simply link to the relevant docs in that module.

**Rationale**: A consistent documentation standard is crucial because the repository's "AI-friendliness" depends on how well the AI agent can navigate and understand the docs. Clear separation of human vs AI documentation prevents the AI from reading irrelevant or verbose material. The structured front matter and regulated format ensure the AI can reliably parse the docs to confirm it's reading the correct file for its context. Progressive disclosure addresses context window limits by guiding the AI to only load small, relevant pieces of documentation as needed. Strict naming and placement conventions reduce ambiguity and support automated indexing of documents. Overall, these standards lay the groundwork for the AI to perform retrieval, planning, and coding tasks efficiently and safely, by providing a predictable knowledge landscape.

**Expected Outcomes**: By the end of Phase 1, the repository will contain a well-defined documentation hierarchy and style guide. All initial documentation files (global README, ROUTING.md at the root and any core subdirectories, global AGENTS.md, global ABILITY.md, etc.) will be created with proper front matter and content structure, serving as templates for future docs. We will have documented the naming conventions and formatting rules in a visible place (perhaps a doc_human/CONTRIBUTING.md or similar). Tools or linters will be set up to validate these conventions (e.g. checking front matter presence, line count limits, naming patterns) as part of CI. This provides a solid base upon which all further development phases will build. Documentation quality at this stage is a deliverable: all AI-oriented docs should pass a "doc lint" check (ensuring no broken links, correct front matter, and adherence to the structural schema), and the doc routing map (to be built in Phase 2) will recognize all these docs.

## Phase 2: Document Routing System Implementation

Objective: Build a multi-level documentation routing system that directs the AI to relevant information efficiently. This system will use hierarchical ROUTING.md files to map high-level contexts to specific topics and situations, ensuring the AI knows “where to go look” for any given query or task.

Scope: In Phase 2, we create the routing structure that spans the entire repository and its modules. Key activities and components include:

- **Designing the ROUTING.md files for each relevant directory level**: Each ROUTING.md will contain a structured list of context_routes entries, typically in a three-tier format: scope → topic → when (a hierarchical guide that filters by high-level domain, then specific subject, then contextual condition). For each combination, the routing entry will specify target_docs – links to the actual document(s) that the AI should load for that situation. This is akin to a library index: telling the AI which "section of the library" and which "book" to consult for its current goal.
- **Ensuring coverage of all knowledge domains**: The root ROUTING.md will list major scopes (e.g. "Documentation & Standards", "Execution – Database", "Execution – API", "Module Guides", etc.) mapping down to module-specific or topic-specific ROUTING.md files or docs. Module-level ROUTING.md files (if present) will further route into that module's internals (or directly to that module's leaf docs if the module is simple). Every AI-relevant document must be reachable via some path of ROUTING.md references (this will be verified via a doc map audit).
- **Integrating project-specific or requirement-specific documents**: A new rule established during design review is that any important project-specific requirement or background doc must be transformed into an AI-readable form and hooked into the routing tree. We will identify if any such docs exist (e.g. design specs, requirement docs provided by users) and ensure they appear as leaf nodes or linked guides in the routing system. This guarantees the AI does not overlook custom project details – essentially treating "requirements → documentation → routing" as a mandatory chain for AI awareness.
- **Clarifying the registration scope of routing**: We confirm that only routing files themselves and their direct leaf nodes are explicitly listed in context_routes indices. We will not attempt to register every subsection or example file – sub-sections within a leaf doc should be discovered by the AI progressively as needed, not enumerated in the routing (to avoid overloading the index). Human-oriented docs remain outside the routing index entirely. This approach keeps the routing tables focused and maintainable.
- **Implementing routing front matter and metadata**: Each ROUTING.md will include a front matter (with doc_kind: router, audience, etc.) and a standardized structure. The content will likely be a table or list mapping scope/topic/when to target_docs. We will also maintain a central YAML file (e.g. doc_agent/orchestration/doc-node-map.yaml) that acts as an authoritative registry of all doc nodes and their roles. This map will help automated tools verify consistency between the routing files and actual docs.
- **Adhering to the layered routing responsibility**: Top-level ROUTING.md focuses on broad "what to read/where to go" directions, while lower-level ones might include more context-specific instructions or link to quickstarts and guides. We avoid duplicating content at multiple levels; each ROUTING.md only points to the next level or final docs, rather than repeating those docs' details. This keeps each route file concise and the overall navigation clear.
- **Creating initial router entries for core topics**: For example, the root ROUTING.md will likely have an entry like Scope: "Documentation & Standards" – Topic: "Repository Conventions" – When: "setting up or updating docs" that leads to a documentation guide or standard (possibly in doc_agent/guide/ directory). Another example: Scope: "Execution – Database" – Topic: "Schema Changes" – When: "modifying database schema or migrations" pointing to the DB subsystem's docs (like a DB spec or migration guide). Similarly, Execution – API entries will route to API gateway and contract docs. We'll populate these routes based on the content developed in later phases (Phase 6 for DB, Phase 7 for API, etc.), but the placeholders and structure will be established now.
- **Setting up tooling to manage routing**: We plan to implement or configure scripts to validate and refresh the routing index. For instance, a doc_node_map_sync script can parse all ROUTING.md files to build the doc-node-map.yaml, or vice versa, ensuring no dead links or orphaned docs. A make route_lint command will be introduced to catch errors like missing route targets or inconsistent front matter in referenced docs.

**Rationale**: The routing system is the backbone that enables our AI orchestrator to fetch the right context. Without it, the AI might waste time searching or load irrelevant documents. By systematically mapping contexts to documents, we increase task planning efficiency and reliability – the AI will have explicit guidance on where to retrieve instructions or reference materials for any given subtask. The structured scope-topic-when format also encodes institutional knowledge about when certain docs are relevant, helping the AI with better planning decisions (essentially encoding part of the AI's prompt strategy into the filesystem). The modifications we integrated (like including project-specific info in routing) further ensure that customizing this template to a specific project will not result in any AI-accessible knowledge gaps. Maintaining an authoritative doc map and using automation for consistency checks guards against human error in updating routes – which is critical as the system grows and evolves.

**Expected Outcomes**: By the end of Phase 2, every directory of interest will contain a properly formatted ROUTING.md if needed, and the entire repository's documentation graph will be connected. We expect to be able to perform tests such as: starting from the root ROUTING.md, an AI should reach any relevant piece of documentation within a few hops (the aim is typically no more than 2 jumps from the root to find a needed leaf). The doc-node map and registry will be up-to-date, listing all route entries and their target docs without mismatch. Acceptance criteria include zero broken links or missing references in routing (verified via route_lint), and that every leaf doc referenced is properly labeled with matching front matter and purpose. This phase lays the navigational groundwork which subsequent phases (especially those introducing capabilities and modules) will plug into.

---

## Phase 3: Capability Orchestration Graph and Ability Index

Objective: Develop the capability orchestration system, which includes defining an agent graph (or function routing graph) and an ability index. This system represents the functional “abilities” of the repository (both low-level methods and higher-level AI-driven workflows) and how they orchestrate together. Essentially, we are building the AI’s toolbox and the map of how tools can be composed.

Scope: Phase 3 involves creating the data structures and documentation for representing both atomic capabilities and orchestrated processes:

- **Establishing a two-tier capability model**: We adopt the design of two tiers of functional nodes:
  - **Low-tier capabilities**: Fine-grained, often implementation-specific actions. These include repository-internal scripts, external service calls (to model providers or APIs), database operations, etc. We categorize low-tier nodes by kind, e.g. script (internal script), mcp (external microservice or cloud provider call), api (external API invocation). Low-tier capabilities are meant to be interchangeable building blocks.
  - **High-tier abilities**: Coarse-grained "abilities" which are orchestrated workflows or agents combining multiple steps or decisions. There are typically two kinds: workflow (a deterministic pipeline or sequence, often calling multiple scripts in order) and agent (an AI-driven decision-making procedure that may call on various low-tier methods dynamically). High-tier nodes represent the tasks that the orchestrator will schedule and manage; these appear as the actual actionable units for the orchestrator, whereas low-tier are invoked inside these high-tier processes.
- **Implementing capability registries**: We will maintain two YAML-based registry files to list and define these nodes:
  - **A method registry for low-tier capabilities** (proposed file: doc_agent/orchestration/method_registry.yaml). Each entry includes a unique id of the form base.<domain>.<action> (for example, base.db.migrate for a database migration script). The entry will detail how to execute it, e.g. if kind is script, provide the script path and function; if kind is mcp, the target service and action; if api, the external API endpoint or key, etc. We also list input/output schema references, an owner (responsible maintainer or module), and tags for searchability.
  - **An ability registry for high-tier abilities** (proposed file: doc_agent/orchestration/ability_registry.yaml). Each entry has an id of the form able.<domain>.<intent> (e.g. able.db.apply_all_migrations for a workflow that applies migrations, or able.repo.maintenance_assistant for an agent that helps with repository upkeep). Fields include the tier (high), kind (workflow or agent), and details like the steps (for workflows, a sequence of low-tier ids to call, with dependencies) or capabilities it can call (for agent kind, a whitelist of low-tier ids it may invoke). The entry will also have an owner_module (linking it to the module responsible) and possibly domain and intent fields to clarify purpose.
- **Renaming the capability index document**: Per revised standards, we will use ABILITY.md as the name for the capability index documentation (replacing the older term CAPABILITIES.md). Each directory or module that exposes capabilities will have an ABILITY.md summarizing the abilities (and possibly key low-tier methods) available in that scope. The global repository ABILITY.md will serve as the canonical index of high-level abilities accessible to the orchestrator (the "ability index" for the whole repo). It will be marked with doc_kind: capability_index (or perhaps ability_index if we align naming) and limited to ~250 lines to keep it concise. This doc will list capability nodes and their descriptions/IDs, likely grouped by domain or module, which aids the AI in capability discovery.
- **Defining owner and exposure attributes**: We will ensure each capability entry includes an owner_id or owner_module field tying it to a module or domain owner. For high-tier abilities, only one module can own a given ability – that module is where the ability's implementation lives and its local policy applies. We also introduce an exposure level for abilities (e.g. public vs internal). Public abilities are ones that represent business-level operations or endpoints meant to be directly orchestrated (entry points for cross-module or external use), while internal abilities are more like subroutines or "library" functions intended for reuse by other abilities but not invoked as standalone tasks. This distinction guides both documentation (e.g. we may only list public ones in certain summary tables) and orchestrator behavior (the orchestrator will primarily schedule public abilities for top-level tasks, while internal ones appear only as dependencies).
- **Integrating triggers and guardrails with the graph**: The capability model will be extended to link with our automation triggers system (to be fully developed in Phase 8, but conceptually prepared here). We add a safety_profile on ability entries to reference any trigger or guardrail IDs relevant to that ability. For example, an ability like func.db.apply_all_migrations could list that it's associated with a trigger db-migration and a guardrail policy require_approval_for_prod, with an overall risk_level: high. Conversely, in the trigger definitions, we will include a target field pointing back to relevant ability ids. This two-way linking ensures traceability: if a certain ability is high-risk, we can quickly find what triggers/approvals guard it, and if a trigger fires, the system knows which ability (or module) it's safeguarding. Although triggers and guardrails will be fully implemented later, we will define the data schema for these links now and include a couple of initial examples in the registries for testing.
- **Setting registration rules and maturity levels**: To prevent uncontrolled growth of the capability set, we introduce governance for adding new capabilities:
  - **Low-tier (method) registration**: Low-tier methods are relatively unconstrained – any needed script or call can be added to the method registry. However, temporary or ad-hoc scripts are not allowed to be registered. All methods must be intended as reusable or permanent capabilities (ephemeral one-off scripts should remain in a temp directory and out of the registry).
  - **High-tier (ability) registration**: We implement a maturity field for high-tier abilities. New abilities can start as candidate (meaning they are experimental or under development). Candidate abilities are available for AI and developers to see and possibly use in testing, but they will not be included in the official ability index or actively orchestrated in production. Once an ability is proven (e.g. has at least 2 test cases, documentation, stable interface), it can be marked stable. Only stable abilities get fully integrated: they appear in ABILITY.md and are considered by the orchestrator for live task execution. Our CI pipeline will enforce that rule: if an ability is not stable, it should not be listed in the formal index or invoked in production workflows. The orchestrator will filter out non-stable abilities when planning. This provides a safety net: experimental features won't accidentally trigger critical operations until vetted.
- **Developing capability registration tools**: To streamline adding new capabilities, we plan to build interactive scripts. For example, a register_lowtier.py script that guides developers (or the AI) through adding a new low-tier method entry. It would prompt for all required fields (enforcing none are missing), and validate based on kind (for instance, if kind is script, ensure a script path and entrypoint function are given; if api, require API endpoint and credentials, etc.). It would also run a lint (e.g. check for duplicate IDs or broken references). Similarly, we might create register_ability.py for high-tier nodes, including logic to set initial maturity to candidate and ensure an owner and at least a stub doc/policy exists for it. These tools help maintain the single source of truth principle by funneling all changes through a controlled path.

**Rationale**: The orchestration graph is the heart of the AI integration: it formalizes what the AI can do (capabilities) and how complex tasks are composed (abilities). By separating low-tier and high-tier, we achieve modularity – low-tier methods can be swapped or reused, and high-tier workflows encapsulate business logic. This prevents the orchestrator from getting overwhelmed by fine details (it "sees" only the high-tier graph) and simplifies enforcing policies (guardrails mainly attach to high-level abilities rather than every atomic action). The naming convention (base.* vs able.*) and tier field explicitly encode this separation, reducing ambiguity. Introducing maturity levels is crucial for safe AI co-development: it creates a staging ground for new abilities, ensuring only vetted functions become part of the core execution set. The registry tools and schema validations aim to mitigate errors in what will be a complex network of references (ids, dependencies, triggers, owners, etc.). During design discussions, the team identified that mismanaging these relationships could lead to orchestration errors or security gaps, hence the emphasis on a unified configuration schema and automation for these relations. A rigorous registry also means the AI and devs can query a single source (the YAML or the ABILITY.md) to discover available capabilities and their usage, improving transparency and reuse.

**Expected Outcomes**: By the end of Phase 3, we will have:

- The method registry and ability registry YAML files in place, each populated with initial entries covering the basic operations and workflows that come out-of-the-box with the template (e.g. methods for running tests, applying DB migrations, generating documentation, etc., and abilities like setting up a new module, performing maintenance tasks). These entries will incorporate fields like owner, kind, etc., as described, and follow the new naming scheme.
- A global ABILITY.md documenting the high-tier abilities (with brief descriptions and references to their registry definitions or docs). For consistency it will reflect the YAML content (we may generate parts of it from the registry automatically). This doc will be validated for line count and completeness.
- Initial integration of triggers/guardrails in the data model (even if the actual trigger handling logic comes later). We will confirm that for an example risky ability (like database schema migration), the ability entry in YAML has a safety_profile linking to a trigger id, and that a corresponding trigger entry (to be defined in Phase 8) can refer back to the ability. This round-trip link will be tested by our consistency scripts.
- The agent (or orchestrator) graph model constructed. We might have a file agent-graph.yaml representing the directed graph of high-tier nodes and how they connect (this could be derived from the ability registry, especially for workflows which define edges via depends_on). At minimum, all high-tier abilities and their relationships (calls or steps) will be clear and documented. Tools like agent_graph_check.py will be introduced to validate this graph structure (e.g. no cycles unless explicitly allowed, all dependency nodes exist, etc.).
- All of this should pass an initial capability lint: running make capability_index_check and scripts/registry_check.py to ensure our ABILITY.md, registry YAMLs, and agent graph are in sync. This serves as a baseline for future changes. The ability index and graph will then be ready to integrate domain-specific abilities from upcoming phases.

---

## Phase 4: Global and Local Agent Policy Documentation (AGENTS.md)

Objective: Define and implement the agent policy documents that govern what the AI (code model) is allowed and expected to do. This includes a global policy (root AGENTS.md), an aggregated index of policies (doc_agent/AGENTS.md), and module-specific policy files, each outlining rules, guardrails, and important procedures for the AI when working in that context.

Scope: In Phase 4, we focus on the content and structure of AGENTS.md files:

- **Creating the Global Agent Policy (root AGENTS.md)**: This is the top-level policy document for the entire repository. It will carry audience: ai and doc_kind: agent_policy in front matter, and serve as the first stop for the AI to understand global rules. Key elements to include:
  - **Global guardrails and scope**: A clear definition of what the AI can and cannot do globally. For instance, list prohibited actions (e.g. "do not directly modify production config"), required approvals for certain operations, environment restrictions, etc. This sets the boundary conditions for AI contributions.
  - **Pre-execution requirements**: Policies like "all changes must pass tests and lint before commit" or "document updates must accompany code changes" can be stated here as mandatory steps (the AI will be instructed to always follow these before concluding a task).
  - **AI reading list (must-read docs)**: Provide a curated list of essential documentation the AI should read in full at least once. For example: the documentation routing principles and progressive reading rules (so the AI understands how to navigate docs), an explanation of workdocs and context recovery usage, and an overview of triggers/guardrails mechanisms. These are critical for aligning the AI with the repository's working model, essentially bootstrapping the AI's knowledge of how to work within our system.
  - **Conditional reading list (optional)**: List additional guidelines the AI should load when relevant. For example, a Testing Rules guide that explains when and how to run tests, a PR Rules guide detailing how to structure pull requests (commit message conventions, branch naming, etc.), and any commit/branch conventions summary. The global policy can instruct: "when preparing a PR, read the pr-rules section" and "when about to run tests, refer to test-rules," etc. (We will ensure such guides exist in the docs).
  - **Entry points to routing**: The global policy should explicitly point out where the AI can find the routing indices. E.g., "Use ROUTING.md at the repository root as the canonical starting point for documentation lookup, and ABILITY.md for the list of functional abilities". This helps the AI quickly orient itself to find information or tools.
  - **Workdocs usage**: Clarify expectations around workdocs (e.g. "Always log your plan and key decisions in ai/workdocs/active/plan.md or context.md during task execution, and follow the context recovery guidelines to resume tasks"). Also mention how and when to archive or summarize (though the automation will handle that, the AI should be aware of the process).
  - **Trigger/Approval basics**: Outline the existence of triggers and guardrails, their basic priority levels and what it means if something is blocked, and how the AI should react (e.g. if a trigger requires human approval, the AI should pause and request it in the workdoc, etc.). This primes the AI to cooperate with the safety mechanisms.
- **Creating the Policy Index (aggregated doc_agent/AGENTS.md)**: This file will aggregate references to all specific policy documents across domains or modules. Its role is like a table of contents for policies, making them easily discoverable by the AI. It will:
  - List domain-specific or specialized policies (like "Guardrail guidelines", "Documentation Ops policy", "Database Ops policy", etc.) and provide stable anchor links to the relevant sections in those docs. For example, it might have an entry for "Database Operations – see db/engines/postgres/AGENTS.md" or anchor into a consolidated guardrail guide.
  - Essentially function as a map of all agent policy docs so the AI can search within it to find any policy by keyword (rather than traversing the whole tree). This file will have front matter like doc_kind: agent_policy_index (if we define such a role) and audience: ai.
  - It should not duplicate the global rules or override them. It's strictly an index; global policy stands above it. It also won't contain actual policy details, just links or one-line summaries, to keep it lightweight and focused.
  - This index helps when triggers or orchestrator want to point the AI to a specific policy snippet: they can reference an anchor in doc_agent/AGENTS.md which redirects to the actual file, decoupling triggers from the raw file paths (making reorganization easier without breaking references).
- **Module-Level AGENTS.md (for each module that needs it)**: We will define when and how to use local policies:
  - If a module instance has any special execution logic or risky operations, it should have its own AGENTS.md inside modules/<mod_id>/. That file is a leaf in the doc routing (it will be referenced by the module's ROUTING.md or by an entry in the policy index) and contains rules specific to that module's domain.
  - The module AGENTS should cover local concerns: e.g., "Module X can only modify files in its own directory", or "Operations in Module Y require notifying Module Z's owner" if such cross-effects exist. For simpler modules, this might just summarize its functionality, known risks, and any module-specific tool usage guidelines.
  - If a module's policies are too complex to fit neatly (say the module itself has multiple subdomains or scenarios), the module's AGENTS.md can act as a mini-index or router to more detailed policy docs (like a policy.md or other guides in its doc folder). However, in terms of our overall routing, the module's AGENTS.md is the end point for the AI to load; from there, progressive reading can lead it to subsections or linked docs as needed.
  - Creation or modification of a module's AGENTS.md will follow the same process as any doc: update the module's ROUTING.md or parent routing to ensure it's referenced, maintain front matter, etc. We will highlight in the contribution guide that adding a module AGENTS requires updating the doc routing (so it's not orphaned).
- **Standardizing content requirements for all agent policy docs**: According to the initial spec, every agent policy document (global or local) should include certain sections to comprehensively cover safety and process rules. We will use this as a template:
  - **Access Control**: List permitted and forbidden actions, file paths, commands, etc., for that agent context (e.g. "Allowed: create files under module_X/, Prohibited: deleting customer data without archival, etc.").
  - **Execution Guardrails**: Define what must happen during execution, such as tests that must pass, specific documentation that must be updated (like "update CHANGELOG.md when modifying DB schema"), any required dry-run or approval steps, and fallback strategies if something fails.
  - **Tools & Dependencies**: Enumerate which scripts or tools the AI can use in this context, along with any prerequisites. For example, "This module's agent may call scripts/calc_stats.py (registered as ability ID X), and relies on Python library Y being available". If certain commands have wrappers, instruct to use those (e.g. use make migrate instead of calling psql directly).
  - **Escalation & Logging**: Specify what conditions trigger escalation (e.g. if a certain operation is detected by a trigger as high risk), who (role or team) needs to approve, and what logs or workdoc entries need to be maintained (like "whenever this guardrail triggers, log details to handoff-history.md and tag @DBA"). Define how the AI should hand off to a human or fallback procedure if it hits a policy it cannot satisfy.
  - **Related Routes/Capabilities**: Provide cross-references to relevant routing or ability entries. For instance, mention which ROUTING.md scope or topics this policy relates to (so the AI can confirm context), and list any ability IDs from ABILITY.md that are closely tied to this policy (for quick lookup). This essentially ties together documentation references so the AI can jump between what it's doing and the rules that govern it.
  - These sections will be included in the global AGENTS (with a broad scope) and tailored in each module's AGENTS as needed. We will keep each AGENTS doc within ~200 lines and use bullet points or tables rather than long paragraphs for clarity.
- **Setting up maintenance and linking**: We outline that whenever a capability or trigger is added/changed, we may need to update the relevant policy doc. The change management process will be: update registry/graph → update routing (if needed) → update policy (AGENTS.md) → update ability index (ABILITY.md) → record in workdocs. We'll note this in the contribution workflow documentation, to keep all components in sync.
- **If a directory does not allow any execution** (e.g. purely informational), it can omit AGENTS.md entirely as noted earlier. Also, if multiple subdirectories share the same policy, we might allow one central AGENTS.md at a higher level and have sub-routes reference it to avoid duplication. We'll ensure our routing supports that via related_routes or similar references.

**Rationale**: The agent policy documentation is effectively the contract between the human maintainers and the AI agent. It ensures that the AI's contributions remain within safe bounds and follow project conventions, acting as in-line documentation of governance. By having a global policy, we set a common ground for all tasks (the AI reads this first to internalize global rules). By having module-specific policies, we capture nuances of each domain (for example, database changes vs. UI changes might have different approval requirements). The design discussion highlighted the need for a global must-read checklist to speed up the AI's orientation – that's implemented via the global AGENTS reading lists, so the AI knows about the repository's working model from the get-go. The separation of a policy index (doc_agent/AGENTS.md) prevents cluttering the routing tree with too many policy references and allows triggers to link directly to specific rules without making the policies themselves part of the routing logic. Standard content sections (Access Control, etc.) ensure no aspect of safe execution is overlooked when writing these docs, and that uniform structure helps automated linting or even an AI agent reviewing them for completeness. Maintaining these policies will be an ongoing effort; we will enforce updates by linking them with changes in capabilities and triggers (with checks like verifying every ability's graph_node_id matches an entry and that each has a corresponding policy entry if needed). Ultimately, these policy docs equip the AI to self-regulate to an extent, by checking "am I allowed to do X?" or "what procedure must I follow for Y?" in the relevant AGENTS.md before it proceeds with an action.

**Expected Outcomes**: By the end of Phase 4:

- The root AGENTS.md will be written, containing all the sections described (global rules, reading lists, etc.). This document will effectively serve as the AI's primary guide to working in the repository.
- The aggregated doc_agent/AGENTS.md index will be created with entries linking to all other policy docs (including anchors for sections of the root policy or specialized domain policies).
- Each module that requires it will have an AGENTS.md with content specific to its context. For initial modules (like the default ones for DB and API to be introduced in later phases), their local policies will be drafted in this phase as exemplars.
- All AGENTS docs will have proper front matter (e.g., doc_kind: agent_policy, graph_node_id if they correspond to a graph node, etc.). We will verify that every high-tier ability node that requires an agent policy has one documented and accessible via the index.
- The content will be validated with an agent lint tool: for example, a script agent_lint.py that checks each AGENTS.md for required sections, ensures their front matter is consistent with registry (each one should list a graph_node_id if applicable and that ID exists in the graph), and verifies that any capabilities or routes they reference actually exist. This automated check will become part of make dev_check.
- With policies in place, we expect an AI agent reading them would have clarity on how to proceed with tasks safely, and triggers (Phase 8) will reference these policies when needed (e.g., a DB schema change trigger can point the AI to the "db-migration-policy" section in a policy doc for instructions). This phase thus produces the rulebook that will govern AI actions in the subsequent development phases and beyond.

--- 

## Phase 5: Modular Architecture and Module Initialization

Objective: Implement the repository’s modular architecture, defining how modules are structured, related, and initialized. This phase ensures that new modules can be added to the repository in a consistent manner using a scaffold, and that the concepts of module hierarchy, types, and instances are clearly established for both AI and humans.

Scope: Key tasks and components in Phase 5 include:

- **Defining the module directory structure and naming**: All modules reside under the modules/ directory as subfolders. We standardize module folder naming as mod_<domain>_<name> (for example, mod_order_management for an order management module). This naming scheme allows easy identification of domain and function, and we will enforce it via lint checks (to catch any non-conforming module names).
- **Establishing the required content for each module instance**: When a new module is created, a set of files and subdirectories must be generated by default:
  - A YAML manifest file (e.g. modules/<mod_id>/MANIFEST.yaml) capturing key metadata about the module (its module_type, any dependencies on other modules, authors, etc.).
  - Core AI documentation files in the module root:
    - `AGENTS.md`: the module's local policy (if the module has any execution or special rules; see Phase 4).
    - `ABILITY.md`: the module's local ability index listing any abilities defined within that module (this complements the global ability index by scoping to the module's capabilities).
    - `ROUTING.md`: the module's documentation router if the module contains multiple documentation topics or sub-guides. For simple modules, this may not be needed (the module can be treated as a leaf in the global routing).
  - Standard subdirectories for module content, for example:
    - `doc/`: for additional module-specific documentation (guides, contracts, specs relevant to this module).
    - `workdocs/`: for module-specific work-in-progress notes (though main workdocs are often at repo level, module-specific tasks can be tracked here if needed).
    - `config/`: for module configuration files.
    - Code directories, which might include `backend/`, `frontend/`, `core/`, etc., depending on technology (the exact structure will follow conventions, e.g., a web module might have api/ or service/ directory).
    - A temporary `init/` subdirectory during module creation (discussed below).
- **Developing a module scaffolding process**: We will implement a script (e.g., under scripts/scaffold-module/) to automate module creation. The process:
  1. Interactive information gathering: The AI (or developer) will provide or be asked for necessary details like module domain, name, type, and any initial configuration (like does it connect to a database, etc.).
  2. The scaffold script then generates the module skeleton: creates the directory modules/<mod_id>/, populates it with the files listed above (filling in placeholders in YAML front matter or manifest with provided info, e.g., module name, type, date, owner).
  3. It creates an modules/<mod_id>/init/ folder which contains any transient resources for the setup – for instance, a quickstart guide or questionnaire that the scaffold used to generate the module, or some generated examples/tests. This init/ folder acts as a staging area during creation (e.g., perhaps storing intermediate answers or an initial to-do list for the module). After the module is fully initialized and verified, this folder will be removed (the idea is that everything needed is either in the manifest or documentation, and any ephemeral instructions don't clutter the final repo).
  4. The scaffold will also update central indexes: it will add an entry to a module registry YAML (we plan a doc_agent/modules/instance_registry.yaml) listing the new module, its type, and relationships. It will update any global lists in the root README or routing if necessary (e.g., adding the module to a table of contents of modules in the root docs).
  5. The scaffold might also insert a skeleton entry in the global ability registry if the module is expected to have a public ability (with maturity: candidate initially, to be fleshed out by the developer/AI later).
  6. After generation, the scaffold can run a quick consistency check: e.g., run make dev_check in a mode that verifies the new module's docs are linked (the new module's ROUTING is included in parent routing, etc.), naming is correct, and even a placeholder test can be executed (like an empty test passes). Only if these pass, the module scaffolding is considered complete.
  7. Once confirmed, the scaffold can prompt removal of the init/ folder and mark the module ready.
- **Defining module initialization completion criteria**: We explicitly state that a module is considered initialized when it has all the structural and documentation elements in place (registered in indexes, routing reachable, policies in place, placeholders for any capabilities or code as needed), but it does not need to provide actual business functionality yet. In other words, after scaffolding, the module will be empty of real logic (aside from maybe some template code or example test), which is acceptable. The goal is to ensure the skeleton passes all checks and is ready to be filled in. No incomplete stubs should break CI. This approach encourages small initial PRs that just add the structure, then subsequent development adds real features.
- **Clarifying module concepts: level, type, instance**:
  - **Module Instance**: A concrete module in the modules/ directory (like mod_order_core for order core services). This is the actual unit of development and deployment. It is the only first-class citizen in terms of execution: all abilities and code run in the context of a module instance. Every ability or low-tier method is owned by a module instance (via owner_module field), and all documentation (workdocs, AGENTS, etc.) is maintained per instance.
  - **Module Type**: An abstract categorization of modules. Types are used to enforce consistency and known patterns (for example, "Controller" type modules vs "Service" type modules, or types corresponding to phases like suggested naming 1_Assign, 2_Select etc. in the initial idea). We will maintain a module type graph (in doc_agent/modules/type_graph.yaml) which defines how module types relate (some may depend on others, or form a layered architecture blueprint). This graph acts as a design blueprint but not something the runtime orchestrator uses directly. Each module instance declares its module_type in its manifest, and possibly which other modules it requires (dependencies). We will enforce via CI that if module A requires module B, such a relationship is allowed by the type graph (e.g., a "UI" type can depend on a "Service" type, but not vice versa, if defined so in the type graph).
  - **Module Level**: A concept to denote a hierarchical containment (e.g. a product might contain sub-modules for different bounded contexts). Module instances can be organized in a tree of parent-child (for example, a top-level module for a feature may have sub-modules for subcomponents). Each module manifest can include a module_level or a pointer to a parent_module if it's a sub-module. This is mostly for human understanding of system structure and could influence documentation grouping. It is decoupled from module type (a parent and child module could be of different or same types). The level hierarchy might also mirror the business decomposition (like an Epic containing Features, containing Components as modules).
  - We will document these concept definitions in both AI and human docs for clarity. A concise AI-facing guide (e.g. doc_agent/guides/module.md) will explain the three concepts (instance, type, level) and their roles. A more detailed human-facing doc (e.g. doc_human/guide/modules.md) will elaborate on how to decide module splits, how types are structured, etc., with structured formatting.
- **Managing module relationships in orchestration**: It's important that orchestrated abilities respect module boundaries:
  - Abilities (high-tier functions) will be associated with a single owner module, and orchestrator tasks should ideally not span multiple top-level modules in one go for clarity/responsibility. Cross-module calls are permitted but only as dependencies, not as separate parallel tasks in one context. For example, an ability func.order.create owned by mod_order_core might internally call an ability of mod_inventory (if needed) via depends_on, but from the orchestrator's perspective, it's still executing a single high-level ability in mod_order_core.
  - For the AI developer perspective, they can see all abilities in the registry (including those of other modules marked internal), so they can consider reusing an internal "wheel" instead of reinventing logic if appropriate. But the orchestrator (in production planning) will not directly invoke internal ones, maintaining one module per orchestrated goal for clarity.
  - We set a rule that each ability registry entry must include the owner_module, and any high-tier ability with exposure: public is considered a business entry (like an API or a workflow that can be triggered externally or by orchestrator), whereas exposure: internal means it's a helper not to be directly scheduled.
- **Planning for module removal and large refactors**: We add guardrail considerations for module-level changes. For instance, deletion of a module should be a controlled operation:
  - All of a module's abilities and docs should be removed or reassigned first (cannot have orphan capabilities lingering).
  - The impact on other modules' dependencies must be checked; if uncertain, require manual approval to proceed.
  - We enforce (via triggers in Phase 8) that an AI cannot in one operation refactor or delete across multiple modules simultaneously – e.g., triggers can block a PR that touches too many module directories at once, unless a special override is given. This ensures changes remain localized and reviewable.
  - These rules will be baked into guardrail policies or triggers (like a trigger that flags "modular boundaries" if an AI attempt spans modules).
  - We document this in the global AGENTS or relevant guide as well (so the AI is aware that it should focus on one module at a time for significant changes).

**Rationale**: The modular architecture provides scalability and maintainability for the template. It allows different teams or AI agents to work on different parts of a project without stepping on each other, and helps the orchestrator manage context by segmenting tasks. By scaffolding modules with all necessary docs and configurations, we ensure that every module is AI-ready from its inception. The discussion highlighted eliminating the outdated concept of "eight documents" to generate and instead explicitly listing what needs to be created for a module – our scaffold approach addresses this by generating exactly those needed files, avoiding confusion. The module type system encourages consistency across modules of similar nature and aids the AI in understanding the project structure ("these modules follow a pattern, connect in defined ways"), which can improve its planning (for example, knowing all modules of type "controller" should call one of type "service"). The type graph as SSOT (single source of truth) ensures we have one maintained model of module relationships, which we can use to validate manifest declarations and even to auto-generate a system architecture diagram if needed. By not maintaining a separate manual diagram for humans (which could get out-of-sync), we reduce documentation drift. Guardrails around module changes address a key risk: an AI refactoring too broadly could cause chaos; imposing one-module-at-a-time rules mitigates that. All these measures aim to keep the system modular, understandable, and under control as it evolves.

**Expected Outcomes**: By the end of Phase 5:

- The repository will include at least one example module (perhaps a "base" or template module) fully scaffolded, demonstrating the structure with MANIFEST.yaml, AGENTS.md, ABILITY.md, etc., in place. Additional modules that are central (like the api_gateway module for API, or a db module if structured that way) will also be scaffolded now or in their respective phases.
- The module scaffolding script will be implemented and tested by creating a dummy module. It should produce a module where immediately after creation, running make dev_check passes (meaning docs are linked, any placeholder tests pass, etc.).
- The instance registry (instance_registry.yaml) will contain entries for the modules (with type, etc.), and the type graph (type_graph.yaml) will be created to describe allowed module type relationships. The CI should include a check that every module instance's module_type is defined in the type graph, and that its declared requires_modules (dependencies) are permissible edges in that graph.
- Documentation: A doc_agent/guide/module.md (AI-facing) will outline module-level, type, instance definitions in ~100 lines or less. A corresponding doc_human/guide/module.md will provide a richer explanation for developers, possibly including examples or scenarios of module interactions.
- We will have updated the global ROUTING.md to include a section enumerating available modules (at least listing them as part of the navigation). For instance, the root README/ROUTING could have a summary table of modules and links to their quickstarts or main docs to help the AI find module entry points easily.
- The guardrail/triggers configuration (though fully tackled in Phase 8) will have placeholders or initial rules to ensure a module addition runs the required sync scripts (for updating the graphs and registries) and that deletion or multi-module changes are flagged. We will verify that trying to remove a module without following procedure triggers an alert.
- With modular foundations in place, we can proceed to domain-specific integrations (DB, API) in subsequent phases, confident that they will plug into this module system properly (e.g., the api_gateway is a module of a certain type, the DB migrations might belong to a db module or similar). Essentially, by this phase, the infrastructure for scaling the repository to multiple modules is ready, and initial modules are in place as needed by the next features.

---

## Phase 6: Database Integration and Data Management Automation

Objective: Integrate database support into the template, including structured schema management, migration workflow, and AI agents for CRUD operations and cache (Redis) management. This phase ensures the template can handle typical database lifecycle tasks in a controlled, documented manner.

Scope: Phase 6 focuses on PostgreSQL (primary database) and Redis (cache) integration as per initial requirements, implementing both the necessary code infrastructure and the AI orchestration hooks:
- **Setting up the database directory structure and assets**:  We will create a db/ directory (or possibly under modules/db/ if treating DB as a module; however, since DB touches multiple modules, it might be a shared resource directory). As per initial design:
- **For PostgreSQL**:  db/engines/postgres/ will contain:
- **A schemas/ folder with YAML files describing each table (tables/<tablename>.yaml). These YAML files define the schema**:  fields with name, type, nullability, default, descriptions, and tags (e.g. marking if a field is an index, foreign key, etc.). This structured schema serves as the SSOT for the database structure.
- **A migrations/ folder containing SQL migration scripts. Migrations are organized in timestamped pairs**:  for each change, an <timestamp>_up.sql and corresponding <timestamp>_down.sql for rollback. We’ll enforce that every up migration has a down script and discourage destructive operations like raw DROP TABLE – instead preferring to mark things as deprecated and clean up in a later migration.
- Optionally, a docs/ subfolder for DB-specific documentation (like DB_SPEC.md or YAML) which could be an AI-facing spec summing up the database design (the initial acceptance criteria mentions a DB_SPEC.yaml reflecting the schemas).
- **For Redis**:  possibly a similar structure under db/engines/redis/ for any TTL policies or caching schema documentation. Redis often doesn’t have a schema like SQL, but we can document expected cache keys, data patterns, and expiration rules in a spec document.
- We will include a high-level README.md or ROUTING.md in db/ or db/engines/postgres/ explaining how to navigate the DB files (for humans, since AI will use routing).
- **Implementing AI Agents for data operations**: 
- A Data CRUD Agent for structured data (Postgres) will be included in the ability registry. This could be a high-tier ability (kind agent or workflow) responsible for handling typical DB tasks – perhaps generating SQL queries, performing migrations, etc., under controlled conditions. For example, able.db.schema_manager or able.db.crud_operator as an agent node that knows how to apply migrations or do schema reads.
- A Redis Lifecycle Agent similarly to manage caching patterns, ensure keys expiration, etc. It might wrap around certain operations like refreshing caches after data changes, etc. The template will include such an agent stub.
- **During this phase, when we introduce these agents, we will**: 
- Add their low-tier capabilities to the method_registry (e.g. scripts to apply migrations, to run schema lint, etc.) and their high-tier definitions to ability_registry (with appropriate owner_module or perhaps they belong to a db module or some ops module).
- **Provide local documentation**:  e.g. db/engines/postgres/AGENTS.md that outlines policies for DB operations (like requiring code review for migrations on production, etc.) and db/engines/postgres/ABILITY.md listing DB-related abilities if not captured globally.
- Ensure these agents are registered in the orchestrator’s registry as well, with appropriate graph_node_id and tools_allowed fields (tools meaning scripts they can call, e.g., migration script). This will let the orchestrator know how to call them.
- **Automation of DB workflow**: 
- **Linting and Verification**:  We will provide scripts such as db_lint.py to verify YAML schema definitions (check for consistency, required fields, naming conventions) and possibly to compare the YAML schema with the actual database (in case of an existing DB) to catch drifts. Also migrate_check.py could simulate applying migrations in a test environment and verify they match the schema and can rollback cleanly.
- **Migrations process**:  The repository will incorporate a standard migration process. Possibly a Make command like make migrate to apply new migrations to a local dev database, make migrate_check to run validations (like no missing down scripts, etc.), and make rollback_check to test down migrations. We define in docs that destructive changes require special handling (mark as deprecated first). The AI agent when asked to modify the DB will:
  1. Update or create a YAML schema file (tables/xyz.yaml).
  2. Generate a new pair of migration scripts for the change.
  3. Possibly update a high-level spec or changelog.
  4. Run make db_lint and make migrate_check as validations.
  5. Ensure all this is recorded in workdocs and gets approval if needed.
- **Routing and documentation**:  In the global ROUTING.md, we’ll add an entry under a scope like “Execution – Database” or “Data Management” with relevant topics (like “Designing or Changing Schema”, “Performing Migrations”) that point the AI to:
- The database spec document or schema guide (as a quick reference of how to design schemas).
- The DB operations policy (AGENTS.md) for safety rules (like needing approval for prod changes, etc.).
- The Data/Schema agent’s documentation if separate.
- Tools or script references (like pointing to make migrate_check command documentation).
- Ensuring that the AI has quick access to these DB-related docs via routing means when it gets a task involving DB, it will load these guidelines and not do something out of process.
- **Triggers and Guardrails for DB tasks**:  Database changes are high-risk, so we’ll enforce strong triggers:
- **For example, in agent-triggers.yaml, we will have an entry for when a new file appears in db/engines/**/migrations/** (or a schema file is modified) that triggers a high-priority action. This trigger (id**:  db-migration) might have enforcement: block if it’s production or critical, requiring a guardrail approval. It would preload_docs like the DB policy and spec (so the AI knows the rules), and list required_checks such as db_lint and db_migration_dry_run (ensuring the migration can run on a dry-run basis).
- Guardrail policies will specify that any attempt to apply migrations to a production database requires explicit human approval (the AI should route through an approval step, logging it in handoff-history.md).
- We reaffirm from discussions that all DB-related operations must use the registered abilities/methods – the AI should never just run SQL ad-hoc; it should always go through our defined scripts or procedures. Config settings might determine whether those scripts run against a local dev DB or a cloud instance, etc., but that’s abstracted from the AI. We will ensure the tools (scripts) themselves handle environment targeting, and triggers ensure the correct environment is used (e.g., perhaps a trigger to prevent running migration scripts on a production DSN without approval).
- **Additional safety**:  possibly a trigger to detect any direct database connection usage in code and warn or block if not through the allowed methods.
- **Documentation & spec sync**:  We will ensure there’s a process to keep YAML schema files and any exported specs (like an ERD or docs) in sync. For instance, if we have a DB_SPEC.yaml as a consolidated view (maybe generated by combining all table YAMLs or containing cross-table constraints), we should update it whenever schemas change. The AI can be tasked with this as part of the migration process (e.g., after writing a new table YAML, update a section in DB_SPEC.yaml). A check in CI will compare the module’s db/engines/postgres/docs/DB_SPEC.yaml with a master copy in doc_agent/specs/DB_SPEC.yaml to ensure they’re consistent.
- **Redis specifics**:  For caching, we’ll include in the ability registry something like a cache_invalidator method or ensure that when data changes, the orchestration has a step to update caches. There may be triggers for keys with unusually long TTL or high memory usage (but that might be beyond initial scope, likely we focus on making sure the AI can perform basic set/get and know to expire keys or wait for TTL).
- **The closing the loop on DB changes**:  All DB operations should result in:
- Updated schema documentation (so docs are never outdated).
- Updated migration scripts (version-controlled).
- Logs in workdocs (like “Migration X applied/tested”).
- Potentially an update to a changelog for DB (maybe a line in CHANGELOG.md or in a workdocs/ledger).
- This ensures the DB SSOT and code do not diverge.

**Rationale**: Database management is singled out as a crucial domain requiring careful integration with AI workflows, due to the structured nature of data and the high risk of errors. By representing schema as data (YAML files), we allow the AI to reason about and manipulate the DB structure in a controlled way rather than editing raw SQL, and we make it easier to run validations (since we can lint YAML and auto-generate SQL). The requirement to always pair up migrations and avoid direct destructive changes enforces safe migration patterns. Introducing dedicated AI agents for DB ensures that the AI actions regarding data follow a guided approach (the agent can be coded to do things like “first update schema yaml, then create migration, then run checks, etc.”). The triggers on migration file changes act as a safety net, catching any such attempt and ensuring the AI goes through the proper procedure with approvals. This aligns with the emphasis in discussions that DB operations must be tightly controlled with guardrails and triggers due to their impact and the need for consistency (SSOT). Additionally, having a formal process means human DBAs or reviewers can audit changes easily – everything’s in YAML and SQL, not hidden in ad-hoc scripts. The orchestrator’s involvement here ensures any data-related ability is executed with pre-checks (like context usage tracking could measure if DB queries are frequently failing to optimize later). Overall, Phase 6 embeds database best practices into the template, making the AI a capable but safe database engineer in the context of this repository.

**Expected Outcomes**: By the end of Phase 6:
- The repository will contain the db/engines/postgres/ directory with at least one example table schema YAML and a pair of example migration scripts, serving as a template for future migrations. We will test the scenario of adding a new table (as described in the initial example of adding an orders table) through the AI agent to ensure the workflow (schema file + migration + tests) works.
- A db/engines/postgres/AGENTS.md file (or an integrated section in a global DataOps policy doc) will outline the rules specific to database changes (like requiring approvals for certain triggers, who to notify, etc.). It will have front matter marking it as agent_policy and audience ai.
- **The Data CRUD agent and Redis agent abilities will be present in ability_registry.yaml with tier**:  high, and corresponding low-tier entries (scripts) added, such as:
- Script for applying migrations (possibly invoked via a make command or a Python script).
- Script for checking migration consistency (lint).
- Perhaps a script for generating an ERD or schema diagram (optional).
- Script for interacting with Redis (like flushing cache for a key, or monitoring keys).
These will be documented in ABILITY.md or in capability docs, and their presence verified via our registry checks.
- The routing entries for database operations will be live, so if the AI is asked about a DB task, going through the root ROUTING.md it will find the relevant DB entries and thus load the DB spec and policy docs.
- **All CI checks around the DB integration will be operational**:  Running make db_lint on the example schema yields no errors (proving our schema format is valid), and make migrate_check confirms up/down are in sync and do no forbidden ops. The triggers for migrations will be set to observe mode or block mode as appropriate and tested in dry-run (Phase 8 will cover testing triggers, but we might simulate one here).
- **Importantly, no modifications to the database schema can occur without going through this documented pipeline. This phase’s success is measured by demonstrating an end-to-end example (for instance, the AI adding a new column to a table**:  it updates YAML, writes migration, passes checks, and requires a human sign-off through the guardrail agent) and everything being logged properly. After Phase 6, the repository template is ready to handle evolving data models reliably and safely.

---

## Phase 7: API Lifecycle Integration and Management

Objective: Integrate API development and management capabilities into the template, covering the full API lifecycle from contract definition to implementation, testing, and gateway orchestration. This ensures the template supports rapid setup of API endpoints with strong consistency checks and documentation.

Scope: Phase 7 focuses on RESTful API (or similar) support, including contract-first development and gateway module setup:
- **Initializing an API Gateway module**:  We will create a module (e.g. modules/api_gateway) that acts as the entry point for external API calls. This module will be scaffolded (using our Phase 5 process) with:
- AGENTS.md – containing policies about routing, authentication, rate limiting, etc., at the gateway level.
- doc/ROUTING.md – the routing for API topics (e.g. differentiating scopes of different API sets).
- doc/CONTRACT.md – this file describes the API contract for the gateway module itself (like global endpoints or how internal services are exposed).
- doc/MIDDLEWARE.md – detailing any middleware or cross-cutting concerns (logging, auth) applied at gateway.
These provide a template for how modules that expose APIs should document themselves.
- **Establishing API contract files for each module**:  Any module that provides API endpoints will have a doc/CONTRACT.md file inside its doc/ directory. This contract file:
- **Uses Markdown with a front matter indicating audience**:  ai, doc_role: contract, route_role: leaf, etc..
- Lists the module’s endpoints (with method, path, auth requirements), and defines request/response schemas likely via JSON Schema or similar (these can be included directly or via references to schema files).
- This contract serves as a single source of truth for the API interface of that module, which will be used for client generation and compatibility checks.
- For version control, we might have a hidden directory (like .contracts_baseline/) to store snapshots of contract files for compatibility checks between versions.
- **Automation for contract compliance and generation**: 
- Provide a script or tool to perform contract compatibility checks (e.g., scripts/contract_compat_check.py). This will compare the current CONTRACT.md of a module with a baseline (previous version or a known good version in .contracts_baseline/) to ensure no breaking changes are introduced without proper versioning or alerts. This can catch things like removed endpoints or changed schemas that would break clients.
- **Provide an OpenAPI generation step**:  a script scripts/generate_openapi.py that reads all CONTRACT.md files and produces an aggregate openapi.json (or YAML) definition for the entire API. This ensures documentation and client stubs can be derived automatically. We will include an tools/openapi.json file (or similar) that gets updated by this script.
- Provide a client type synchronization tool (maybe scripts/type_contract_check.py as mentioned) that ensures data models in code match the contract (or updates client libraries). For instance, if we generate TypeScript types or Python dataclasses from the OpenAPI spec, this script verifies they are up to date.
- These scripts will be integrated into CI (so that any PR touching contract files must run make contract_compat_check etc.) and possibly into triggers (see below).
- **Registering API abilities in orchestrator**: 
- The API Gateway agent (module) will be added to the orchestrator’s ability graph. For example, an ability id like able.api_gateway.v1 of kind agent that represents the API orchestration logic (like it routes calls to appropriate module abilities). This agent might list in tools_allowed the lower-level utilities it uses (like the contract validation script, openapi generator).
- Each module that has API endpoints may get a high-tier ability representing its service (e.g. able.order_service.api of kind workflow that the gateway calls). But often, API calls might map one-to-one to abilities that already exist in modules as public abilities.
- We ensure that for every CONTRACT.md, the described endpoints correspond to either specific module abilities or are handled by the gateway routing to module methods. This mapping could be documented within the contract (like linking an endpoint to a capability id for the handler).
- **Documentation and routing**: 
- **In the root ROUTING.md, create a scope for API execution. For instance**:  Scope: “Execution – API”, Topic: “Endpoint Lifecycle”, When: “designing or changing interface contracts” with target docs being:
- The API Gateway ROUTING.md and AGENTS.md (so the AI can navigate into the API gateway module docs).
- Possibly the tools/openapi.json (the specification output) wrapped via a small doc describing how to use it (so the AI can load the OpenAPI if needed to reason about the entire API).
- The relevant module’s CONTRACT.md if a specific module’s API is in question.
- Essentially, we route the AI to either the gateway docs or directly to a module’s contract doc depending on context.
- Each module’s ROUTING.md (if complex enough) should include its CONTRACT.md as a leaf node reference as well.
- **Security and approval flows for API changes**: 
- **API changes can be as sensitive as DB changes (breaking clients, etc.), so triggers will monitor them. For example, an agent-triggers.yaml entry for changes in any modules/*/doc/CONTRACT.md or the generated openapi.json would be flagged as critical or block. The trigger could be id**:  contract-change with enforcement: block that, upon detecting an API contract edit, preloads a “Guardrail guide for API changes” and requires certain checks (like run the contract compatibility script).
- The guardrail agent for this trigger would ensure that the AI ran compatibility checks and that perhaps a human approved major changes (especially if breaking).
- Also, if an OpenAPI spec is updated, possibly automatically raise a review to ensure documentation is published, etc.
- **Execution guidelines**: 
- **Provide a step-by-step SOP (Standard Operating Procedure) for making API changes. We might include a guide or checklist (like a table summarizing phases**:  Requirements & Design, Implementation, Testing, etc., with must-do actions at each stage). In the initial content there’s a section 7.8 with a table for API change execution – we can incorporate that as a guide that the AI/humans can follow:
  1. Design: Update CONTRACT.md with new endpoint and schema, run make contract_compat_check to verify no unintended breakage.
  2. Planning: Document the change in a workdoc plan, including any cross-module impacts.
  3. Implementation: Write the handler code in the module’s backend, ensuring it adheres to the contract (test with sample payloads).
  4. Testing: Write or update integration tests for the endpoint, run scripts/generate_openapi.py and use it to generate client and run tests against it.
  5. Documentation: Confirm openapi.json is updated and commit it, and ensure any examples or guides reflect the new endpoint.
  6. Approval: If trigger requires, get approval for deploying the change (especially if it’s a breaking change).
  7. The table in documentation helps ensure no step is missed (the AI can be guided by it).
- **The Acceptance criteria for API integration**: 
- All CONTRACT.md files have complete front matter and are registered in routing.
- The tools/openapi.json (or similar) should always reflect the latest contracts (we might enforce this by regenerating it in CI and failing if there’s a diff).
- The CI runs for make contract_compat_check, make generate_openapi, make type_contract_check should pass with no errors for the baseline (initial contracts).
- We might also want to ensure that regenerating client types is idempotent (script can be rerun anytime and results should not change if no contract change).
- Document that clients (if any included, e.g., TypeScript types) are updated accordingly.

**Rationale**: API integration is critical for enabling the repository to serve as a starting point for actual service development. By enforcing a contract-first approach (with CONTRACT.md files and automated checks), we prevent the common pitfalls of API drift (where implementation and documentation diverge). The contract becomes a shared truth for AI and humans: the AI will consult it to know what endpoints exist and what data they expect, and humans can review it to ensure it meets requirements. The OpenAPI generation automates documentation and client updates, reducing manual effort and errors. The triggers and approvals around contracts reflect the high impact of API changes – they ensure that any change gets scrutinized (which is important if an AI is making changes, to avoid accidentally breaking external integrations). By integrating these in the template, even a brand new project gets robust API governance from day one. The API Gateway module encapsulates cross-cutting concerns (auth, rate limiting) which decouples those from business modules – also it provides a single place to enforce global API policies (like if we needed to shut off an endpoint, the gateway can do it by policy). The modifications confirm that no major structural changes were needed beyond reinforcing the importance of contract management and guardrails for interface changes, which we have incorporated with triggers and a focus on approval.

**Expected Outcomes**: By the end of Phase 7:
- The modules/api_gateway/ module is fully set up, with its docs and policies, and it appears in the global routing under an API scope. The orchestrator registry includes an entry for api_gateway agent, and possibly some initial tools (like a basic health-check endpoint ability).
- At least one other module (say mod_example_service) has a doc/CONTRACT.md illustrating how a module defines its API contract. We will test the flow by adding an example endpoint there and verifying the OpenAPI generation picks it up and that the contract check passes (i.e., baseline updated).
- Running the OpenAPI generator produces a valid specification file that matches the contracts. This spec can be used to generate a simple client (we may test generating one to ensure the pipeline works, though actual client code can be considered out of scope, it’s more about verifying consistency).
- **The triggers for contract changes are active**:  an intentional edit to a CONTRACT.md without updating the baseline or running the check should cause a CI failure or at least a warning. The guardrail flow requiring approval for major changes is in place (maybe simulated by marking a contract change as breaking and seeing that a priority: critical trigger fires, instructing the AI to get approval).
- Documentation wise, an API Guide might be added (for AI, maybe not needed if the routing and policies cover it, but for humans a short guide in doc_human/guide/api.md could summarize how to add a new endpoint following these steps).
- With Phase 7 complete, our template can support adding new API endpoints through an AI-driven or semi-automated process with high confidence that nothing will be missed (docs updated, tests run, etc.), closing the loop from design to documentation.

---

## Phase 8: Automation Pipeline and Trigger System

Objective: Implement the automated code development pipeline (retrieve → plan → code → write → update context) in the repository, primarily through a configurable trigger system. This ensures that certain events or conditions automatically guide the AI’s behavior, enforce guardrails, and maintain context, making development more autonomous and safe.

Scope: In Phase 8, we focus on the automation hooks that connect the pieces built in earlier phases into a cohesive development loop:
- **Designing the Trigger Mechanism**:  We formalize triggers as YAML-defined rules in (likely) doc_agent/triggers/agent-triggers.yaml (or under doc_agent/orchestration/). Each trigger has:
- An id (we adopt a naming convention like <domain>.<scope>.<action> to convey its intent clearly).
- A priority level (e.g., P0, P1, … where P0 is highest). This priority will determine execution order if multiple triggers fire simultaneously.
- **An enforcement mode**:  e.g. observe (just log), warn (warn AI but allow proceed), or block (halt until approval).
- A match condition that can be based on file system events (like file path patterns, content regex) or on AI output (like a detected plan to do something risky). For now, most triggers will be file-centric (like detecting modifications in certain directories, presence of certain keywords in commit messages, etc.).
- **preload_docs**:  a list of documents (with optional anchors) to automatically load into context when the trigger fires. For example, if a trigger detects a DB migration file addition, it preloads the DB migration policy section for the AI.
- **required_checks or actions**:  a list of commands or script invocations that must be run (and pass) when this trigger is activated. E.g., for a migration trigger, required_checks: [db_lint, db_migration_dry_run] to ensure those validations run.
- Possibly a target_agent or target_function reference indicating which high-level ability this trigger is guarding or associated with (though we also have the safety_profile in abilities linking back). We might use this to allow the orchestrator to identify which workflow to start. For instance, if a trigger is about “on push to main branch”, target might be some CI ability pipeline.
- **Grouping or mutual exclusion**:  maybe a concept of trigger groups or mutex to avoid redundant firings, but this might be advanced. We can document the idea of a “mutex group” or that triggers can override each other if needed (but initial approach is simpler).
- **Implementing trigger evaluation logic**:  In practice, triggers will be evaluated either:
- Proactively by the orchestrator during AI planning (e.g., if the AI proposes an action, we simulate what triggers would fire) and by a file-watching mechanism in the development environment (for example, a git pre-commit hook or a CI step that runs make trigger_check to see if any triggers are relevant to the changes).
- We will provide a script trigger_check.py or incorporate into make trigger_check that goes through each trigger rule and checks if its condition is met by the current change set or context, outputting any hits.
- **The orchestration logic (phase 10) will detail how triggers integrate with the agent’s decision loop (basically**:  before executing a step, check triggers; if any block triggers fire, pause and escalate).
- **Handling concurrent triggers and priorities**:  The discussion explicitly mentioned addressing scenarios with multiple triggers:
- If multiple triggers match, we will sort by priority and handle the highest priority first. Lower priority triggers might be queued or noted for later.
- We also ensure that even if a lower priority trigger doesn’t get to execute action, its occurrence is still logged somewhere (like in the workdoc context) for awareness.
- This is implemented by adding a priority field to each trigger and by coding the evaluation to sort triggers by that.
- **Example**:  Trigger A (P0, block) and Trigger B (P1, warn) both fire. The system will address A first (block and require resolution). B’s condition is noted (perhaps a warning written to context), but B’s action might be skipped if it’s something that would have happened simultaneously. Alternatively, B could still execute if it doesn’t conflict. We document this behavior in the trigger guardrail design.
- Also mention if triggers belong to a mutual exclusion group (e.g., one might suppress another if they are contradictory, but we likely won’t implement that initially; just noting we considered it).
- **Trigger examples to implement**: 
- **High-risk file change triggers**:  e.g., modifying modules/*/AGENTS.md might trigger reminding the AI to update doc_node_map.yaml and to not duplicate global rules.
- **Approval triggers**:  e.g., any change to tools/openapi.json or CONTRACT.md triggers a guardrail as discussed in Phase 7.
- **Ops triggers**:  e.g., detection of a new script in scripts/ directory triggers a process to register it (or at least warn if not registered) – we could have a trigger that if a .py file is added in scripts without an entry in method_registry.yaml, it flags that.
- **Multi-module change trigger**:  if a commit touches files in more than one modules/<name>/ directory (except maybe trivial docs), trigger a warning to ensure the AI isn’t doing a cross-module refactor in one go (ties to Phase 5 guardrail on focusing one module).
- **Workdoc triggers**:  e.g., if a workdoc gets beyond a certain size or age, a trigger might note it for archiving (though actual archival is via CI not immediate, as per modify, but a trigger could schedule it).
- **Quality triggers**:  e.g., if test fails or lint fails, triggers might catch that and route the AI to fix it (though test failures are more event outcomes than static triggers).
- Many triggers are about file events (which naturally fit into pre-commit or CI checks). For AI operation in a live loop, triggers could also monitor for certain words in AI plans or outputs and react (like if the AI says “I will disable all guardrails”, a content trigger might intervene).
- **Automation of the “chain” (retrieve→plan→code→write→context)**:  We will document and partially implement how an AI task flows:
  1. Retrieve: The orchestrator uses the routing system (Phase 2) to get relevant docs.
  2. Plan: The AI formulates a plan. At this point, triggers can apply to the plan content. (We might have pattern triggers on certain plan keywords like “delete module” which could prompt caution).
  3. Code: The AI writes code or docs. As it produces artifacts, triggers on file changes can fire (for example, it creates a migration file, so the DB trigger fires and loads policy, requiring tests).
  4. Write/Execute: The changes are applied or tested. Automation here means running required checks automatically when triggers fire (we could integrate with a makefile so that when a file is saved, associated checks run).
  5. Update Context: The workdocs get updated with what happened. Some triggers explicitly require adding entries to the context or workdoc. For example, a guardrail trigger might require the AI to record in context.md#Triggers which triggers fired and what was done.
- **We consolidate these steps into an automation guide**:  e.g., an internal automation-links-guide.md which describes how triggers, routing, and guardrails interact through the chain. This is primarily for AI to reference a one-stop explanation of the chain if needed.
- **Periodic maintenance via triggers**:  Some triggers or automated tasks run not on immediate events but on a schedule (like CI nightly jobs):
- E.g., a scheduled trigger to run guardrail_runner.py --dry-run all weekly to test all enforcement flows.
- A scheduled job to run context cleanup (we added dynamic context management in Phase 10, but scheduling is done likely in CI cron).
- We will plan adding these as notes in documentation and ensure scripts exist to support them.
- **CI integration**:  Many triggers correspond to CI checks. We’ll incorporate running make trigger_check in CI, which simulates all triggers in a dry-run to ensure none of the new changes triggers an unresolved block. If any block triggers appear with no resolution or approval, CI should fail the build, forcing attention.
- **AI PR Review integration**:  The modify discussion floated the idea of using AI for PR review in the automation chain. This is an advanced enhancement. We note it as a consideration: possibly having a trigger for when a PR is opened that calls an AI review agent to check code and docs. This might not be implemented now, but we’ll document the idea and perhaps leave hooks for it (like an entry in routing or an optional ability in the registry). This could be a future Phase or an optional pipeline for later.

**Rationale**: The automation pipeline with triggers brings together all our earlier work to truly enable AI-driven development with guardrails. Triggers act as the proactive sentinels, catching conditions that might slip past passive policy docs. By automatically loading the right instructions and running checks, they drastically increase the AI’s efficiency and safety: the AI doesn’t have to search for what tests to run – the trigger demands it via required_checks; the AI doesn’t accidentally commit a migration without approval – the trigger blocks it and forces the guardrail process. Prioritization of triggers addresses an important edge case: multiple things going wrong at once. We ensure the AI addresses the most critical issue first (e.g., security issues over minor warnings). Logging all trigger hits (even those not immediately acted on) in the context helps maintain situational awareness – for both the AI in subsequent steps and for human auditors. In effect, triggers implement a form of reflexive feedback loop for the AI: as soon as it does something, the environment reacts (through triggers) to guide the next move. The careful design of the trigger definitions and associated documentation ensures this reflex is transparent and consistent.

**Expected Outcomes**: By the end of Phase 8:
- The agent-triggers.yaml (or equivalent) will be populated with a set of well-defined triggers covering known high-risk or critical workflow junctures (as exemplified above). Each will have clear actions and documentation references.
- **We will have a new automation guide doc (perhaps doc_agent/flows/automation-links-guide.md) that describes how the automation chain works, what triggers are, and how to interpret them. This doc (audience**:  ai, doc_role: guide) will serve as a reference for the AI if it needs to recall or explain the automation process.
- A maintenance plan (in ai/workdocs/active/maintenance/plan.md or similar) will list routine automated tasks (like weekly guardrail dry-runs, periodic context cleanup, etc.), and we’ll have set up CI or scripts to perform those.
- **To test success**:  simulate a scenario with multiple triggers. For example, create a dummy situation where adding a migration (DB trigger) and editing a contract (API trigger) occur in one batch. Ensure the system processes the P0 trigger first (say DB is P0, API is P1), blocking appropriately and logging both triggers. Then verify after addressing P0 (approving migration), it then handles P1. This can be done via a dry-run mode in guardrail scripts.
- The make trigger_check will be included in make dev_check or similar, so that any developer or AI commit will run through triggers and fail if any unaddressed block triggers are present. We might mark certain triggers as allowed to be pending (like if they require an actual human approval, CI could detect the approval artifact).
- All of the above ensures that moving forward into actual development (Phase 10 interactive dev, and Phase 11 testing) the groundwork is laid for a highly automated and safe development cycle.

---

## Phase 9: Script Management and Tooling Conventions

Objective: Define the organization, registration, and quality standards for scripts and tools in the repository, ensuring they are easily discoverable by the AI and consistently integrated into the capability index and automation pipeline.

Scope: In Phase 9, we concentrate on the scripts/ directory and related processes:
- **Structuring the scripts directory**:  We will enforce a clear layout for all utility scripts. For example:
- **A root scripts/operations-guide.md which is the AI-facing entry point to the scripts domain (with audience**:  ai, doc_role: guide). This guide will describe how scripts are organized, naming conventions, and how to use/extend them. It also serves as the canonical router – triggers or AI queries about scripting will point here.
- A root scripts/README.md for human developers that outlines the scripts structure and links to operations-guide.md for details.
- Subdirectories under scripts/ grouped by purpose (e.g. scripts/db/ for database related scripts, scripts/ci/ for CI automation, scripts/scaffold-module/ for scaffolding, etc.). Each subdirectory can have its own README or at least an entry in the operations-guide pointing to it.
- **Enforcing script naming and header conventions**:  All non-temporary scripts (scripts that are part of the repo’s capabilities, not just one-off) must:
- Follow a naming convention (lowercase, words separated by underscores or hyphens, depending on language standard).
- Include a standardized header comment or YAML front-matter describing their purpose, usage, parameters, and any risk/dry-run mode available. For example, a Python script might start with a block comment listing usage, or a shell script might echo usage if run with -h. We might formalize this in a small YAML preamble in comments.
- **These header details should be synchronized with documentation**:  The scripts/operations-guide.md will have a section that lists each script and key info (e.g. what it does, what ability id it’s registered under). When we update the standard for script headers, we update the guide first and then propagate to all scripts, to keep consistency.
- Possibly define a template for script files to include (like a commented template new scripts can use as a starting point).
- **Capability registration for scripts**:  Any operational script that is meant to be invoked by the AI or automated processes must be registered in capabilities.yaml (method registry). We will integrate this into the development workflow:
- **If a new script is added under scripts/, the developer/AI should run a scaffold to register it. We might create a small CLI or checklist**:  the scripts/operations-guide.md will include a “How to add a new script” section explaining this. It will instruct to:
  1. First search if an existing script covers the need (to encourage reuse). Possibly provide commands using rg (ripgrep) or referencing CAPABILITIES.md to find similar capabilities.
  2. If not found, proceed to implement the new script but then update the capability registry. Add a new entry in method_registry.yaml or capabilities.yaml for it. Also update the operations-guide “Capability Registration” section to include the steps and any naming conventions for capability IDs related to scripts.
  3. Also, document the new script in operations-guide.md under the relevant category (so AI has a one-glance reference).
  4. If the script should be triggered by some event, ensure to add/update a trigger in trigger_map.yaml or the triggers doc listing condition->script.
- There will be no unregistered script that is meant for AI usage; any script not registered will be considered a temp or internal only accessible by humans.
- **Trigger mapping for scripts**:  Maintain a mapping of triggers to scripts (and vice versa) possibly in a trigger_map.yaml or in an appendix of the operations-guide.md. This helps developers see, for example, “script X is invoked when trigger Y fires” and “trigger Y monitors file pattern Z and calls script X.” Keeping this up to date ensures transparency of automation. We might embed this matrix in the documentation to avoid separate source of truth (maybe auto-generate from triggers config).
- **Script reuse and versioning policy**:  We note strategies:
- Before writing a new script, search for existing ones (the guide stresses this).
- We discourage duplicating functionality; if needed, refactor an existing script to cover new use cases rather than creating near-duplicates.
- **For versioning**:  If a script’s interface needs to change (breaking change), treat it like an API change – possibly mark old one as deprecated and create a new ID or require bump of some version field.
- **Possibly include a note on script deprecation**:  how to mark and phase out old scripts (maybe via tags in the registry).
- **Documentation integration**:  The top-level scripts/operations-guide.md will:
- Present an overview of script directories and their purposes.
- Provide a quick reference “command cheat sheet” listing important make commands or script commands and what they do (especially those the AI might need to run, e.g., make dev_check, make db_lint, etc.).
- Outline an SOP for using scripts (like “to run any script, prefer using the Make command if available, otherwise call the Python script directly with python scripts/...”).
- If a script requires certain environment or config, mention it.
- If a script has a dry-run mode (which critical ones should have), note how to invoke that.
- **Link to any relevant policy**:  e.g., mention that if scripts have side effects, there is a guardrail for them.
- Possibly have a checklist for adding a new script (as described above) embedded in the doc (the design mentioned a “新增脚本流程” which implies an addition process).
- **Quality and testing requirements for scripts**:  We set standards to ensure scripts are robust:
- **Complexity**:  e.g., we aim for a McCabe complexity under 10 for functions (if over, justify via comments or break it down).
- **Linting**:  All scripts must pass language-specific linters (PEP8 for Python, ShellCheck for shell, etc.).
- **Dry-run**:  Scripts that perform changes (like applying migrations, running destructive ops) should have a --dry-run option that simulates changes or at least outputs the plan without applying. We enforce documenting this and testing it.
- **Logging**:  Scripts should log their key actions (especially if they run in CI or as part of automation, so we have traceability).
- **Tests**:  If a script is complex, ideally have a small test script or at least incorporate a test in CI (maybe using a sample scenario).
- These requirements will be documented in operations-guide.md (quality section) and possibly enforced via CI (like running linters, measuring complexity via a tool, etc.).
- **Testing the script framework**:  We’ll likely add a dummy script to demonstrate the process. For instance, if not already done, a scripts/example.py with a header, properly registered, and perhaps used in a dummy trigger so we can see it end-to-end.
- **Temporary scripts**:  We clarify that any script suffixed with _temp and placed in a temp/ directory is exempt from registration and will be periodically cleaned out. These could be truly one-off or debug scripts and triggers should generally ignore them.

**Rationale**: The scripts directory contains the actual executable tools that the AI or CI might call – they’re the implementation of many low-tier capabilities. Having a strong convention here is vital for the AI to use them correctly and for maintainers to not lose track. By providing an operations guide and ensuring all scripts are indexed, we make scripts discoverable by AI queries (the AI can search in operations-guide to find a script that does X). The registration ensures every script is considered part of the AI’s capability set (no shadow operations the AI doesn’t know about). Quality gates on scripts are critical because these scripts might run as automated jobs affecting data or production; we ensure they are reliable and auditable. Also, by documenting a process to add scripts, we incorporate the modifications note that scripts should be added via a registration scaffold rather than ad hoc – which keeps our method registry consistent.

**Expected Outcomes**: By the end of Phase 9:
- The scripts/operations-guide.md is in place and thoroughly documents script conventions, organization, and processes (covering points from naming to adding to testing). It will likely be a central reference for both AI and human when dealing with scripts.
- All initial scripts introduced in earlier phases (like doc_node_map_sync.py, registry_check.py, guardrail_runner.py, etc.) are documented in this guide and have standardized headers matching our format. Their entries exist in method_registry.yaml (with appropriate IDs and tags).
- The capability index (ABILITY.md) is updated to reflect script-based capabilities (though the ABILITY.md focuses on high-tier, it might list an ability that wraps multiple scripts – we ensure consistency between that and the method registry).
- **The CI pipeline runs script quality checks**:  e.g., a combined make lint that includes script linting, complexity check, etc. We will test that adding a poorly formatted or too complex script triggers CI failure.
- The triggers matrix (or mapping) is compiled in the doc or a YAML file, and we run a check that it matches the triggers config (to avoid doc drift).
- After Phase 9, the repository’s operational tooling is well-codified. This means the AI should rarely be confused about how to run a particular maintenance task – it can refer to the operations guide, find the script and how to call it, and even see example usage or cautions there. For maintainers, onboarding a new script follows a checklist that ensures nothing is forgotten (like updating docs or registry).

---

## Phase 10: Interactive Development Workflow and Context Management

Objective: Implement the framework for interactive project development using the AI agent, ensuring that throughout the loop of planning, execution, and iteration, context is preserved and guardrails are applied. This phase focuses on orchestrator guardrail behavior, work documentation for context, and dynamic context management.

Scope: Phase 10 ties together orchestration and context handling to support an interactive dev cycle:
- **Defining Orchestration Layers and Responsibilities**:  Clarify how the orchestrator and guardrails interact:
- The Control Layer (top-level Orchestrator and Guardrail agents) sits at the root. Assets here include the root ROUTING.md, the global AGENTS.md, the triggers config (agent-triggers.yaml), the registries (registry.yaml, trigger-map.yaml if used). The orchestrator uses these to decide which module or workflow to delegate a task to, and the guardrail uses them to intercept policy violations or special conditions.
- **Module-level Orchestrators (Module Agents)**:  If needed, modules might have their own orchestrator logic or at least the agent defined in their AGENTS.md that enforces local rules. But typically, tasks are either orchestrated by the global orchestrator or directly executed by a module ability. For simplicity, we consider the orchestrator as one entity that can invoke module abilities.
- **Guardrail Agent**:  A specialized agent/policy enforcer that steps in on triggers with block enforcement. It’s responsible for carrying out approval workflows, halting execution until conditions are met, etc. We treat it as part of the orchestration layer because it mediates when normal orchestration is overridden by safety concerns.
- Document in a guide (or in AGENTS.md itself) what each layer does, e.g., orchestrator orchestrates tasks and routing, guardrail approves/blocks risky actions, module agents ensure local compliance.
- **Trigger to Agent Policy to Execution flow (Hook integration)**:  Outline and implement how a trigger event goes through to execution:
  1. Trigger fires – e.g., an event is detected (file change or plan content).
  2. Orchestrator/Guardrail loads the specified preload_docs (policy, guides) for context.
  3. If enforcement is block, the Guardrail agent takes control. The AI should now follow the guardrail procedure:
- **Check in the policy what is required**:  likely needing a human to approve or run a dry-run first etc.
- The guardrail agent might produce a message or a required checklist (like “Approval needed from X. Did you run Y checks?”).
  4. The AI (or system) must then fulfill required_checks – e.g., run the scripts listed and gather their outputs.
  5. The guardrail then either automatically approves if checks pass and criteria allow, or it waits for a human (for critical triggers). Perhaps the system could allow a configured auto-approval for non-critical or for test environment triggers.
  6. Once approval is obtained or not required, execution resumes – the orchestrator either continues the plan (if approved) or aborts/rolls back (if denied).
- **We implement a simple interface for guardrail approvals**:  possibly a section in the workdocs/active/context.md named “Approval” where a human can put an “Approved by …” note that the AI can detect, or a label in the repository (in real usage, could be a PR comment or a specific file updated by an approver).
- If not approved, the guardrail agent should initiate rollback steps if any changes were partially applied. For example, if a deployment was halfway done and blocked, revert it (this might not happen often in local dev context, but for completeness).
- **Logging the result**:  every trigger event and guardrail decision gets logged to ai/workdocs/active/context.md under a relevant section (like #Triggers or #Automation) and also appended to ai/maintenance_reports/route-health.md if it’s a permanent record of route health.
- **Workdocs context structure and usage**:  Our repository will utilize the ai/workdocs/ area heavily during interactive sessions:
- **Under ai/workdocs/active/, for each active task or session (maybe organized by task ID or branch), we have files like**: 
- plan.md – where the AI or developer writes the plan for the task (this is often where the AI enumerates steps, and it’s updated as steps complete or change).
- **context.md – a running context log that includes sections for triggers, approvals, QA, etc. For example, it should have**: 
- # Session Progress summarizing triggered events, approvals pending, next trigger check times, etc. This is basically an index of what’s happening, linking to details.
- # Automation where any outputs from dry-run or automation scripts are pasted.
- # QA where test results or checklist outcomes are recorded (like results of make dev_check before PR submission).
- Possibly a # Triggers section listing each trigger that fired and how it was resolved (like which policy doc was loaded, who approved it, etc.).
- tasks.md – if needed, a structured list of sub-tasks with statuses (pending/in_progress/completed). The idea from initial spec was to maintain a to-do list of steps or triggers and update their status, which a periodic trigger check can verify.
- **We ensure that the AI is instructed to maintain these**:  e.g., after reading a doc that says to log progress, the AI will add an entry like “- [ ] Trigger XYZ: awaiting approval” in tasks.md and then later mark it done.
- When a session completes, these active workdocs are moved to ai/workdocs/archive/<session>/... for record-keeping. (Phase 11 testing might cover verifying that nothing remains in active when done, etc.)
- **Audience and usage**:  Workdocs are not part of the knowledge base for new tasks (they are not routed or indexed beyond the session). But they are critical for context recovery if an AI disconnects or for human supervisors to see what happened in a session. So we will keep them lightweight and factual.
- **Dynamic context management (clean-up and summarization)**: 
- **Over time, active workdocs can grow large (the AI writing a lot). We add a mechanism (as per modifications) to automatically prune and summarize them**: 
- We’ll instrument the workdoc updating code (or advise the AI) to append a timestamp and update count to entries so we can monitor how big it gets.
- **Set thresholds**:  e.g. if context.md exceeds 150 lines or has >5 major updates, or contains content older than 30 days (for long-running tasks), trigger a clean-up.
- **Clean-up process (likely not done by AI spontaneously, but via a scheduled script workdocs_gc.py maybe)**: 
- Take the old content (e.g. older than last N entries) and feed it to a summarizer (the AI itself can do this or we can have a script that compresses it).
- Save the summary to ai/workdocs/archive/<session>/summary_<date>.md or in an archive/summary/ location.
- Remove or collapse those parts in the active file, maybe leaving a note “[Summary of earlier content moved to archive]”.
- Ensure nothing needed for active decision-making is removed (the heuristics of last N lines or based on markers should ensure current open tasks remain).
- This mechanism will run via CI or a periodic task, not by the AI agent during a conversation (to avoid confusion). Possibly triggered by a commit hook if a workdoc file crosses length threshold, or just nightly.
- The goal is to keep the active context the AI deals with manageable, while not losing the history (just summarizing and archiving it).
- We’ll test this by simulating a long session and running the summarization script.
- **Task status tracking**:  As per modifications, we require the AI to update task statuses (in tasks.md or similar) as it proceeds. For example, at the start, list tasks 1,2,3 as pending. After finishing task 1, mark it done. This provides visibility and helps any supervising process or person know what’s done. It also ties into triggers: a health check script could verify that no tasks are left pending unexpectedly (or that triggers flagged as pending were eventually completed).
- **Guardrail Execution & Approval Loop**:  We detail the formal steps which we partially did above:
- **Document in a Guardrail guide (or AGENTS.md section) what the enforcement process is. E.g., “If enforcement**:  block, the process is: 1) Orchestrator halts further code execution. 2) Required docs X, Y are loaded. 3) The AI must perform checks A, B (which are required_checks). 4) The AI must then notify a human or look for approval in handoff-history.md or PR. 5) Once approval is found, log it and proceed; if denied or rollback needed, perform rollback steps and log those”.
- **Implement support scripts**:  e.g., guardrail_runner.py which can simulate or actually drive an approval. For instance, running guardrail_runner.py --event <id> could replay what happened for auditing, as noted in spec. Or a guardrail_runner.py --dry-run all to test all block triggers automatically if they were triggered (like a test mode).
- **Keep an audit log**:  handoff-history.md to record all approvals and rejections, with timestamps and reasons. Possibly one global or one per event. The guardrail agent will append entries here whenever a trigger required human input and got resolved.
- Ensuring all block triggers indeed produce logs that can be replayed is part of Phase 11 testing.
- **Continuous improvement hooks**:  We note that if an orchestrator or guardrail process changes (like we add a new trigger or alter a policy), we must update the relevant docs and maps, and run our sync scripts (like module_registry_sync.py and doc_node_map_sync.py) to propagate changes. We’ll incorporate that into a maintenance section saying “whenever adjusting orchestrator or guardrail config, run these commands and paste results in workdoc QA section”.

**Rationale**: Phase 10 is about making the development process interactive and resilient. The AI will be effectively pair-programming with our guardrails and using the workdocs as memory. By establishing clear logging and state tracking (tasks and context), we enable the AI to handle complex tasks over time (maybe it stops at end of day, then resumes next day by reading context.md to recall what happened – this is exactly why we keep those logs). Dynamic context management addresses the challenge of context window limits in long sessions, summarizing older info so the AI can still access it if needed (from archive) but isn’t bogged down by it in active memory. The guardrail loop is critical for safety: even with triggers, we need a formal process for when something is blocked. By designing it clearly, we ensure such events (which are high stress points) are handled systematically, not ad hoc. Logging everything to workdocs and audit files means transparency – any stakeholder can review what the AI did, what it was stopped from doing, and why. This builds trust in the AI processes and makes debugging easier (if a trigger misfires, the logs will show it). Also, requiring the AI to update statuses and logs trains it to be more self-documenting, a desirable quality. The modifications we included – like cleaning workdocs and explicit status updates – come from practical concerns that an ever-growing context could confuse the AI or lead to runaway tasks. Addressing these now sets up the environment to scale to lengthy or multiple concurrent tasks without losing clarity.

**Expected Outcomes**: By the end of Phase 10:
- **A demonstration interactive session scenario (perhaps as part of a test or example) will show the AI going through a non-trivial task using our system. For instance, the AI is tasked with adding a new feature that involves a DB change and an API change**: 
- It writes a plan in plan.md.
- It consults routing and loads needed docs.
- It makes code changes, triggers fire (DB migration, API contract).
- Guardrail blocks the DB migration pending approval. The AI runs checks, logs in context, asks for approval (we simulate giving approval by adding a line in context or a file).
- AI sees approval, proceeds to run migration (maybe in dry-run), completes the work.
- It updates tasks.md statuses, runs tests (maybe fails or passes).
- It prepares a PR with the workdoc QA snippet (like outputs of dev_check) pasted in context.md QA section.
- All these steps leave traces in ai/workdocs/active which we then archive at end.
- The ai/workdocs/active directory structure and templates will be established, possibly with a template plan.md and context.md that tasks can copy or initialize with (with sections pre-laid out, e.g., a context.md template with placeholders for each section).
- **The dynamic trimming of workdocs will be in place**:  we can simulate adding lots of lines to context.md and then run the cleaning script to verify it archives old content properly.
- **The guardrail approval mechanism will be tested**:  e.g., the guardrail agent should detect an “Approved” marker or a special commit status. We’ll have documented how a human maintainer can provide approval (maybe by editing handoff-history.md to add an approval entry, or via a PR comment which is beyond our offline environment, so likely by editing a file or section that the AI monitors).
- In ai/reports/route-health.md, we will start recording interesting metrics (though fully doing telemetry was more in Phase 3.5 and Phase 11). But by now, route-health might log how many triggers fired in a session, average time to approval, etc., which helps future improvements.
- Summing up, Phase 10 ensures that when an AI agent is actively working on the repository, it can do so in a structured way with continuous context and safety nets. This sets the stage for the final testing/QA phase to verify all these mechanisms.

---

## Phase 11: Testing, Validation, and Quality Assurance

Objective: Establish a rigorous testing and validation framework for the repository template, comprising multiple layers of checks (documentation consistency, capability contract tests, automation dry-runs), and define the standards for verifying contributions (PR process). This phase ensures that by the time the template (or any subsequent changes) is delivered, it meets all defined requirements and is free of structural or logical errors.

Scope: Phase 11 implements a three-tier test plan as well as contribution guidelines enforcement:
- **Layer 1**:  Structural and Routing Consistency Checks – Verify that all documentation and routing links are correct and aligned:
- Run make route_lint which will scan all ROUTING.md files and other docs for syntax errors, missing fields, etc..
- Run scripts/doc_node_map_sync.py --report to compare the actual docs and links with the doc-node-map.yaml index, ensuring every route node points to a valid doc and that every doc is accounted for in routing.
- Run make registry_gen --check which likely rebuilds an expected registry of docs (maybe from README or registry.yaml) and compares to see if all new docs are registered properly. For example, if a new module was added, is it listed in the modules registry; if a new guide doc added, is it referenced somewhere.
- The results of these commands should show no errors (meaning no orphan docs, no broken links, etc.). We capture the output and note any corrections made in ai/workdocs/active/<task>/context.md#QA for transparency.
- **Acceptance criteria**:  “No syntax or linkage errors; every AI-facing document has proper doc_role, route_role etc. that matches how it’s referenced; the routing tables match actual content”. This ensures the doc system (Phase 1 & 2) is intact.
- **Layer 2**:  Capability Contract Tests – Validate the consistency between our capabilities registry/graph and actual functional elements, as well as domain-specific contract tests:
- **Run make capability_index_check and python scripts/registry_check.py. These will ensure**: 
- Each ability in ability_registry.yaml has a corresponding entry (or section) in ABILITY.md and vice versa, with matching graph_node_id, tags, etc..
- For each ability, all its referenced methods exist in method_registry.yaml and the mapping in orchestrator registry (e.g. agent-graph.yaml or registry.yaml that binds ability nodes to implementations) is consistent.
- tools_allowed lists in orchestrator vs actual scripts existence, etc., are consistent.
- **Domain-specific contract tests**: 
- **For Database**:  make contract_compat_check (or a DB-specific check if named differently) to verify doc_agent/specs/DB_SPEC.yaml matches db/engines/postgres/docs/DB_SPEC.yaml and that no schema drift occurred (if we simulate a baseline of schemas). Possibly also applying all migrations on a fresh DB instance to ensure they align with schema definitions.
- **For API**:  make contract_compat_check and python scripts/type_contract_check.py to verify no breaking API changes (needs a baseline contract to compare with) and that generated types match the contract.
- We ensure these pass or if there’s intentional changes (like we updated a contract), that the process was followed (like bump version or got approval).
- If any issues, we log them and require either fixes or a documented exception (maybe a contract changed knowingly, then the baseline update must be done to satisfy the test).
- **Acceptance criteria**:  “All capability registry tests pass, meaning the implementation graph and documentation are in sync. All API/DB contract tests pass, meaning external interfaces and data schemas have no unapproved divergences”. Additionally, any change to contracts triggers an update in workdocs plan to handle approvals (we check that if there were contract diffs, a note in plan.md about approval exists).
- **Layer 3**:  Automation Pipeline Dry-Run and Trigger Validation – Ensure that the automation/triggers behave as expected in a test scenario:
- Run make trigger_check to simulate triggers on a representative set of file changes or events. This should produce output showing each trigger and any errors if misconfigured (like a broken regex). We expect no errors from reading the triggers config.
- Run python scripts/context_usage_tracker.py report --limit 10 to generate a short report on trigger hits or context usage (assuming it logs trigger usage during tests). This is more for after some usage; since it’s a template, we might have to simulate some events to populate data for this. Possibly skip or trivial output if none.
- **For newly added or changed automation flows**:  have the AI or CI run those flows in --dry-run mode. For example, if we added a new deploy ability with triggers, run it with --dry-run to confirm it outputs logs and risk warnings properly.
- Paste outputs of these dry-run tests into context.md#Automation in workdocs and update route-health.md if needed.
- Check that no trigger produced an unexpected result (like it didn’t catch something it should, or it flagged something incorrectly).
- **Acceptance**:  “All triggers can be loaded without error; their dry-run execution yields expected logging; and no automation script fails or behaves unexpectedly in test mode”. Essentially, the pipeline should appear “healthy” and ready for actual use.
- **Note**:  We do not yet incorporate cost monitoring or such (explicitly left out).
- **(Optional) Layer 4**:  Deployment Pipeline – The original mention of section 11.4 suggests building and deployment might be extended later. For now, we skip actual deployment tests. We note that after these three layers pass, one could integrate this into a CI/CD pipeline for deployment, but that’s outside current scope and budget (no cost or production deployment in template by default).
- **PR Submission and Verification Process**:  Define the checklist that every contribution must adhere to (particularly important when AI is generating PRs):
- **Before opening a PR, the contributor (AI or human) must run make dev_check which includes all relevant tests**:  formatting, unit tests, doc lint, capability checks, etc.. Only if it passes (all tests green) can they proceed.
- Also run make trigger_check to ensure no unresolved triggers.
- They should gather key outputs (like the summary of make dev_check and any contract check results) and paste them into the ai/workdocs/active/<task>/context.md#QA section, which later can be referenced in the PR description.
- Verify all documentation is updated as per changes (if a new doc was added, it’s in routing; if an ability added, ABILITY.md updated; if a policy changed, doc_node_map updated, etc.).
- **Write the PR description following a template (the repository will provide a PR template, likely in .github/PULL_REQUEST_TEMPLATE.md). The description must include**: 
- A brief summary of changes.
- Scope of impact (which modules, features).
- Steps to verify (which could link to the context workdoc where test outputs are).
- Workdoc link (e.g., mention “See ai/workdocs/active/Task123/context.md#QA for test logs”).
- Any TODO or follow-ups.
- If AI-facing docs changed, list the route nodes added/removed so maintainers can update any external indexes accordingly.
- **Ensure commit history is clean**:  abide by conventional commits format (or at least the chosen commit style) and ideally squash or organize commits logically. The policy from modifications suggests trunk-based dev with short-lived branches, and commit messages like feat(scope): ... etc.. We enforce that no large reformat commits are mixed with feature changes (maybe via a check on diff if it’s mostly whitespace).
- **Approval gating**:  If a PR triggers a guardrail (like touches critical files), it must not be merged until the designated owner approves. e.g., if the PR includes a migration in a prod directory, it should tag the DBA or such for approval (this can be automated by a CODEOWNERS or by our triggers posting a comment). We note that “if triggers with enforcement block are hit, the PR should @ the responsible person and merging blocked until approval”. This will likely be enforced by branch protections or CI (like requiring a label).
- **Encourage small, iterative PRs**:  The AI or dev should not bundle too many changes. Ideally each PR addresses one issue or module. If a larger change spans modules, break it into a parent issue with multiple PRs (the modifications mention a parent-child PR structure for cross-module changes). This aligns with our triggers that prefer one module at a time.
- Also require that the requirements or issue are phrased in a verifiable way (so testers can check each acceptance criterion). This makes the review process easier and aligns AI planning with actual goals.
- **Exclusions**:  Acknowledge that things like multi-language support, cost monitoring, or user feedback loops are not covered in these tests or template scope (consistent with initial and modified spec). We might mention that they remain future enhancements beyond current QA.

**Rationale**: This testing phase is where we validate that everything built in Phases 1-10 actually holds together. Without these checks, an AI integrated system can degrade over time (missing a doc update or a registry entry can throw the AI off, but might not be obvious until much later). The three-layer approach addresses documentation integrity, functional integrity (capabilities and interfaces), and dynamic process integrity (automation working smoothly). By enforcing them from the template stage, we ensure any project starting with this template has quality built-in. The PR process standardization ensures that contributions (especially by AI) remain high quality and traceable. It’s effectively our final guardrail making sure that the human-in-the-loop can trust the changes and that nothing sneaks by (like an AI making a subtle dangerous change would be caught either by triggers or by failing a test or by requiring an explicit sign-off).

**Expected Outcomes**: By the end of Phase 11:
- **A fully automated CI pipeline configured (likely using GitHub Actions or similar, though in the template we might just provide a Makefile and instructions) that runs**: 
- Linting, tests, etc.
- make dev_check which composes various checks (we will have updated it to include doc lints, triggers, etc.).
- make trigger_check and perhaps any special contract checks.
- Ensures any failure prevents merge.
- All current content (docs, code, config) passes these checks. We’ll produce a final maintenance report or log showing “All checks PASS” for the baseline template.
- Documentation for contributors (maybe a CONTRIBUTING.md in doc_human/ or notes in AGENTS.md) clearly states the PR requirements and testing process.
- **The project’s maintainers (or an AI playing that role) will have a clear checklist to verify when reviewing a PR**: 
  1. CI is green (meaning all tests including our custom ones passed).
  2. The PR description includes required info and references workdocs evidence.
  3. If applicable, approvals from domain owners are present.
  4. Documentation changes are included and correct.
- Because we have decided not to include extraneous features (like cost monitoring, etc.), our tests focus strictly on what we built and we confirm that aligns with user’s acceptance (the spec explicitly said those extras are out-of-scope and we keep it that way for simplicity).
- Ultimately, Phase 11 gives us the confidence to declare the repository template implementation complete and robust, as it will have gone through a gauntlet of validation ensuring adherence to all earlier phases’ guidelines.

---

## Phase 12: Full Repository Initialization and Onboarding

Objective: Develop a procedure and tooling for initializing the entire repository template for a specific project. This phase ensures that when applying this template to start a new project, all required information is gathered and the template is customized appropriately with minimal manual effort.

Scope: In Phase 12, we treat the scenario where this repository template is used as a boilerplate for a new project. Key aspects:
- **Creating an Initialization Script/Process**:  We’ll implement a utility (potentially scripts/scaffold-repo/ or an /init/ directory script) that guides the user (or AI) through converting the template into a project:
  1. It will prompt for project-specific information: e.g. project name, description, initial modules needed, any specific config (like default port numbers, etc.), possibly credentials placeholders, etc..
  2. It might ask the user to provide an initial requirements document or fill a questionnaire (which could even be an AI-assisted step: the user gives high-level description, AI produces some initial docs to place in the project).
  3. Based on input, the script will replace placeholders in the template. For instance, change the repository name in README, set an environment variable file with the project name, update any global identifiers (like package names).
  4. If the user provided a requirements doc, perhaps convert it into an AI-readable form and attach it in doc_human/ and reference it in doc_agent/ROUTING.md under a “Project Requirements” scope (fulfilling the idea that special project docs are integrated).
  5. If initial modules are specified, run the module scaffold to create them (so Phase 5’s scaffold is invoked by the repo-init for each module).
  6. Possibly run all the checks after initialization to ensure the new repo passes CI.
- **Defining Initialization End-State**:  The repository initialization is considered done when:
- All necessary directories and files for the baseline are in place and updated with project-specific info (like the project name inserted in docs where needed, any sample placeholders removed or confirmed).
- The documentation route is fully connected (e.g., if some generic placeholder content existed, it’s replaced or removed).
- All CI tests pass in the new context (so the template’s checks still succeed).
- **Essentially “just registered + ready docs, everything is set up, but no actual business logic implemented yet”. This mirrors module init concept**:  we deliver a skeleton that is fully coherent and test-passing, though it might not provide any real feature beyond a hello-world.
- **Minimal manual follow-up needed**:  maybe the maintainer only has to push the repo to their chosen VCS and set up the CI secrets, etc., but not fix template issues.
- **Providing an Init Directory and Guides**:  As part of the template, include an /init/ directory:
- It will contain usage instructions (maybe an INIT_GUIDE.md or a printed message) on how to run the initialization.
- Possibly a config file or template (like project_config.yaml) that can be filled out with project details and fed to the script in non-interactive mode.
- Scripts or helper files used only during init.
- After running the initialization, this directory can be safely removed from the project (the script could do it or instruct the user to remove it).
- **The idea is to separate concerns**:  the template comes with extra stuff to help start a project, but that stuff isn’t needed once the project is up, so it should be cleaned out to avoid confusion.
- **Ensuring common assets are in place**:  The template itself will carry most of the generic content:
- All the docs we prepared (policies, routing, guides) remain. Some might need slight tuning based on project specifics (like naming). The script should handle that (e.g., if the project name should be inserted in the README, or if certain doc references should include the name).
- All baseline tools and scripts remain so that from day 1, the new project has the full arsenal (the new project will likely keep all scripts like doc_node_map_sync.py, etc., because they are part of the ongoing maintenance).
- We assume the template’s default modules (like api_gateway and the DB integration) are relevant to any project using this stack. But if a user chooses not to include some (maybe they don’t want the DB part), the init process might allow deselecting some features. This can be advanced, but at least we can structure things so optional modules can be removed (with corresponding docs like if you remove DB, remove its routing entries, etc.). Possibly not automated now, but mention as a caution.
- **Documentation**:  Provide a short guide, possibly in README.md, that instructs how to perform initial project setup using the init script. Also emphasize that after initialization, you should run the full test suite to ensure everything is green (though our script likely does it).
- **For the AI context**:  After initialization, the doc_agent/ROUTING.md and others might still talk about template stuff. The init script should ensure that anything template-specific (like references to “template” vs “project”) are adjusted. And that no extraneous info (like this planning spec) remains in AI-consumable docs.
- Perhaps move this planning spec to ai/reports/archive or remove it, since the project now is actual and the AI doesn’t need to see the speculative planning.

**Rationale**: This phase addresses how the template transitions to real-world usage. Without an initialization step, users might have to manually strip out template placeholders or feel confused about how to start. By automating it, we reduce human error (ensuring nothing critical is omitted when turning the template into a working repo). It’s essentially the template’s first application, and thus serves as a test of all previous phases in a holistic manner (if our template is truly modular and well-indexed, the init process should be straightforward). By removing the init scaffolding after use, we keep the project clean – which is important because we don’t want leftover scaffold scripts that could be misused or that the AI might accidentally run later. This also ties into security (ensuring no default credentials or keys are left in, etc.).

**Expected Outcomes**: By the end of Phase 12:
- The template repository includes the /init/ directory with all needed materials to onboard a new project.
- We have tested the initialization script on a dummy “project name X” and it produced a new repository variant that passes all tests and has docs properly set.
- For example, if the user says project “Retail API”, the script renames references (maybe the README “AI-integrated Repo Template” becomes “Retail API Dev Repository” etc.), and includes any specific domain info given.
- After running it, the script might output “Initialization complete. You may now remove the /init directory and start development. All tests passed.”.
- We ensure the initial commit for the new project is ready to go, requiring no further template tweaking. The maintainers can focus on building features rather than configuring the pipeline.
- Documented clearly in the repository’s main README how to execute this initialization in case someone not using the AI agent wants to do it manually.
- At this point, the template is not just theoretical – it’s been validated to spin up a working project instance easily.

---

## Phase 13: Future Enhancements and Evolution (Post-Implementation)

Objective: Outline additional enhancements and best practices that can be pursued after the core template is implemented, acknowledging areas like code style enforcement, dependency management, advanced guardrails (cost, privacy), and scalability to different project sizes.

Scope: This is not an active development phase for the initial template, but rather a compilation of potential improvements that were identified but deferred, which can guide future maintenance:
- **Coding Standards & Lint Enhancements**:  While we set up basic lint checks, we can further integrate:
- Extended style guides or formatters, and caution that large-scale reformatting should not be done in the same PR as functional changes (to keep diffs clean).
- Possibly auto-formatting tools (Black for Python, Prettier for frontend) configured but ensure the contributor guidelines say to separate formatting-only changes.
- **Dependency and Supply Chain Management**: 
- Implement dependency locking (like using pip-tools for Python to lock versions) so that builds are deterministic.
- Maintain a license allowlist (avoid pulling in unknown licensed code).
- Introduce triggers for dependency changes, e.g. if requirements.txt changes, run a security audit tool or require review (future idea).
- Automate dependency updates on a schedule with AI assistance (e.g. a bot that bumps versions, then triggers tests).
- **Secrets and Configuration Security**: 
- Enforce that no plaintext secrets are committed (set up a git pre-commit or a scanning in CI for patterns like API keys).
- Provide a pattern for using environment variables or Vault integration for secrets, documented in a security guide.
- Possibly integrate a secret scanning tool for CI.
- **AI Cost and Quota Guardrails**: 
- **As the project evolves to possibly use AI services (like calling an external API), define budgets and implement guardrails**: 
- E.g., a trigger that counts how many times the AI attempted an expensive operation and if above threshold, forces a different strategy or halts to prevent cost overrun.
- This could tie into context_usage_tracker to monitor tokens used, etc. (Though not immediate, we note it for future).
- **Data Privacy Minimization**: 
- If dealing with sensitive data, ensure by design no PII is logged or stored unnecessarily.
- Could require anonymization or redaction of logs if PII might appear.
- If needed, have a policy doc about data privacy that AI can read and follow (not included now, but might add if project deals with user data).
- **Enhanced Interaction Tools**: 
- Develop utilities to visualize relationships, e.g., a script to output a graph of high-tier vs low-tier capabilities (so maintainers can see the big picture easily).
- Tools to simulate triggers and guardrails at scale (maybe run a scenario generator through all triggers to see if any conflict).
- **A tool to analyze cross-impact**:  if a file or API changes, list what documentation or registry entries might need updating. Could be as simple as grep for references, or as complex as tracking dependencies in a graph.
- **Project Scalability**: 
- After verifying the template on a medium project, consider making a lite version for small projects that might not need all components (maybe only one module, no DB or simpler docs). This could be a subset template or a flag in the init script (e.g., --minimal mode that only sets up core without DB/API).
- **Multi-language support**:  If expansions to other programming languages or frameworks are needed (say a Node.js module or a multi-language repo), consider how to incorporate that. Possibly have separate templates, or design the doc and orchestration in a language-agnostic way so it can be reused.
- **Monitoring & Feedback**: 
- Introduce cost monitoring of AI calls and performance metrics aggregator (maybe integrated later when real usage is observed).
- Possibly incorporate a user feedback loop where user test results or feedback are captured and fed into a workdoc or backlog (the template doesn’t cover this yet, but as an enhancement one might add a mechanism for AI to get user bug reports from a file or issue tracker and act on them).
- **Continuous Improvement Process**: 
- Suggest adding to the project’s README or maintenance docs the plan to iteratively enhance these aspects as the project matures (maybe through maintenance sprints, etc.).
- Emphasize that any such enhancements should also follow the same principles (documented, tested, etc.).

**Rationale**: Listing future enhancements serves as a roadmap for maintainers. It ensures that while we have a robust starting point, we are aware of its boundaries and ready to address them in the future without mixing them into the initial deliverable (to avoid complexity creep now). Some items like code style are minor but help maintain long-term health. Others like cost and feedback loops depend on actual project context and thus are deferred. Including them in an internal plan highlights that we have considered them and intentionally postponed – which is itself a sign of thorough planning. Also, as AI tooling evolves, we might incorporate new best practices (e.g. new guardrail frameworks, better context management algorithms) – having a section acknowledging that encourages future maintainers (perhaps another AI or team) to continue adhering to the template’s philosophy when making upgrades.

**Expected Outcomes**: Phase 13 itself doesn’t produce direct changes in the repository (since it’s beyond current scope), but as a planning spec:
- The README.md or a maintenance doc might include a short section “Future Work” summarizing these points for transparency (especially things like cost monitoring being out-of-scope now to set expectation).
- The team has a clear list of enhancements that can be taken on once the current system is stable, possibly logged as backlog items or issues in the tracker.
- The AI (if continuing to assist) can be guided by this list to not try to implement these prematurely and instead treat them as optional or next-phase tasks.
- In essence, we ensure the template is not a dead end but a foundation that can adapt to additional needs as they arise.

⸻

Conclusion: This phased development plan provides a structured path from laying the documentation and orchestration groundwork (Phases 1–5), through integrating key domain features (Phases 6–7), to building automation and ensuring safe AI-driven workflows (Phases 8–10), and finally validating everything via comprehensive QA (Phase 11) and easing template adoption (Phase 12). By following these phases sequentially, the development team (and AI agents involved) can focus on one aspect at a time with clear objectives and acceptance criteria. Each phase builds upon previous ones, culminating in a repository template that is not only functional and AI-integrated, but also maintainable, extensible, and well-governed. The plan also sets the stage for continuous improvement, ensuring the template can evolve with future requirements.